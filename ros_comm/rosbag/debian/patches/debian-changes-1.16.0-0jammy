Description: <short summary of the patch>
 TODO: Put a short summary on the line above and replace this paragraph
 with a longer explanation of this change. Complete the meta-information
 with other relevant fields (see below for details). To make it easier, the
 information below has been extracted from the changelog. Adjust it or drop
 it.
 .
 ros-noetic-rosbag (1.16.0-0jammy) jammy; urgency=high
 .
   * add missing repeat_latched initialization (#2314 <https://github.com/ros/ros_comm/issues/2314>)
   * Contributors: Robin Vanhove
Author: Michael Carroll <michael@openrobotics.org>

---
The information above should follow the Patch Tagging Guidelines, please
checkout http://dep.debian.net/deps/dep3/ to learn about the format. Here
are templates for supplementary fields that you might want to add:

Origin: <vendor|upstream|other>, <url of original patch>
Bug: <url in upstream bugtracker>
Bug-Debian: https://bugs.debian.org/<bugnumber>
Bug-Ubuntu: https://launchpad.net/bugs/<bugnumber>
Forwarded: <no|not-needed|url proving that it has been forwarded>
Reviewed-By: <name and email of someone who approved the patch>
Last-Update: 2024-08-18

--- /dev/null
+++ ros-noetic-rosbag-1.16.0/CHANGELOG.rst
@@ -0,0 +1,332 @@
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+Changelog for package rosbag
+^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+
+1.16.0 (2023-02-15)
+-------------------
+* add missing repeat_latched initialization (`#2314 <https://github.com/ros/ros_comm/issues/2314>`_)
+* Contributors: Robin Vanhove
+
+1.15.15 (2022-11-23)
+--------------------
+* Move @jacobperron from maintainer to author (`#2302 <https://github.com/ros/ros_comm/issues/2302>`_)
+* Fix rosbag reindex not seeking to truncated position after broken chunk (`#2286 <https://github.com/ros/ros_comm/issues/2286>`_)
+* Contributors: Emerson Knapp, Shane Loretz
+
+1.15.14 (2022-01-06)
+--------------------
+
+1.15.13 (2021-09-22)
+--------------------
+
+1.15.12 (2021-09-21)
+--------------------
+
+1.15.11 (2021-04-06)
+--------------------
+* Handle SIGINT in rosbag play (`#2150 <https://github.com/ros/ros_comm/issues/2150>`_)
+* Catch all exceptions in record thread (`#2151 <https://github.com/ros/ros_comm/issues/2151>`_)
+* raw_input does not exist in python 3 (`#2143 <https://github.com/ros/ros_comm/issues/2143>`_)
+* Contributors: Martin Pecka, Sebastian Scherer, pseyfert
+
+1.15.10 (2021-03-18)
+--------------------
+* Add missing Boost (`#2108 <https://github.com/ros/ros_comm/issues/2108>`_)
+* Start player in paused state (`#2086 <https://github.com/ros/ros_comm/issues/2086>`_)
+* Contributors: Francisco Vina, Timo RÃ¶hling
+
+1.15.9 (2020-10-16)
+-------------------
+* Update maintainers (`#2075 <https://github.com/ros/ros_comm/issues/2075>`_)
+* Fix spelling (`#2066 <https://github.com/ros/ros_comm/issues/2066>`_)
+* Gracefully stop recording upon SIGTERM and SIGINT (`#2038 <https://github.com/ros/ros_comm/issues/2038>`_)
+* Fix compatibility issue with boost 1.73 and above (`#2023 <https://github.com/ros/ros_comm/issues/2023>`_)
+* Use heapq.merge instead of custom merge sort code (`#2017 <https://github.com/ros/ros_comm/issues/2017>`_)
+* Contributors: Devin Bonnie, Florian Friesdorf, Sean Yen, Shane Loretz, tomoya
+
+1.15.8 (2020-07-23)
+-------------------
+
+1.15.7 (2020-05-28)
+-------------------
+
+1.15.6 (2020-05-21)
+-------------------
+
+1.15.5 (2020-05-15)
+-------------------
+* add option to repeat latched messages at the start of bag splits (`#1850 <https://github.com/ros/ros_comm/issues/1850>`_)
+* fix bag migration failures caused by typo in connection_header assignment (`#1952 <https://github.com/ros/ros_comm/issues/1952>`_)
+
+1.15.4 (2020-03-19)
+-------------------
+* restrict boost dependencies to components used (`#1871 <https://github.com/ros/ros_comm/issues/1871>`_)
+
+1.15.3 (2020-02-28)
+-------------------
+* remove Boost version check since Noetic only targets platforms with 1.67+ (`#1903 <https://github.com/ros/ros_comm/issues/1903>`_)
+
+1.15.2 (2020-02-25)
+-------------------
+
+1.15.1 (2020-02-24)
+-------------------
+* use setuptools instead of distutils (`#1870 <https://github.com/ros/ros_comm/issues/1870>`_)
+
+1.15.0 (2020-02-21)
+-------------------
+
+1.14.4 (2020-02-20)
+-------------------
+* bump CMake minimum version to avoid CMP0048 warning (`#1869 <https://github.com/ros/ros_comm/issues/1869>`_)
+* add quotes around file name so they can be click selected in terminal (`#1813 <https://github.com/ros/ros_comm/issues/1813>`_)
+* catch exceptions by const ref (`#1874 <https://github.com/ros/ros_comm/issues/1874>`_)
+* read GPG passphrase from an environment variable (`#1856 <https://github.com/ros/ros_comm/issues/1856>`_)
+* fix missing import of roslib (`#1818 <https://github.com/ros/ros_comm/issues/1818>`_)
+* fix regression from pycrypodome switchover (`#1814 <https://github.com/ros/ros_comm/issues/1814>`_)
+* use condition attributes to specify Python 2 and 3 dependencies (`#1792 <https://github.com/ros/ros_comm/issues/1792>`_)
+* add pycryptodome as default (`#1609 <https://github.com/ros/ros_comm/issues/1609>`_)
+* encrypted rosbag fixes for Python 3 (`#1777 <https://github.com/ros/ros_comm/issues/1777>`_)
+* fix bug in bag migration (`#1786 <https://github.com/ros/ros_comm/issues/1786>`_)
+* keep latched topics latched (`#1708 <https://github.com/ros/ros_comm/issues/1708>`_)
+* wrap the rosbag filter eval in a lambda (`#1712 <https://github.com/ros/ros_comm/issues/1712>`_)
+* record: fix signed int overflow (`#1741 <https://github.com/ros/ros_comm/issues/1741>`_)
+* switch to yaml.safe_load(_all) to prevent YAMLLoadWarning (`#1688 <https://github.com/ros/ros_comm/issues/1688>`_)
+* pickleable rosbag exceptions (`#1210 <https://github.com/ros/ros_comm/issues/1210>`_ revisited). (`#1652 <https://github.com/ros/ros_comm/issues/1652>`_)
+* fix topic message count for rosbag indexed v1.2 (`#1648 <https://github.com/ros/ros_comm/issues/1648>`_)
+* fix wrong error handling in migration (`#1639 <https://github.com/ros/ros_comm/issues/1639>`_)
+* modernization: replaced BOOST_FOREACH with range-based for-loops, used algorithm where appropriated (`#1641 <https://github.com/ros/ros_comm/issues/1641>`_)
+* fix IOError during Python file operation (`#1617 <https://github.com/ros/ros_comm/issues/1617>`_)
+* add Windows.h usage explicitly (`#44 <https://github.com/ros/ros_comm/issues/44>`_) (`#1616 <https://github.com/ros/ros_comm/issues/1616>`_)
+* fix waitForSubscribers hanging with simtime (`#1543 <https://github.com/ros/ros_comm/issues/1543>`_)
+* publish last message from latch topics when start time > 0 (`#1537 <https://github.com/ros/ros_comm/issues/1537>`_)
+* add a new option to publish when a bag write begin (`#1527 <https://github.com/ros/ros_comm/issues/1527>`_)
+
+1.14.3 (2018-08-06)
+-------------------
+* restore API compatibility (`#1473 <https://github.com/ros/ros_comm/issues/1473>`_) (regression from 1.14.0)
+* throw BagException when disk is full (`#1451 <https://github.com/ros/ros_comm/issues/1451>`_)
+
+1.14.2 (2018-06-06)
+-------------------
+
+1.14.1 (2018-05-21)
+-------------------
+
+1.14.0 (2018-05-21)
+-------------------
+* keep connection header info in rosbag filter/compress (`#1372 <https://github.com/ros/ros_comm/issues/1372>`_)
+* implement bag encryption/decryption (`#1206 <https://github.com/ros/ros_comm/issues/1206>`_)
+* add TransportHint options --tcpnodelay and --udp (`#1295 <https://github.com/ros/ros_comm/issues/1295>`_)
+* fix check for header first in rosbag play for rate control topic (`#1352 <https://github.com/ros/ros_comm/issues/1352>`_)
+
+1.13.6 (2018-02-05)
+-------------------
+* return an error status on error in rosbag (`#1257 <https://github.com/ros/ros_comm/issues/1257>`_)
+* fix warn of --max-splits without --split (`#1237 <https://github.com/ros/ros_comm/issues/1237>`_)
+
+1.13.5 (2017-11-09)
+-------------------
+
+1.13.4 (2017-11-02)
+-------------------
+
+1.13.3 (2017-10-25)
+-------------------
+* fix publishing of selected topics from bag file (`#1156 <https://github.com/ros/ros_comm/issues/1156>`_)
+* fix Python 3 compatibility (`#1150 <https://github.com/ros/ros_comm/issues/1150>`_)
+
+1.13.2 (2017-08-15)
+-------------------
+
+1.13.1 (2017-07-27)
+-------------------
+* fix handling connections without indices (`#1109 <https://github.com/ros/ros_comm/pull/1109>`_)
+* improve message of check command (`#1067 <https://github.com/ros/ros_comm/pull/1067>`_)
+* fix BZip2 inclusion (`#1016 <https://github.com/ros/ros_comm/pull/1016>`_)
+* expose rate-control-topic and rate-control-max-delay args to command line tool (`#1015 <https://github.com/ros/ros_comm/pull/1015>`_)
+* improve migration rule generation (`#1009 <https://github.com/ros/ros_comm/pull/1009>`_, `#1010 <https://github.com/ros/ros_comm/pull/1010>`_, `#1011 <https://github.com/ros/ros_comm/pull/1011>`_)
+
+1.13.0 (2017-02-22)
+-------------------
+
+1.12.7 (2017-02-17)
+-------------------
+* throw exception instead of accessing invalid memory (`#971 <https://github.com/ros/ros_comm/pull/971>`_)
+* move headers to include/xmlrpcpp (`#962 <https://github.com/ros/ros_comm/issues/962>`_)
+* added option wait-for-subscriber to rosbag play (`#959 <https://github.com/ros/ros_comm/issues/959>`_)
+* terminate underlying rosbag play, record  on SIGTERM (`#951 <https://github.com/ros/ros_comm/issues/951>`_)
+* add pause service for rosbag player (`#949 <https://github.com/ros/ros_comm/issues/949>`_)
+* add rate-control-topic and rate-control-max-delay. (`#947 <https://github.com/ros/ros_comm/issues/947>`_)
+
+1.12.6 (2016-10-26)
+-------------------
+* fix BagMigrationException in migrate_raw (`#917 <https://github.com/ros/ros_comm/issues/917>`_)
+
+1.12.5 (2016-09-30)
+-------------------
+
+1.12.4 (2016-09-19)
+-------------------
+
+1.12.3 (2016-09-17)
+-------------------
+* set default values for min_space and min_space_str (`#883 <https://github.com/ros/ros_comm/issues/883>`_)
+* record a maximum number of splits and then begin deleting old files (`#866 <https://github.com/ros/ros_comm/issues/866>`_)
+* allow 64-bit sizes to be passed to robag max_size (`#865 <https://github.com/ros/ros_comm/issues/865>`_)
+* update rosbag filter progress meter to use raw uncompressed input size (`#857 <https://github.com/ros/ros_comm/issues/857>`_)
+
+1.12.2 (2016-06-03)
+-------------------
+
+1.12.1 (2016-04-18)
+-------------------
+* promote the result of read_messages to a namedtuple (`#777 <https://github.com/ros/ros_comm/pull/777>`_)
+* use directory specific compiler flags (`#785 <https://github.com/ros/ros_comm/pull/785>`_)
+
+1.12.0 (2016-03-18)
+-------------------
+* add missing parameter to AdvertiseOptions::createAdvertiseOptions (`#733 <https://github.com/ros/ros_comm/issues/733>`_)
+
+1.11.18 (2016-03-17)
+--------------------
+
+1.11.17 (2016-03-11)
+--------------------
+* use boost::make_shared instead of new for constructing boost::shared_ptr (`#740 <https://github.com/ros/ros_comm/issues/740>`_)
+
+1.11.16 (2015-11-09)
+--------------------
+* show size unit for --size of rosbag record in help string (`#697 <https://github.com/ros/ros_comm/pull/697>`_)
+
+1.11.15 (2015-10-13)
+--------------------
+* add option --prefix for prefixing output topics (`#626 <https://github.com/ros/ros_comm/pull/626>`_)
+
+1.11.14 (2015-09-19)
+--------------------
+* reduce memory usage by using slots for IndexEntry types (`#613 <https://github.com/ros/ros_comm/pull/613>`_)
+* remove duplicate topics (`#647 <https://github.com/ros/ros_comm/issues/647>`_)
+* better exception when calling get_start_time / get_end_time on empty bags (`#657 <https://github.com/ros/ros_comm/pull/657>`_)
+* make support for lz4 in rosbag optional (`#642 <https://github.com/ros/ros_comm/pull/642>`_)
+* fix handling of "play --topics" (`#620 <https://github.com/ros/ros_comm/issues/620>`_)
+
+1.11.13 (2015-04-28)
+--------------------
+
+1.11.12 (2015-04-27)
+--------------------
+
+1.11.11 (2015-04-16)
+--------------------
+* add support for pausing when specified topics are about to be published (`#569 <https://github.com/ros/ros_comm/pull/569>`_)
+
+1.11.10 (2014-12-22)
+--------------------
+* add option to specify the minimum disk space at which recording is stopped (`#500 <https://github.com/ros/ros_comm/pull/500>`_)
+* add convenience API to Python rosbag (`#508 <https://github.com/ros/ros_comm/issues/508>`_)
+* fix delay on detecting a running rosmaster with use_sim_time set (`#532 <https://github.com/ros/ros_comm/pull/532>`_)
+
+1.11.9 (2014-08-18)
+-------------------
+
+1.11.8 (2014-08-04)
+-------------------
+
+1.11.7 (2014-07-18)
+-------------------
+
+1.11.6 (2014-07-10)
+-------------------
+* fix rosbag record prefix (`#449 <https://github.com/ros/ros_comm/issues/449>`_)
+
+1.11.5 (2014-06-24)
+-------------------
+* Fix typo in rosbag usage
+
+1.11.4 (2014-06-16)
+-------------------
+* Python 3 compatibility (`#426 <https://github.com/ros/ros_comm/issues/426>`_, `#430 <https://github.com/ros/ros_comm/issues/430>`_)
+
+1.11.3 (2014-05-21)
+-------------------
+
+1.11.2 (2014-05-08)
+-------------------
+
+1.11.1 (2014-05-07)
+-------------------
+* add lz4 compression to rosbag (Python and C++) (`#356 <https://github.com/ros/ros_comm/issues/356>`_)
+* fix rosbag record --node (`#357 <https://github.com/ros/ros_comm/issues/357>`_)
+* move rosbag dox to rosbag_storage (`#389 <https://github.com/ros/ros_comm/issues/389>`_)
+
+1.11.0 (2014-03-04)
+-------------------
+* use catkin_install_python() to install Python scripts (`#361 <https://github.com/ros/ros_comm/issues/361>`_)
+
+1.10.0 (2014-02-11)
+-------------------
+* remove use of __connection header
+
+1.9.54 (2014-01-27)
+-------------------
+* readd missing declaration of rosbag::createAdvertiseOptions (`#338 <https://github.com/ros/ros_comm/issues/338>`_)
+
+1.9.53 (2014-01-14)
+-------------------
+
+1.9.52 (2014-01-08)
+-------------------
+
+1.9.51 (2014-01-07)
+-------------------
+* move several client library independent parts from ros_comm into roscpp_core, split rosbag storage specific stuff from client library usage (`#299 <https://github.com/ros/ros_comm/issues/299>`_)
+* fix return value on platforms where char is unsigned.
+* fix usage of boost include directories
+
+1.9.50 (2013-10-04)
+-------------------
+* add chunksize option to rosbag record
+
+1.9.49 (2013-09-16)
+-------------------
+
+1.9.48 (2013-08-21)
+-------------------
+* search for exported rosbag migration rules based on new package rosbag_migration_rule
+
+1.9.47 (2013-07-03)
+-------------------
+
+1.9.46 (2013-06-18)
+-------------------
+* fix crash in bag migration (`#239 <https://github.com/ros/ros_comm/issues/239>`_)
+
+1.9.45 (2013-06-06)
+-------------------
+* added option '--duration' to 'rosbag play' (`#121 <https://github.com/ros/ros_comm/issues/121>`_)
+* fix missing newlines in rosbag error messages (`#237 <https://github.com/ros/ros_comm/issues/237>`_)
+* fix flushing for tools like 'rosbag compress' (`#237 <https://github.com/ros/ros_comm/issues/237>`_)
+
+1.9.44 (2013-03-21)
+-------------------
+* fix various issues on Windows (`#189 <https://github.com/ros/ros_comm/issues/189>`_)
+
+1.9.43 (2013-03-13)
+-------------------
+
+1.9.42 (2013-03-08)
+-------------------
+* added option '--duration' to 'rosrun rosbag play' (`#121 <https://github.com/ros/ros_comm/issues/121>`_)
+* add error message to rosbag when using same in and out file (`#171 <https://github.com/ros/ros_comm/issues/171>`_)
+
+1.9.41 (2013-01-24)
+-------------------
+
+1.9.40 (2013-01-13)
+-------------------
+* fix bagsort script (`#42 <https://github.com/ros/ros_comm/issues/42>`_)
+
+1.9.39 (2012-12-29)
+-------------------
+* first public release for Groovy
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/CMakeLists.txt
@@ -0,0 +1,79 @@
+cmake_minimum_required(VERSION 3.0.2)
+project(rosbag)
+
+if(NOT WIN32)
+  set_directory_properties(PROPERTIES COMPILE_OPTIONS "-Wall;-Wextra")
+endif()
+
+find_package(catkin REQUIRED COMPONENTS rosbag_storage rosconsole roscpp std_srvs topic_tools xmlrpcpp)
+find_package(Boost REQUIRED COMPONENTS date_time filesystem program_options regex thread)
+find_package(BZip2 REQUIRED)
+
+catkin_python_setup()
+
+# Support large bags (>2GB) on 32-bit systems
+add_definitions(-D_FILE_OFFSET_BITS=64)
+
+include_directories(include ${catkin_INCLUDE_DIRS} ${Boost_INCLUDE_DIRS}
+  ${BZIP2_INCLUDE_DIR}
+)
+
+catkin_package(
+  LIBRARIES rosbag
+  INCLUDE_DIRS include
+  CATKIN_DEPENDS rosbag_storage rosconsole roscpp std_srvs topic_tools xmlrpcpp)
+
+add_library(rosbag
+  src/player.cpp
+  src/recorder.cpp
+  src/time_translator.cpp)
+
+target_link_libraries(rosbag ${catkin_LIBRARIES} ${Boost_LIBRARIES}
+  ${BZIP2_LIBRARIES}
+)
+
+add_executable(record src/record.cpp)
+target_link_libraries(record rosbag)
+
+add_executable(play src/play.cpp)
+target_link_libraries(play rosbag)
+
+if(NOT WIN32)
+  add_executable(encrypt src/encrypt.cpp)
+  target_link_libraries(encrypt ${catkin_LIBRARIES} ${Boost_LIBRARIES})
+endif()
+
+install(DIRECTORY include/${PROJECT_NAME}/
+  DESTINATION ${CATKIN_PACKAGE_INCLUDE_DESTINATION}
+  FILES_MATCHING PATTERN "*.h")
+install(TARGETS rosbag
+  ARCHIVE DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}
+  LIBRARY DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}
+  RUNTIME DESTINATION ${CATKIN_GLOBAL_BIN_DESTINATION})
+install(TARGETS record play
+  ARCHIVE DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}
+  LIBRARY DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}
+  RUNTIME DESTINATION ${CATKIN_PACKAGE_BIN_DESTINATION})
+if(NOT WIN32)
+  install(TARGETS encrypt
+    ARCHIVE DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}
+    LIBRARY DESTINATION ${CATKIN_PACKAGE_LIB_DESTINATION}
+    RUNTIME DESTINATION ${CATKIN_PACKAGE_BIN_DESTINATION})
+endif()
+catkin_install_python(PROGRAMS
+  scripts/bag2png.py
+  scripts/bagsort.py
+  scripts/fastrebag.py
+  scripts/fixbag.py
+  scripts/fixbag_batch.py
+  scripts/fix_md5sums.py
+  scripts/fix_moved_messages.py
+  scripts/fix_msg_defs.py
+  scripts/makerule.py
+  scripts/savemsg.py
+  scripts/topic_renamer.py
+  DESTINATION ${CATKIN_PACKAGE_BIN_DESTINATION})
+
+if(CATKIN_ENABLE_TESTING)
+  catkin_add_nosetests(test)
+endif()
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/FORMATS
@@ -0,0 +1,215 @@
+Version 0:
+-------------
+topic_name
+md5sum
+datatype
+<repeating N>
+  time.sec
+  time.nsec
+  length
+  MSGDATA
+-------------
+
+Version 1.0:
+-------------
+#ROSLOG V1.0
+quantity
+<repeating quantity>
+  topic_name
+  md5sum
+  datatype
+<repeating N>
+  topic_name
+  time.sec
+  time.nsec
+  length
+  MSGDATA
+-------------
+
+Version 1.1:
+-------------
+#ROSLOG V1.1
+<repeating N>
+  topic_name
+  md5sum
+  datatype
+  time.sec
+  time.nsec
+  length
+  MSGDATA
+-------------
+
+Alternative Version 1.1:
+-------------
+#ROSRECORD V1.1
+<repeating N>
+  topic_name
+  md5sum
+  datatype
+  time.sec
+  time.nsec
+  length
+  MSGDATA
+-------------
+
+Version 1.2:
+-------------
+#ROSRECORD V1.2
+file header
+  op=2
+  index_pos=index_1.pos
+  ---
+  <empty> (padded with ' ' to 4KB)
+
+msg_def_1
+msg_1
+msg_2
+...
+msg_N
+
+index_1
+  ver=0
+  topic=topic
+  count=count
+  ---
+  timestamp_1, uncompressed_byte_offset_1
+  timestamp_2, uncompressed_byte_offset_2
+  ...
+  timestamp_N, uncompressed_byte_offset_N
+
+index_2
+  ...
+
+Version 2.0:
+-------------
+#ROSBAG V2.0
+file_header
+  op=3
+  index_pos=msg_def_1.pos      # position of index
+  conn_count=conn_count        # total number of distinct connections in the bag
+  chunk_count=chunk_count      # total number of chunks in the bag
+  ---
+  <empty> (padded with ' ' to 4KB)
+
+chunk_1
+  op=5
+  compression=bz2              # type of compression (optional)
+  size=<uncompressed bytes>    # size of data when uncompressed (optional)
+  ---
+  conn_1i
+    op=7
+    topic=topic
+    id=0
+    ---
+    topic=topic
+    message_definition=
+    type=
+    md5sum=
+    latching=1
+    callerid=caller1234
+  msg_1
+    op=2
+    topic=...
+    time=...
+    conn=...
+    ---
+    <serialized msg>
+  msg_2
+    op=2
+    topic=...
+    time=...
+    conn=...
+    ---
+    <serialized msg>
+
+  ...
+
+  msg_N
+
+chunk_index_1.0
+  op=4
+  ver=1
+  conn=conn
+  count=count
+  ---
+  timestamp_1, uncompressed_byte_offset_1
+  timestamp_2, uncompressed_byte_offset_2
+  ...
+  timestamp_N, uncompressed_byte_offset_N
+
+chunk_index_1.1
+  op=4
+  ver=1
+  conn=conn
+  count=count
+  ---
+  ...
+
+chunk_2
+  op=5
+  compression=bz2
+  size=<uncompressed bytes>
+  ---
+  msg_N+1
+  msg_N+2
+  ...
+  msg_N+M
+
+chunk_index_2.0
+  op=4
+  ver=1
+  conn=conn
+  count=count
+  ---
+  timestamp_N+1, uncompressed_byte_offset_1
+  timestamp_N+2, uncompressed_byte_offset_2
+  ...
+  timestamp_N+M, uncompressed_byte_offset_N+M
+
+...
+
+conn_1
+  op=7
+  id=0
+  ---
+  latching=1
+  callerid=caller1234 
+
+msg_def_1
+  op=1
+  topic=
+  type=
+  md5sum=
+  def=
+  ---
+  <empty>
+
+msg_def_2
+  op=1
+  topic=
+  type=
+  md5sum=
+  def=
+  ---
+  <empty>
+
+...
+
+chunk_info_1
+  op=6
+  ver=1
+  chunk_pos=chunk_1.pos
+  start=start_stamp_1
+  end=end_stamp_1
+  count=K
+  ---
+  conn_1, count_1
+  conn_2, count_2
+  ...
+  conn_K, count_K
+
+chunk_info_2
+  ...
+
+...
+<eof>
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/examples/write.cpp
@@ -0,0 +1,49 @@
+// Copyright (c) 2010, Willow Garage, Inc.
+// All rights reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//     * Redistributions of source code must retain the above copyright
+//       notice, this list of conditions and the following disclaimer.
+//     * Redistributions in binary form must reproduce the above copyright
+//       notice, this list of conditions and the following disclaimer in the
+//       documentation and/or other materials provided with the distribution.
+//     * Neither the name of Willow Garage, Inc. nor the names of its
+//       contributors may be used to endorse or promote products derived from
+//       this software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
+// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+// POSSIBILITY OF SUCH DAMAGE.
+
+#include "rosbag/bag.h"
+#include "std_msgs/String.h"
+#include "std_msgs/Int32.h"
+
+int main()
+{
+  // %Tag(WRITE)%
+  rosbag::Bag bag;
+  bag.open("test.bag", rosbag::bagmode::Write);
+
+  std_msgs::String str;
+  str.data = std::string("foo");
+
+  std_msgs::Int32 i;
+  i.data = 42;
+
+  bag.write("chatter", ros::Time::now(), str);
+  bag.write("numbers", ros::Time::now(), i);
+
+  bag.close();
+  // %EndTag(WRITE)%
+}
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/examples/write.py
@@ -0,0 +1,48 @@
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2012, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+# %Tag(WRITE_PY)%
+import rosbag
+from std_msgs.msg import Int32, String
+
+bag = rosbag.Bag('test.bag', 'w')
+
+str = String()
+str.data = 'foo'
+
+i = Int32()
+i.data = 42
+
+bag.write('chatter', str);
+bag.write('numbers', i);
+
+bag.close();
+# %EndTag(WRITE_PY)%
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/include/rosbag/player.h
@@ -0,0 +1,245 @@
+/*********************************************************************
+* Software License Agreement (BSD License)
+*
+*  Copyright (c) 2008, Willow Garage, Inc.
+*  All rights reserved.
+*
+*  Redistribution and use in source and binary forms, with or without
+*  modification, are permitted provided that the following conditions
+*  are met:
+*
+*   * Redistributions of source code must retain the above copyright
+*     notice, this list of conditions and the following disclaimer.
+*   * Redistributions in binary form must reproduce the above
+*     copyright notice, this list of conditions and the following
+*     disclaimer in the documentation and/or other materials provided
+*     with the distribution.
+*   * Neither the name of Willow Garage, Inc. nor the names of its
+*     contributors may be used to endorse or promote products derived
+*     from this software without specific prior written permission.
+*
+*  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+*  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+*  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+*  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+*  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+*  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+*  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+*  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+*  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+*  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+*  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+*  POSSIBILITY OF SUCH DAMAGE.
+********************************************************************/
+
+#ifndef ROSBAG_PLAYER_H
+#define ROSBAG_PLAYER_H
+
+#include <sys/stat.h>
+#if !defined(_MSC_VER)
+  #include <termios.h>
+  #include <unistd.h>
+#else
+  #include <windows.h>
+#endif
+#include <time.h>
+
+#include <queue>
+#include <string>
+
+#include <ros/ros.h>
+#include <ros/time.h>
+#include <std_srvs/SetBool.h>
+
+#include "rosbag/bag.h"
+
+#include <topic_tools/shape_shifter.h>
+
+#include "rosbag/time_translator.h"
+#include "rosbag/macros.h"
+
+namespace rosbag {
+
+//! Helper function to create AdvertiseOptions from a MessageInstance
+/*!
+ *  param msg         The Message instance for which to generate adveritse options
+ *  param queue_size  The size of the outgoing queue
+ *  param prefix      An optional prefix for all output topics
+ */
+ros::AdvertiseOptions createAdvertiseOptions(MessageInstance const& msg, uint32_t queue_size, const std::string& prefix = "");
+
+ROSBAG_DECL ros::AdvertiseOptions createAdvertiseOptions(const ConnectionInfo* c, uint32_t queue_size, const std::string& prefix = "");
+
+
+struct ROSBAG_DECL PlayerOptions
+{
+    PlayerOptions();
+
+    void check();
+
+    std::string prefix;
+    bool     quiet;
+    bool     start_paused;
+    bool     at_once;
+    bool     bag_time;
+    double   bag_time_frequency;
+    double   time_scale;
+    int      queue_size;
+    ros::WallDuration advertise_sleep;
+    bool     try_future;
+    bool     has_time;
+    bool     loop;
+    float    time;
+    bool     has_duration;
+    float    duration;
+    bool     keep_alive;
+    bool     wait_for_subscribers;
+    std::string rate_control_topic;
+    float    rate_control_max_delay;
+    ros::Duration skip_empty;
+
+    std::vector<std::string> bags;
+    std::vector<std::string> topics;
+    std::vector<std::string> pause_topics;
+};
+
+
+//! PRIVATE. A helper class to track relevant state for publishing time
+class ROSBAG_DECL TimePublisher {
+public:
+    /*! Create a time publisher
+     *  A publish_frequency of < 0 indicates that time shouldn't actually be published
+     */
+    TimePublisher();
+
+    void setPublishFrequency(double publish_frequency);
+    
+    void setTimeScale(double time_scale);
+
+    /*! Set the horizon that the clock will run to */
+    void setHorizon(const ros::Time& horizon);
+
+    /*! Set the horizon that the clock will run to */
+    void setWCHorizon(const ros::WallTime& horizon);
+
+    /*! Set the current time */
+    void setTime(const ros::Time& time);
+
+    /*! Get the current time */
+    ros::Time const& getTime() const;
+
+    /*! Run the clock for AT MOST duration
+     *
+     * If horizon has been reached this function returns immediately
+     */
+    void runClock(const ros::WallDuration& duration);
+
+    //! Sleep as necessary, but don't let the click run 
+    void runStalledClock(const ros::WallDuration& duration);
+
+    //! Step the clock to the horizon
+    void stepClock();
+
+    bool horizonReached();
+
+private:
+    bool do_publish_;
+    
+    double publish_frequency_;
+    double time_scale_;
+    
+    ros::NodeHandle node_handle_;
+    ros::Publisher time_pub_;
+    
+    ros::WallDuration wall_step_;
+    
+    ros::WallTime next_pub_;
+
+    ros::WallTime wc_horizon_;
+    ros::Time horizon_;
+    ros::Time current_;
+};
+
+
+//! PRIVATE.  Player class to abstract the interface to the player
+/*!
+ *  This API is currently considered private, but will be released in the 
+ * future after view.
+ */
+class ROSBAG_DECL Player
+{
+public:
+    Player(PlayerOptions const& options);
+    ~Player();
+
+    void publish();
+
+private:
+    int readCharFromStdin();
+    void setupTerminal();
+    void restoreTerminal();
+
+    void updateRateTopicTime(const ros::MessageEvent<topic_tools::ShapeShifter const>& msg_event);
+
+    void advertise(const ConnectionInfo* c);
+
+    void doPublish(rosbag::MessageInstance const& m);
+
+    void doKeepAlive();
+
+    void printTime();
+
+    bool pauseCallback(std_srvs::SetBool::Request &req, std_srvs::SetBool::Response &res);
+
+    void processPause(const bool paused, ros::WallTime &horizon);
+
+    void waitForSubscribers() const;
+
+private:
+    typedef std::map<std::string, ros::Publisher> PublisherMap;
+
+    PlayerOptions options_;
+
+    ros::NodeHandle node_handle_;
+
+    ros::ServiceServer pause_service_;
+
+    bool paused_;
+    bool delayed_;
+
+    bool pause_for_topics_;
+
+    bool pause_change_requested_;
+
+    bool requested_pause_state_;
+
+    ros::Subscriber rate_control_sub_;
+    ros::Time last_rate_control_;
+
+    ros::WallTime paused_time_;
+
+    std::vector<boost::shared_ptr<Bag> >  bags_;
+    PublisherMap publishers_;
+
+    // Terminal
+    bool    terminal_modified_;
+#if defined(_MSC_VER)
+    HANDLE input_handle;
+    DWORD stdin_set;
+#else
+    termios orig_flags_;
+    fd_set  stdin_fdset_;
+#endif
+    int     maxfd_;
+
+    TimeTranslator time_translator_;
+    TimePublisher time_publisher_;
+
+    ros::Time start_time_;
+    ros::Duration bag_length_;
+};
+
+
+} // namespace rosbag
+
+#endif
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/include/rosbag/recorder.h
@@ -0,0 +1,200 @@
+/*********************************************************************
+* Software License Agreement (BSD License)
+*
+*  Copyright (c) 2008, Willow Garage, Inc.
+*  All rights reserved.
+*
+*  Redistribution and use in source and binary forms, with or without
+*  modification, are permitted provided that the following conditions
+*  are met:
+*
+*   * Redistributions of source code must retain the above copyright
+*     notice, this list of conditions and the following disclaimer.
+*   * Redistributions in binary form must reproduce the above
+*     copyright notice, this list of conditions and the following
+*     disclaimer in the documentation and/or other materials provided
+*     with the distribution.
+*   * Neither the name of Willow Garage, Inc. nor the names of its
+*     contributors may be used to endorse or promote products derived
+*     from this software without specific prior written permission.
+*
+*  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+*  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+*  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+*  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+*  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+*  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+*  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+*  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+*  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+*  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+*  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+*  POSSIBILITY OF SUCH DAMAGE.
+********************************************************************/
+
+#ifndef ROSBAG_RECORDER_H
+#define ROSBAG_RECORDER_H
+
+#include <sys/stat.h>
+#if !defined(_MSC_VER)
+  #include <termios.h>
+  #include <unistd.h>
+#endif
+#include <time.h>
+
+#include <queue>
+#include <string>
+#include <vector>
+#include <list>
+
+#include <boost/thread/condition.hpp>
+#include <boost/thread/mutex.hpp>
+#include <boost/regex.hpp>
+
+#include <ros/ros.h>
+#include <ros/time.h>
+
+#include <std_msgs/Empty.h>
+#include <std_msgs/String.h>
+#include <topic_tools/shape_shifter.h>
+
+#include "rosbag/bag.h"
+#include "rosbag/stream.h"
+#include "rosbag/macros.h"
+
+namespace rosbag {
+
+class ROSBAG_DECL OutgoingMessage
+{
+public:
+    OutgoingMessage(std::string const& _topic, topic_tools::ShapeShifter::ConstPtr _msg, boost::shared_ptr<ros::M_string> _connection_header, ros::Time _time);
+
+    std::string                         topic;
+    topic_tools::ShapeShifter::ConstPtr msg;
+    boost::shared_ptr<ros::M_string>    connection_header;
+    ros::Time                           time;
+};
+
+class ROSBAG_DECL OutgoingQueue
+{
+public:
+    OutgoingQueue(std::string const& _filename, std::queue<OutgoingMessage>* _queue, ros::Time _time);
+
+    std::string                  filename;
+    std::queue<OutgoingMessage>* queue;
+    ros::Time                    time;
+};
+
+struct ROSBAG_DECL RecorderOptions
+{
+    RecorderOptions();
+
+    bool            trigger;
+    bool            record_all;
+    bool            regex;
+    bool            do_exclude;
+    bool            quiet;
+    bool            append_date;
+    bool            snapshot;
+    bool            verbose;
+    bool            publish;
+    bool            repeat_latched;
+    CompressionType compression;
+    std::string     prefix;
+    std::string     name;
+    boost::regex    exclude_regex;
+    uint32_t        buffer_size;
+    uint32_t        chunk_size;
+    uint32_t        limit;
+    bool            split;
+    uint64_t        max_size;
+    uint32_t        max_splits;
+    ros::Duration   max_duration;
+    std::string     node;
+    unsigned long long min_space;
+    std::string min_space_str;
+    ros::TransportHints transport_hints;
+
+    std::vector<std::string> topics;
+};
+
+class ROSBAG_DECL Recorder
+{
+public:
+    Recorder(RecorderOptions const& options);
+
+    void doTrigger();
+
+    bool isSubscribed(std::string const& topic) const;
+
+    boost::shared_ptr<ros::Subscriber> subscribe(std::string const& topic);
+
+    int run();
+
+private:
+    void printUsage();
+
+    void updateFilenames();
+    void startWriting();
+    void stopWriting();
+
+    bool checkLogging();
+    bool scheduledCheckDisk();
+    bool checkDisk();
+
+    void snapshotTrigger(std_msgs::Empty::ConstPtr trigger);
+    //    void doQueue(topic_tools::ShapeShifter::ConstPtr msg, std::string const& topic, boost::shared_ptr<ros::Subscriber> subscriber, boost::shared_ptr<int> count);
+    void doQueue(const ros::MessageEvent<topic_tools::ShapeShifter const>& msg_event, std::string const& topic, boost::shared_ptr<ros::Subscriber> subscriber, boost::shared_ptr<int> count);
+    void doRecord();
+    void checkNumSplits();
+    bool checkSize();
+    bool checkDuration(const ros::Time&);
+    void doRecordSnapshotter();
+    void doCheckMaster(ros::TimerEvent const& e, ros::NodeHandle& node_handle);
+
+    bool shouldSubscribeToTopic(std::string const& topic, bool from_node = false);
+
+    template<class T>
+    static std::string timeToStr(T ros_t);
+
+private:
+    RecorderOptions               options_;
+
+    Bag                           bag_;
+
+    std::string                   target_filename_;
+    std::string                   write_filename_;
+    std::list<std::string>        current_files_;
+
+    std::set<std::string>         currently_recording_;  //!< set of currently recording topics
+    int                           num_subscribers_;      //!< used for book-keeping of our number of subscribers
+
+    int                           exit_code_;            //!< eventual exit code
+
+    std::map<std::pair<std::string, std::string>, OutgoingMessage> latched_msgs_;
+
+    boost::condition_variable_any queue_condition_;      //!< conditional variable for queue
+    boost::mutex                  queue_mutex_;          //!< mutex for queue
+    std::queue<OutgoingMessage>*  queue_;                //!< queue for storing
+    uint64_t                      queue_size_;           //!< queue size
+    uint64_t                      max_queue_size_;       //!< max queue size
+
+    uint64_t                      split_count_;          //!< split count
+
+    std::queue<OutgoingQueue>     queue_queue_;          //!< queue of queues to be used by the snapshot recorders
+
+    ros::Time                     last_buffer_warn_;
+
+    ros::Time                     start_time_;
+
+    bool                          writing_enabled_;
+    boost::mutex                  check_disk_mutex_;
+    ros::WallTime                 check_disk_next_;
+    ros::WallTime                 warn_next_;
+
+    ros::Publisher                pub_begin_write;
+};
+
+} // namespace rosbag
+
+#endif
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/include/rosbag/time_translator.h
@@ -0,0 +1,74 @@
+/*********************************************************************
+* Software License Agreement (BSD License)
+*
+*  Copyright (c) 2010, Willow Garage, Inc.
+*  All rights reserved.
+*
+*  Redistribution and use in source and binary forms, with or without
+*  modification, are permitted provided that the following conditions
+*  are met:
+*
+*   * Redistributions of source code must retain the above copyright
+*     notice, this list of conditions and the following disclaimer.
+*   * Redistributions in binary form must reproduce the above
+*     copyright notice, this list of conditions and the following
+*     disclaimer in the documentation and/or other materials provided
+*     with the distribution.
+*   * Neither the name of Willow Garage, Inc. nor the names of its
+*     contributors may be used to endorse or promote products derived
+*     from this software without specific prior written permission.
+*
+*  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+*  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+*  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+*  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+*  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+*  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+*  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+*  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+*  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+*  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+*  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+*  POSSIBILITY OF SUCH DAMAGE.
+*********************************************************************/
+
+#ifndef ROSBAG_TIME_TRANSLATOR_H
+#define ROSBAG_TIME_TRANSLATOR_H
+
+#include "ros/time.h"
+#include "rosbag/macros.h"
+
+namespace rosbag {
+
+//! Helper class for translating between two times
+/*!
+ * The time translator can be configured with a Real start time, a
+ * Translated start time, and a time scale.
+ * 
+ * It will convert a time from a series starting at realStartTime to a
+ * comparable time series instead starting at translatedStartTime.
+ * All durations in the time-sequence as scaled by 1/(timeScale).
+ *
+ * That is, a time-sequence with time-scale 2 will finish twice as
+ * quickly.
+ */
+class ROSBAG_DECL TimeTranslator
+{
+public:
+    TimeTranslator();
+
+    void      setTimeScale(double const& s);
+    void      setRealStartTime(ros::Time const& t);
+    void      setTranslatedStartTime(ros::Time const& t);  //!< Increments the translated start time by shift.  Useful for pausing.
+    void      shift(ros::Duration const& d);               //!< Increments the translated start time by shift.  Useful for pausing.
+    ros::Time translate(ros::Time const& t);
+
+private:
+    double    time_scale_;
+    ros::Time real_start_;
+    ros::Time translated_start_;
+};
+
+} // namespace rosbag
+
+#endif
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/mainpage.dox
@@ -0,0 +1,9 @@
+/**
+\mainpage
+\htmlinclude manifest.html
+
+\b rosbag is a set of tools and API's for recording/writing messages to bag files and playing/reading them back.
+
+Most code (not relying on the ROS client library) has been moved to the <a href="../../../rosbag_storage/html/c++/">rosbag_storage</a> package.
+
+*/
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/package.xml
@@ -0,0 +1,60 @@
+<?xml version="1.0"?>
+<?xml-model
+  href="http://download.ros.org/schema/package_format3.xsd"
+  schematypens="http://www.w3.org/2001/XMLSchema"?>
+<package format="3">
+  <name>rosbag</name>
+  <version>1.16.0</version>
+  <description>
+    This is a set of tools for recording from and playing back to ROS
+    topics.  It is intended to be high performance and avoids
+    deserialization and reserialization of the messages. 
+  </description>
+  <maintainer email="michael@openrobotics.org">Michael Carroll</maintainer>
+  <maintainer email="sloretz@openrobotics.org">Shane Loretz</maintainer>
+  <license>BSD</license>
+
+  <url type="website">http://wiki.ros.org/rosbag</url>
+  <url type="bugtracker">https://github.com/ros/ros_comm/issues</url>
+  <url type="repository">https://github.com/ros/ros_comm</url>
+  <author>Tim Field</author>
+  <author>Jeremy Leibs</author>
+  <author>James Bowman</author>
+  <author email="dthomas@osrfoundation.org">Dirk Thomas</author>
+  <author email="jacob@openrobotics.org">Jacob Perron</author>
+
+  <depend>libboost-date-time-dev</depend>
+  <depend>libboost-filesystem-dev</depend>
+  <depend>libboost-program-options-dev</depend>
+  <depend>libboost-regex-dev</depend>
+  <depend>libboost-thread-dev</depend>
+  <depend>rosbag_storage</depend>
+  <depend>rosconsole</depend>
+  <depend>roscpp</depend>
+  <depend>std_srvs</depend>
+  <depend>xmlrpcpp</depend>
+
+  <buildtool_depend version_gte="0.5.78">catkin</buildtool_depend>
+
+  <build_depend>cpp_common</build_depend>
+  <build_depend condition="$ROS_PYTHON_VERSION == 2">python-imaging</build_depend>
+  <build_depend condition="$ROS_PYTHON_VERSION == 3">python3-pil</build_depend>
+  <build_depend>roscpp_serialization</build_depend>
+  <build_depend>topic_tools</build_depend>
+
+  <exec_depend>genmsg</exec_depend>
+  <exec_depend>genpy</exec_depend>
+  <exec_depend condition="$ROS_PYTHON_VERSION == 2">python-pycryptodome</exec_depend>
+  <exec_depend condition="$ROS_PYTHON_VERSION == 3">python3-pycryptodome</exec_depend>
+  <exec_depend condition="$ROS_PYTHON_VERSION == 2">python-gnupg</exec_depend>
+  <exec_depend condition="$ROS_PYTHON_VERSION == 3">python3-gnupg</exec_depend>
+  <exec_depend condition="$ROS_PYTHON_VERSION == 2">python-rospkg</exec_depend>
+  <exec_depend condition="$ROS_PYTHON_VERSION == 3">python3-rospkg</exec_depend>
+  <exec_depend>roslib</exec_depend>
+  <exec_depend>rospy</exec_depend>
+  <exec_depend>topic_tools</exec_depend>
+
+  <export>
+    <rosdoc config="${prefix}/rosdoc.yaml"/>
+  </export>
+</package>
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/rosdoc.yaml
@@ -0,0 +1,6 @@
+ - builder: epydoc
+   output_dir: python
+ - builder: doxygen
+   name: C++ API
+   output_dir: c++
+   file_patterns: '*.c *.cpp *.h *.cc *.hh *.dox'
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/scripts/bag2png.py
@@ -0,0 +1,69 @@
+#!/usr/bin/env python
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2009, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+import sys
+import array
+import Image
+
+import rospy
+import rosbag
+
+def int16_str(d):
+    return array.array('B', [ min(x, 255) for x in d ]).tostring()
+    #return array.array('f', [ float(x) for x in d ]).tostring()
+
+def msg2im(msg):
+    """Take an sensor_msgs/Image and return a PIL image"""
+    if len(msg.uint8_data.data) == 0 and len(msg.int16_data.data) == 0:
+        return None
+    
+    if msg.depth == 'uint8':
+        ma, image_data = msg.uint8_data, ma.data
+    else:
+        ma, image_data = msg.int16_data, int16_str(ma.data)
+        
+    dim = dict([(d.label, d.size) for d in ma.layout.dim])
+    mode = { ('uint8',1) : "L", ('uint8',3) : "RGB", ('int16',1) : "L" }[msg.depth, dim['channel']]
+    (w, h) = (dim['width'], dim['height'])
+
+    return Image.fromstring(mode, (w, h), image_data)
+
+counter = 0
+for topic, msg, t in rosbag.Bag(sys.argv[1]).read_messages():
+    if topic.endswith('stereo/raw_stereo'):
+        for (mi, c) in [ (msg.left_image, 'L'), (msg.right_image, 'R'), (msg.disparity_image, 'D')]:
+            im = msg2im(mi)
+            if im:
+                ext = { 'L':'png', 'RGB':'png', 'F':'tiff' }[im.mode]
+                im.save('%06d%s.%s' % (counter, c, ext))
+        counter += 1
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/scripts/bagsort.py
@@ -0,0 +1,67 @@
+#!/usr/bin/env python
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2009, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+from __future__ import print_function
+
+import rospy
+import rosbag
+
+def sortbags(inbag, outbag):
+    rebag = rosbag.Bag(outbag, 'w')
+
+    try:
+        schedule = [(t, i) for i, (topic, msg, t) in enumerate(rosbag.Bag(inbag).read_messages(raw=True))]
+
+        schedule = [i for (t, i) in sorted(schedule)]
+        print(schedule)
+    
+        stage = {}
+        for i, (topic, msg, t) in enumerate(rosbag.Bag(inbag).read_messages(raw=True)):
+            stage[i] = (topic, msg, t)
+            while (len(schedule) > 0) and (schedule[0] in stage):
+                (topic, msg, t) = stage[schedule[0]]
+                rebag.write(topic, msg, t, raw=True)
+                del stage[schedule[0]]
+                schedule = schedule[1:]
+
+        assert schedule == []
+        assert stage == {}
+    finally:
+        rebag.close()
+
+if __name__ == '__main__':
+    import sys
+    if len(sys.argv) == 3:
+        sortbags(sys.argv[1], sys.argv[2])
+    else:
+        print("usage: bagsort.py <inbag> <outbag>")
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/scripts/fastrebag.py
@@ -0,0 +1,49 @@
+#!/usr/bin/env python
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2009, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+from __future__ import print_function
+
+import rosbag
+
+def fastrebag(inbag, outbag):
+    rebag = rosbag.Bag(outbag, 'w')
+    for i, (topic, msg, t) in enumerate(rosbag.Bag(inbag).read_messages(raw=True)):
+        rebag.write(topic, msg, t, raw=True)
+    rebag.close()
+
+if __name__ == '__main__':
+    import sys
+    if len(sys.argv) == 3:
+        fastrebag(sys.argv[1], sys.argv[2])
+    else:
+        print('usage: fastrebag.py <inbag> <outbag>')
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/scripts/fix_md5sums.py
@@ -0,0 +1,69 @@
+#!/usr/bin/env python
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2009, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+from __future__ import print_function
+
+import os
+import rospy
+import rosbag
+
+def fix_md5sums(inbags):
+    for b in inbags:
+        print('Trying to migrating file: %s' % b)
+        outbag = b + '.tmp'
+        rebag = rosbag.Bag(outbag, 'w')
+        try:
+            for i,(topic, msg, t) in enumerate(rosbag.Bag(b).read_messages(raw=True)):
+                rebag.write(topic, msg, t, raw=True)
+            rebag.close()
+        except rosbag.ROSBagException as e:
+            print(' Migration failed: %s' % str(e))
+            os.remove(outbag)
+            continue
+        
+        oldnamebase = b + '.old'
+        oldname = oldnamebase
+        i = 1
+        while os.path.isfile(oldname):
+            i += 1
+            oldname = oldnamebase + str(i)
+        os.rename(b, oldname)
+        os.rename(outbag, b)
+        print(' Migration successful.  Original stored as: %s' % oldname)
+
+if __name__ == '__main__':
+    import sys
+    if len(sys.argv) >= 2:
+        fix_md5sums(sys.argv[1:])
+    else:
+        print("usage: fix_md5sums.py bag1 [bag2 bag3 ...]")
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/scripts/fix_moved_messages.py
@@ -0,0 +1,67 @@
+#!/usr/bin/env python
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2009, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+from __future__ import print_function
+
+import rospy
+import rosbag
+import fileinput
+
+def fixbags(md5file, inbag, outbag):
+    d = {}
+    for line in fileinput.input(md5file):
+        sp = line.split()
+        d[sp[1]] = [sp[0], sp[2], sp[3]]
+
+    rebag = rosbag.Bag(outbag, 'w')
+
+    for topic, msg, t in rosbag.Bag(inbag).read_messages(raw=True):
+        type, bytes, md5 = msg[0], msg[1], msg[2]
+
+        if md5 in d:
+            if type != d[md5][0]:
+                print('WARNING: found matching md5, but non-matching name')
+                continue
+            msg = (d[md5][1], msg[1], d[md5][2])
+
+        rebag.write(topic, msg, t, raw=True)
+
+    rebag.close()
+
+if __name__ == '__main__':
+    import sys
+    if len(sys.argv) == 4:
+        fixbags(sys.argv[1], sys.argv[2], sys.argv[3])
+    else:
+        print('usage: fix_moved_messages.py <name_md5_file> <inbag> <outbag>')
+        exit(2)
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/scripts/fix_msg_defs.py
@@ -0,0 +1,80 @@
+#!/usr/bin/env python
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2009, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+from __future__ import print_function
+
+import sys
+import rosbag.migration
+import roslib.message
+
+if __name__ == '__main__':
+    if len(sys.argv) != 3:
+        print('usage: fix_msg_defs.py <inbag> <outbag>')
+        exit(2)
+
+    mm = rosbag.migration.MessageMigrator()
+
+    checked = set()
+    migrations = []
+
+    inbag = rosbag.Bag(sys.argv[1], 'r')
+    outbag = rosbag.Bag(sys.argv[2], 'w')
+    lookup_cache = {}
+
+    #msg is: datatype, data, pytype._md5sum, bag_pos, pytype
+    for topic, msg, t in inbag.read_messages(raw=True):
+        if msg[4]._md5sum != msg[2]:
+            k = (msg[0], msg[2])
+            if k in lookup_cache:
+                real_msg_type = lookup_cache[k]
+            else:
+                real_msg_type = mm.lookup_type(k)
+                if real_msg_type != None:
+                    print("FOUND: %s [%s] was defined in migration system\n"%(msg[0], msg[2]), file=sys.stderr)
+                else:
+                    systype = roslib.message.get_message_class(msg[0])
+                    if systype != None and systype._md5sum == msg[2]:
+                        real_msg_type = systype
+                        print("FOUND: %s [%s] was defined on your package path\n"%(msg[0], msg[2]), file=sys.stderr)
+                if real_msg_type == None:
+                    real_msg_type = msg[4]
+                    print("WARNING: Type [%s] with md5sum [%s] has an unknown definition.\n"%(msg[0], msg[2]), file=sys.stderr)
+                lookup_cache[k] = real_msg_type
+            outbag.write(topic, (msg[0], msg[1], msg[2], msg[3], real_msg_type), t, raw=True)
+        else:
+            outbag.write(topic, msg, t, raw=True)
+        
+    inbag.close()
+    outbag.close()
+
+    exit(0)
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/scripts/fixbag.py
@@ -0,0 +1,54 @@
+#!/usr/bin/env python
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2009, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+from __future__ import print_function
+
+import sys
+import rosbag.migration
+
+if __name__ == '__main__':
+    if len(sys.argv) < 3:
+        print('usage: fixbag.py <inbag> <outbag> [rulefile1, rulefile2, ...]')
+        exit(2)
+
+    if sys.argv[2].split('.')[-1] == 'bmr':
+        print('Second argument should be a bag, not a rule file.', file=sys.stderr)
+        exit(2)
+
+    mm = rosbag.migration.MessageMigrator(sys.argv[3:])
+    if not rosbag.migration.fixbag(mm, sys.argv[1], sys.argv[2]):
+        print('Bag could not be migrated.')
+        exit(1)
+
+    print('Bag migrated successfully.')
+    exit(0)
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/scripts/fixbag_batch.py
@@ -0,0 +1,67 @@
+#!/usr/bin/env python
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2009, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+from __future__ import print_function
+
+import os
+import sys
+import rosbag.migration
+
+def fixbag_batch(inbags):
+    mm = rosbag.migration.MessageMigrator()
+
+    for b in inbags:
+        print('Trying to migrate: %s' % b)
+        outbag = b + '.tmp'
+        if not rosbag.migration.fixbag(mm, b, outbag):
+            os.remove(outbag)
+            print(' Migration failed.')
+            continue
+
+        oldnamebase = b + '.old'
+        oldname = oldnamebase
+        i = 1
+        while os.path.isfile(oldname):
+            i += 1
+            oldname = oldnamebase + str(i)
+        os.rename(b, oldname)
+        os.rename(outbag, b)
+        print(' Migration successful.  Original stored as: %s' % oldname)
+
+if __name__ == '__main__':
+    import sys
+    if len(sys.argv) >= 2:
+        fixbag_batch(sys.argv[1:])
+    else:
+        print('usage: fixbag_batch.py bag1 [bag2 bag3 ...]')
+        exit(2)
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/scripts/makerule.py
@@ -0,0 +1,157 @@
+#!/usr/bin/env python
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2009, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+from __future__ import print_function
+
+import optparse
+import os
+import re
+import sys
+import rosbag.migration
+
+import genpy.message
+import genpy.dynamic
+
+def print_trans(old, new, indent):
+    from_txt = '%s [%s]' % (old._type, old._md5sum)
+    if new is not None:
+        to_txt= '%s [%s]' % (new._type, new._md5sum)
+    else:
+        to_txt = 'Unknown'
+    print('    ' * indent + ' * From: %s' % from_txt)
+    print('    ' * indent + '   To:   %s' % to_txt)
+
+if __name__ == '__main__':
+    parser = optparse.OptionParser(usage='usage: makerule.py msg.saved [-a] output_rulefile [rulefile1, rulefile2, ...] [-n]')
+    parser.add_option('-a', '--append',    action='store_true', dest='append',    default=False)
+    parser.add_option('-n', '--noplugins', action='store_true', dest='noplugins', default=False)
+    (options, args) = parser.parse_args()
+
+    if len(args) < 2:
+        parser.error("Incorrect number of arguments")
+
+    rulefile = args[1]
+
+    if os.path.isfile(rulefile) and not options.append:
+        print("The file %s already exists.  Include -a if you intend to append." % rulefile, file=sys.stderr)
+        exit(1)
+
+    if not os.path.isfile(rulefile) and options.append:
+        print("The file %s does not exist, and so -a is invalid." % rulefile, file=sys.stderr)
+        exit(1)
+
+    if options.append:
+        append_rule = [rulefile]
+    else:
+        append_rule = []
+
+    f = open(args[0])
+    if f is None:
+        print('Could not open message full definition: %s', file=sys.stderr)
+        sys.exit()
+
+    type_line = f.readline()
+    pat = re.compile(r"\[(.*)]:")
+    type_match = pat.match(type_line)
+    if type_match is None:
+        print("Full definition file malformed.  First line should be: '[my_package/my_msg]:'", file=sys.stderr)
+        sys.exit()
+
+    old_type = type_match.groups()[0]
+    old_full_text = f.read()
+    f.close()
+
+    old_class = genpy.dynamic.generate_dynamic(old_type,old_full_text)[old_type]
+
+    if old_class is None:
+        print('Could not generate class from full definition file.', file=sys.stderr)
+        sys.exit()
+
+    mm = rosbag.migration.MessageMigrator(args[2:]+append_rule,not options.noplugins)
+
+    migrations = rosbag.migration.checkmessages(mm, [old_class])
+
+    if migrations == []:
+        print('Saved definition is up to date.')
+        exit(0)
+
+    print('The following migrations need to occur:')
+
+    all_rules = []
+    for m in migrations:
+        all_rules.extend(m[1])
+
+        print_trans(m[0][0].old_class, m[0][-1].new_class, 0)
+        if len(m[1]) > 0:
+            print("    %d rules missing:" % len(m[1]))
+            for r in m[1]:
+                print_trans(r.old_class, r.new_class, 1)
+
+    if rulefile is None:
+        print("rulefile not specified")
+    else:
+        output = ''
+        rules_left = mm.filter_rules_unique(all_rules)
+
+        if rules_left == []:
+            print("\nNo additional rule files needed to be generated.  %s not created." % rulefile)
+            exit(0)
+
+        while rules_left != []:
+            extra_rules = []
+
+            for r in rules_left:
+                if r.new_class is None:
+                    print("The message type %s appears to have moved.  Please enter the type to migrate it to." % r.old_class._type)
+                    new_type = input('>')
+                    new_class = genpy.message.get_message_class(new_type)
+                    while new_class is None:
+                        print("\'%s\' could not be found in your system.  Please make sure it is built." % new_type)
+                        new_type = input('>')
+                        new_class = genpy.message.get_message_class(new_type)
+                    new_rule = mm.make_update_rule(r.old_class, new_class)
+                    R = new_rule(mm, 'GENERATED.' + new_rule.__name__)
+                    R.find_sub_paths()
+                    new_rules = [r for r in mm.expand_rules(R.sub_rules) if r.valid == False]
+                    extra_rules.extend(new_rules)
+                    print('Creating the migration rule for %s requires additional missing rules:' % new_type)
+                    for nr in new_rules:
+                        print_trans(nr.old_class, nr.new_class,1)
+                    output += R.get_class_def()
+                else:
+                    output += r.get_class_def()
+            rules_left = mm.filter_rules_unique(extra_rules)
+        f = open(rulefile, 'a')
+        f.write(output)
+        f.close()
+        print("\nThe necessary rule files have been written to: %s" % rulefile)
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/scripts/rosbag
@@ -0,0 +1,35 @@
+#!/usr/bin/env python
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2008, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+import rosbag
+rosbag.rosbagmain()
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/scripts/savemsg.py
@@ -0,0 +1,64 @@
+#!/usr/bin/env python
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2009, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+from __future__ import print_function
+
+import optparse
+import sys
+import roslib.message
+import rosbag
+
+if __name__ == '__main__':
+    parser = optparse.OptionParser(usage='usage: savemsg.py [-b <bagfile] type')
+    parser.add_option('-b', '--bagfiles', action='store', dest='bagfile', default=None, help='Save message from a bagfile rather than system definition')
+
+    (options, args) = parser.parse_args()
+
+    if len(args) < 1:
+        parser.error('Message type not specified.')
+
+    if options.bagfile is None:
+        sys_class = roslib.message.get_message_class(args[0])
+        if sys_class is None:
+            print('Could not find message %s.' % args[0], file=sys.stderr)
+        else:
+            print('[%s]:' % args[0])
+            print(sys_class._full_text)
+    else:
+        for topic, msg, t in rosbag.Bag(options.bagfile).read_messages(raw=True):
+            if msg[0] == args[0]:
+                print('[%s]:' % args[0])
+                print(msg[4]._full_text)
+                exit(0)
+
+        print('Could not find message %s in bag %s.' % (args[0], options.bagfile), file=sys.stderr)
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/scripts/topic_renamer.py
@@ -0,0 +1,50 @@
+#!/usr/bin/env python
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2009, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+from __future__ import print_function
+
+import rospy
+import rosbag
+
+def rename_topic(intopic, inbag, outtopic, outbag):
+    rebag = rosbag.Bag(outbag, 'w')
+    for topic, msg, t in rosbag.Bag(inbag).read_messages(raw=True):
+        rebag.write(outtopic if topic == intopic else topic, msg, t, raw=True)
+    rebag.close()
+
+if __name__ == '__main__':
+    import sys
+    if len(sys.argv) == 5:
+        rename_topic(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])
+    else:
+        print("usage: topic_renamer.py <intopic> <inbag> <outtopic> <outbag>")
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/setup.py
@@ -0,0 +1,11 @@
+from setuptools import setup
+from catkin_pkg.python_setup import generate_distutils_setup
+
+d = generate_distutils_setup(
+    packages=['rosbag'],
+    package_dir={'': 'src'},
+    scripts=['scripts/rosbag'],
+    requires=['genmsg', 'genpy', 'roslib', 'rospkg']
+)
+
+setup(**d)
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/src/encrypt.cpp
@@ -0,0 +1,198 @@
+/*********************************************************************
+* Software License Agreement (BSD License)
+*
+*  Copyright (c) 2017, Open Source Robotics Foundation
+*  All rights reserved.
+*
+*  Redistribution and use in source and binary forms, with or without
+*  modification, are permitted provided that the following conditions
+*  are met:
+*
+*   * Redistributions of source code must retain the above copyright
+*     notice, this list of conditions and the following disclaimer.
+*   * Redistributions in binary form must reproduce the above
+*     copyright notice, this list of conditions and the following
+*     disclaimer in the documentation and/or other materials provided
+*     with the distribution.
+*   * Neither the name of Willow Garage, Inc. nor the names of its
+*     contributors may be used to endorse or promote products derived
+*     from this software without specific prior written permission.
+*
+*  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+*  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+*  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+*  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+*  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+*  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+*  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+*  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+*  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+*  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+*  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+*  POSSIBILITY OF SUCH DAMAGE.
+*********************************************************************/
+
+#include <iostream>
+
+#include <boost/scoped_ptr.hpp>
+#include <boost/program_options.hpp>
+#include <boost/progress.hpp>
+#include <boost/regex.hpp>
+
+#include <ros/ros.h>
+
+#include "rosbag/bag.h"
+#include "rosbag/view.h"
+
+namespace po = boost::program_options;
+
+struct EncryptorOptions
+{
+    EncryptorOptions() : quiet(false), compression(rosbag::compression::Uncompressed) { }
+
+    void buildOutbagName();
+
+    bool quiet;
+    std::string plugin;
+    std::string param;
+    rosbag::CompressionType compression;
+    std::string inbag;
+    std::string outbag;
+};
+
+void EncryptorOptions::buildOutbagName()
+{
+    if (!outbag.empty())
+        return;
+    if (inbag.empty())
+        throw ros::Exception("Input bag is not specified.");
+    std::string::size_type pos = inbag.find_last_of('.');
+    if (pos == std::string::npos)
+        throw ros::Exception("Input bag name has no extension.");
+    outbag = inbag.substr(0, pos) + std::string(".out") + inbag.substr(pos);
+}
+
+//! Parse the command-line arguments for encrypt options
+EncryptorOptions parseOptions(int argc, char** argv)
+{
+    EncryptorOptions opts;
+
+    po::options_description desc("Allowed options");
+
+    desc.add_options()
+      ("help,h",   "produce help message")
+      ("quiet,q",  "suppress console output")
+      ("plugin,p", po::value<std::string>()->default_value("rosbag/AesCbcEncryptor"), "encryptor name")
+      ("param,r",  po::value<std::string>()->default_value("*"), "encryptor parameter")
+      ("bz2,j",    "use BZ2 compression")
+      ("lz4",      "use lz4 compression")
+      ("inbag",    po::value<std::string>(), "bag file to encrypt")
+      ("outbag,o", po::value<std::string>(), "bag file encrypted")
+      ;
+
+    po::positional_options_description p;
+    p.add("inbag", -1);
+
+    po::variables_map vm;
+
+    try
+    {
+        po::store(po::command_line_parser(argc, argv).options(desc).positional(p).run(), vm);
+    }
+    catch (const boost::program_options::invalid_command_line_syntax& e)
+    {
+        throw ros::Exception(e.what());
+    }
+    catch (const boost::program_options::unknown_option& e)
+    {
+        throw ros::Exception(e.what());
+    }
+
+    if (vm.count("help"))
+    {
+        std::cout << desc << std::endl;
+        exit(0);
+    }
+
+    if (vm.count("quiet"))
+        opts.quiet = true;
+    if (vm.count("plugin"))
+        opts.plugin = vm["plugin"].as<std::string>();
+    if (vm.count("param"))
+        opts.param = vm["param"].as<std::string>();
+    if (vm.count("bz2"))
+        opts.compression = rosbag::compression::BZ2;
+    if (vm.count("lz4"))
+        opts.compression = rosbag::compression::LZ4;
+    if (vm.count("inbag"))
+        opts.inbag = vm["inbag"].as<std::string>();
+    else
+      throw ros::Exception("You must specify bag to encrypt.");
+    if (vm.count("outbag"))
+        opts.outbag = vm["outbag"].as<std::string>();
+    opts.buildOutbagName();
+
+    return opts;
+}
+
+std::string getStringCompressionType(rosbag::CompressionType compression)
+{
+    switch(compression)
+    {
+    case rosbag::compression::Uncompressed: return "none";
+    case rosbag::compression::BZ2: return "bz2";
+    case rosbag::compression::LZ4: return "lz4";
+    default: return "Unknown";
+    }
+}
+
+int encrypt(EncryptorOptions const& options)
+{
+    if (!options.quiet)
+    {
+        std::cout << "Output bag:  " << options.outbag << "\n";
+        std::cout << "Encryption:  " << options.plugin << ":" << options.param << "\n";
+        std::cout << "Compression: " << getStringCompressionType(options.compression) << "\n";
+    }
+    rosbag::Bag inbag(options.inbag, rosbag::bagmode::Read);
+    rosbag::Bag outbag(options.outbag, rosbag::bagmode::Write);
+    // Compression type is per chunk, and cannot be retained.
+    // If chunk-by-chunk encryption is implemented, compression type could be honored.
+    outbag.setEncryptorPlugin(options.plugin, options.param);
+    outbag.setCompression(options.compression);
+    rosbag::View view(inbag);
+    boost::scoped_ptr<boost::progress_display> progress;
+    if (!options.quiet)
+        progress.reset(new boost::progress_display(view.size(), std::cout, "Progress:\n  ", "  ", "  "));
+    for (rosbag::View::const_iterator it = view.begin(); it != view.end(); ++it)
+    {
+        outbag.write(it->getTopic(), it->getTime(), *it, it->getConnectionHeader());
+        if (progress)
+            ++(*progress);
+    }
+    outbag.close();
+    inbag.close();
+    return 0;
+}
+
+int main(int argc, char** argv)
+{
+    // Parse the command-line options
+    EncryptorOptions opts;
+    try
+    {
+        opts = parseOptions(argc, argv);
+    }
+    catch (const ros::Exception& ex)
+    {
+        ROS_ERROR("Error reading options: %s", ex.what());
+        return 1;
+    }
+    catch (const boost::regex_error& ex)
+    {
+        ROS_ERROR("Error reading options: %s\n", ex.what());
+        return 1;
+    }
+
+    return encrypt(opts);
+}
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/src/play.cpp
@@ -0,0 +1,193 @@
+/*********************************************************************
+* Software License Agreement (BSD License)
+*
+*  Copyright (c) 2008, Willow Garage, Inc.
+*  All rights reserved.
+*
+*  Redistribution and use in source and binary forms, with or without
+*  modification, are permitted provided that the following conditions
+*  are met:
+*
+*   * Redistributions of source code must retain the above copyright
+*     notice, this list of conditions and the following disclaimer.
+*   * Redistributions in binary form must reproduce the above
+*     copyright notice, this list of conditions and the following
+*     disclaimer in the documentation and/or other materials provided
+*     with the distribution.
+*   * Neither the name of Willow Garage, Inc. nor the names of its
+*     contributors may be used to endorse or promote products derived
+*     from this software without specific prior written permission.
+*
+*  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+*  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+*  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+*  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+*  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+*  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+*  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+*  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+*  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+*  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+*  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+*  POSSIBILITY OF SUCH DAMAGE.
+********************************************************************/
+
+#include "rosbag/player.h"
+#include "boost/program_options.hpp"
+
+namespace po = boost::program_options;
+
+rosbag::PlayerOptions parseOptions(int argc, char** argv) {
+    rosbag::PlayerOptions opts;
+
+    po::options_description desc("Allowed options");
+
+    desc.add_options()
+      ("help,h", "produce help message")
+      ("prefix,p", po::value<std::string>()->default_value(""), "prefixes all output topics in replay")
+      ("quiet,q", "suppress console output")
+      ("immediate,i", "play back all messages without waiting")
+      ("pause", "start in paused mode")
+      ("queue", po::value<int>()->default_value(100), "use an outgoing queue of size SIZE")
+      ("clock", "publish the clock time")
+      ("hz", po::value<float>()->default_value(100.0f), "use a frequency of HZ when publishing clock time")
+      ("delay,d", po::value<float>()->default_value(0.2f), "sleep SEC seconds after every advertise call (to allow subscribers to connect)")
+      ("rate,r", po::value<float>()->default_value(1.0f), "multiply the publish rate by FACTOR")
+      ("start,s", po::value<float>()->default_value(0.0f), "start SEC seconds into the bag files")
+      ("duration,u", po::value<float>(), "play only SEC seconds from the bag files")
+      ("skip-empty", po::value<float>(), "skip regions in the bag with no messages for more than SEC seconds")
+      ("loop,l", "loop playback")
+      ("keep-alive,k", "keep alive past end of bag (useful for publishing latched topics)")
+      ("try-future-version", "still try to open a bag file, even if the version is not known to the player")
+      ("topics", po::value< std::vector<std::string> >()->multitoken(), "topics to play back")
+      ("pause-topics", po::value< std::vector<std::string> >()->multitoken(), "topics to pause playback on")
+      ("bags", po::value< std::vector<std::string> >(), "bag files to play back from")
+      ("wait-for-subscribers", "wait for at least one subscriber on each topic before publishing")
+      ("rate-control-topic", po::value<std::string>(), "watch the given topic, and if the last publish was more than <rate-control-max-delay> ago, wait until the topic publishes again to continue playback")
+      ("rate-control-max-delay", po::value<float>()->default_value(1.0f), "maximum time difference from <rate-control-topic> before pausing")
+      ;
+
+    po::positional_options_description p;
+    p.add("bags", -1);
+    
+    po::variables_map vm;
+    
+    try 
+    {
+      po::store(po::command_line_parser(argc, argv).options(desc).positional(p).run(), vm);
+    } catch (const boost::program_options::invalid_command_line_syntax& e)
+    {
+      throw ros::Exception(e.what());
+    } catch (const boost::program_options::unknown_option& e)
+    {
+      throw ros::Exception(e.what());
+    }
+
+    if (vm.count("help")) {
+      std::cout << desc << std::endl;
+      exit(0);
+    }
+
+    if (vm.count("prefix"))
+      opts.prefix = vm["prefix"].as<std::string>();
+    if (vm.count("quiet"))
+      opts.quiet = true;
+    if (vm.count("immediate"))
+      opts.at_once = true;
+    if (vm.count("pause"))
+      opts.start_paused = true;
+    if (vm.count("queue"))
+      opts.queue_size = vm["queue"].as<int>();
+    if (vm.count("hz"))
+      opts.bag_time_frequency = vm["hz"].as<float>();
+    if (vm.count("clock"))
+      opts.bag_time = true;
+    if (vm.count("delay"))
+      opts.advertise_sleep = ros::WallDuration(vm["delay"].as<float>());
+    if (vm.count("rate"))
+      opts.time_scale = vm["rate"].as<float>();
+    if (vm.count("start"))
+    {
+      opts.time = vm["start"].as<float>();
+      opts.has_time = true;
+    }
+    if (vm.count("duration"))
+    {
+      opts.duration = vm["duration"].as<float>();
+      opts.has_duration = true;
+    }
+    if (vm.count("skip-empty"))
+      opts.skip_empty = ros::Duration(vm["skip-empty"].as<float>());
+    if (vm.count("loop"))
+      opts.loop = true;
+    if (vm.count("keep-alive"))
+      opts.keep_alive = true;
+    if (vm.count("wait-for-subscribers"))
+      opts.wait_for_subscribers = true;
+
+    if (vm.count("topics"))
+    {
+      std::vector<std::string> topics = vm["topics"].as< std::vector<std::string> >();
+      for (std::vector<std::string>::iterator i = topics.begin();
+           i != topics.end();
+           i++)
+        opts.topics.push_back(*i);
+    }
+
+    if (vm.count("pause-topics"))
+    {
+      std::vector<std::string> pause_topics = vm["pause-topics"].as< std::vector<std::string> >();
+      for (std::vector<std::string>::iterator i = pause_topics.begin();
+           i != pause_topics.end();
+           i++)
+        opts.pause_topics.push_back(*i);
+    }
+
+    if (vm.count("rate-control-topic"))
+      opts.rate_control_topic = vm["rate-control-topic"].as<std::string>();
+
+    if (vm.count("rate-control-max-delay"))
+      opts.rate_control_max_delay = vm["rate-control-max-delay"].as<float>();
+
+    if (vm.count("bags"))
+    {
+      std::vector<std::string> bags = vm["bags"].as< std::vector<std::string> >();
+      for (std::vector<std::string>::iterator i = bags.begin();
+           i != bags.end();
+           i++)
+          opts.bags.push_back(*i);
+    } else {
+      if (vm.count("topics") || vm.count("pause-topics"))
+        throw ros::Exception("When using --topics or --pause-topics, --bags "
+          "should be specified to list bags.");
+      throw ros::Exception("You must specify at least one bag to play back.");
+    }
+            
+    return opts;
+}
+
+int main(int argc, char** argv) {
+    ros::init(argc, argv, "play", ros::init_options::AnonymousName);
+
+    // Parse the command-line options
+    rosbag::PlayerOptions opts;
+    try {
+        opts = parseOptions(argc, argv);
+    }
+    catch (const ros::Exception& ex) {
+        ROS_ERROR("Error reading options: %s", ex.what());
+        return 1;
+    }
+
+    rosbag::Player player(opts);
+
+    try {
+      player.publish();
+    }
+    catch (const std::runtime_error& e) {
+      ROS_FATAL("%s", e.what());
+      return 1;
+    }
+    
+    return 0;
+}
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/src/player.cpp
@@ -0,0 +1,924 @@
+/*********************************************************************
+* Software License Agreement (BSD License)
+*
+*  Copyright (c) 2008, Willow Garage, Inc.
+*  All rights reserved.
+*
+*  Redistribution and use in source and binary forms, with or without
+*  modification, are permitted provided that the following conditions
+*  are met:
+*
+*   * Redistributions of source code must retain the above copyright
+*     notice, this list of conditions and the following disclaimer.
+*   * Redistributions in binary form must reproduce the above
+*     copyright notice, this list of conditions and the following
+*     disclaimer in the documentation and/or other materials provided
+*     with the distribution.
+*   * Neither the name of Willow Garage, Inc. nor the names of its
+*     contributors may be used to endorse or promote products derived
+*     from this software without specific prior written permission.
+*
+*  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+*  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+*  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+*  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+*  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+*  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+*  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+*  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+*  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+*  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+*  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+*  POSSIBILITY OF SUCH DAMAGE.
+********************************************************************/
+
+#include "rosbag/player.h"
+#include "rosbag/message_instance.h"
+#include "rosbag/view.h"
+
+#if !defined(_MSC_VER)
+  #include <sys/select.h>
+#endif
+
+#include <boost/format.hpp>
+
+#include "rosgraph_msgs/Clock.h"
+
+#include <set>
+
+using std::map;
+using std::pair;
+using std::string;
+using std::vector;
+using boost::shared_ptr;
+using ros::Exception;
+
+namespace rosbag {
+
+bool isLatching(const ConnectionInfo* c)
+{
+    ros::M_string::const_iterator header_iter = c->header->find("latching");
+    return (header_iter != c->header->end() && header_iter->second == "1");
+}
+
+ros::AdvertiseOptions createAdvertiseOptions(const ConnectionInfo* c, uint32_t queue_size, const std::string& prefix) {
+    ros::AdvertiseOptions opts(prefix + c->topic, queue_size, c->md5sum, c->datatype, c->msg_def);
+    opts.latch = isLatching(c);
+    return opts;
+}
+
+
+ros::AdvertiseOptions createAdvertiseOptions(MessageInstance const& m, uint32_t queue_size, const std::string& prefix) {
+    return ros::AdvertiseOptions(prefix + m.getTopic(), queue_size, m.getMD5Sum(), m.getDataType(), m.getMessageDefinition());
+}
+
+// PlayerOptions
+
+PlayerOptions::PlayerOptions() :
+    prefix(""),
+    quiet(false),
+    start_paused(false),
+    at_once(false),
+    bag_time(false),
+    bag_time_frequency(0.0),
+    time_scale(1.0),
+    queue_size(0),
+    advertise_sleep(0.2),
+    try_future(false),
+    has_time(false),
+    loop(false),
+    time(0.0f),
+    has_duration(false),
+    duration(0.0f),
+    keep_alive(false),
+    wait_for_subscribers(false),
+    rate_control_topic(""),
+    rate_control_max_delay(1.0f),
+    skip_empty(ros::DURATION_MAX)
+{
+}
+
+void PlayerOptions::check() {
+    if (bags.size() == 0)
+        throw Exception("You must specify at least one bag file to play from");
+    if (has_duration && duration <= 0.0)
+        throw Exception("Invalid duration, must be > 0.0");
+}
+
+// Player
+
+Player::Player(PlayerOptions const& options) :
+    options_(options),
+    paused_(options.start_paused),
+    // If we were given a list of topics to pause on, then go into that mode
+    // by default (it can be toggled later via 't' from the keyboard).
+    pause_for_topics_(options_.pause_topics.size() > 0),
+    pause_change_requested_(false),
+    requested_pause_state_(false),
+    terminal_modified_(false)
+{
+  ros::NodeHandle private_node_handle("~");
+  pause_service_ = private_node_handle.advertiseService("pause_playback", &Player::pauseCallback, this);
+}
+
+Player::~Player() {
+    for (shared_ptr<Bag>& bag : bags_)
+        bag->close();
+
+    restoreTerminal();
+}
+
+void Player::publish() {
+    options_.check();
+
+    // Open all the bag files
+    for (string const& filename : options_.bags) {
+        ROS_INFO("Opening %s", filename.c_str());
+
+        try
+        {
+            shared_ptr<Bag> bag(boost::make_shared<Bag>());
+            bag->open(filename, bagmode::Read);
+            bags_.push_back(bag);
+        }
+        catch (const BagUnindexedException& ex) {
+            std::cerr << "Bag file " << filename << " is unindexed.  Run rosbag reindex." << std::endl;
+            return;
+        }
+    }
+
+    setupTerminal();
+
+    if (!node_handle_.ok())
+      return;
+
+    if (!options_.prefix.empty())
+    {
+      ROS_INFO_STREAM("Using prefix '" << options_.prefix << "'' for topics ");
+    }
+
+    if (!options_.quiet)
+      puts("");
+    
+    // Publish all messages in the bags
+    View full_view;
+    for (shared_ptr<Bag>& bag : bags_)
+        full_view.addQuery(*bag);
+
+    const auto full_initial_time = full_view.getBeginTime();
+
+    const auto initial_time = full_initial_time + ros::Duration(options_.time);
+
+    ros::Time finish_time = ros::TIME_MAX;
+    if (options_.has_duration)
+    {
+      finish_time = initial_time + ros::Duration(options_.duration);
+    }
+
+    View view;
+    TopicQuery topics(options_.topics);
+
+    if (options_.topics.empty())
+    {
+      for (shared_ptr<Bag>& bag : bags_)
+        view.addQuery(*bag, initial_time, finish_time);
+    } else {
+      for (shared_ptr<Bag>& bag : bags_)
+        view.addQuery(*bag, topics, initial_time, finish_time);
+    }
+
+    if (view.size() == 0)
+    {
+      std::cerr << "No messages to play on specified topics.  Exiting." << std::endl;
+      ros::shutdown();
+      return;
+    }
+
+    // Advertise all of our messages
+    for (const ConnectionInfo* c : view.getConnections())
+    {
+        advertise(c);
+    }
+
+    if (options_.rate_control_topic != "")
+    {
+        std::cout << "Creating rate control topic subscriber..." << std::flush;
+
+        boost::shared_ptr<ros::Subscriber> sub(boost::make_shared<ros::Subscriber>());
+        ros::SubscribeOptions ops;
+        ops.topic = options_.rate_control_topic;
+        ops.queue_size = 10;
+        ops.md5sum = ros::message_traits::md5sum<topic_tools::ShapeShifter>();
+        ops.datatype = ros::message_traits::datatype<topic_tools::ShapeShifter>();
+        ops.helper = boost::make_shared<ros::SubscriptionCallbackHelperT<
+            const ros::MessageEvent<topic_tools::ShapeShifter const> &> >(
+                boost::bind(&Player::updateRateTopicTime, this, boost::placeholders::_1));
+
+        rate_control_sub_ = node_handle_.subscribe(ops);
+
+        std::cout << " done." << std::endl;
+    }
+
+
+    std::cout << "Waiting " << options_.advertise_sleep.toSec() << " seconds after advertising topics..." << std::flush;
+    options_.advertise_sleep.sleep();
+    std::cout << " done." << std::endl;
+
+    std::cout << std::endl << "Hit space to toggle paused, or 's' to step." << std::endl;
+
+    // Publish last message from latch topics if the options_.time > 0.0:
+    if (options_.time > 0.0) {
+        // Retrieve all the latch topics before the initial time and create publishers if needed:
+        View full_latch_view;
+
+        if (options_.topics.empty()) {
+            for (const auto& bag : bags_) {
+                full_latch_view.addQuery(*bag, full_initial_time, initial_time);
+            }
+        } else {
+            for (const auto& bag : bags_) {
+                full_latch_view.addQuery(*bag, topics, full_initial_time, initial_time);
+            }
+        }
+
+        std::set<std::pair<std::string, std::string>> latch_topics;
+        for (const auto& c : full_latch_view.getConnections()) {
+            if (isLatching(c)) {
+                const auto header_iter = c->header->find("callerid");
+                const auto callerid = (header_iter != c->header->end() ? header_iter->second : string(""));
+
+                latch_topics.emplace(callerid, c->topic);
+
+                advertise(c);
+            }
+        }
+
+        if (options_.wait_for_subscribers){
+            waitForSubscribers();
+        }
+
+        // Publish the last message of each latch topic per callerid:
+        for (const auto& item : latch_topics) {
+            const auto& callerid = item.first;
+            const auto& topic = item.second;
+
+            View latch_view;
+            for (const auto& bag : bags_) {
+                latch_view.addQuery(*bag, TopicQuery(topic), full_initial_time, initial_time);
+            }
+
+            auto last_message = latch_view.end();
+            for (auto iter = latch_view.begin(); iter != latch_view.end(); ++iter) {
+                if (iter->getCallerId() == callerid) {
+                    last_message = iter;
+                }
+            }
+
+            if (last_message != latch_view.end()) {
+                const auto publisher = publishers_.find(callerid + topic);
+                ROS_ASSERT(publisher != publishers_.end());
+
+                publisher->second.publish(*last_message);
+            }
+        }
+    } else if (options_.wait_for_subscribers) {
+        waitForSubscribers();
+    }
+
+    while (true) {
+        // Set up our time_translator and publishers
+
+        time_translator_.setTimeScale(options_.time_scale);
+
+        start_time_ = view.begin()->getTime();
+        time_translator_.setRealStartTime(start_time_);
+        bag_length_ = view.getEndTime() - view.getBeginTime();
+
+        // Set the last rate control to now, so the program doesn't start delayed.
+        last_rate_control_ = start_time_;
+
+        time_publisher_.setTime(start_time_);
+
+        ros::WallTime now_wt = ros::WallTime::now();
+        time_translator_.setTranslatedStartTime(ros::Time(now_wt.sec, now_wt.nsec));
+
+
+        time_publisher_.setTimeScale(options_.time_scale);
+        if (options_.bag_time)
+            time_publisher_.setPublishFrequency(options_.bag_time_frequency);
+        else
+            time_publisher_.setPublishFrequency(-1.0);
+
+        paused_time_ = now_wt;
+
+        // Call do-publish for each message
+        for (const MessageInstance& m : view) {
+            if (!node_handle_.ok())
+                break;
+
+            doPublish(m);
+        }
+
+        if (options_.keep_alive)
+            while (node_handle_.ok())
+                doKeepAlive();
+
+        if (!node_handle_.ok()) {
+            std::cout << std::endl;
+            break;
+        }
+        if (!options_.loop) {
+            std::cout << std::endl << "Done." << std::endl;
+            break;
+        }
+    }
+
+    ros::shutdown();
+}
+
+void Player::updateRateTopicTime(const ros::MessageEvent<topic_tools::ShapeShifter const>& msg_event)
+{
+    boost::shared_ptr<topic_tools::ShapeShifter const> const &ssmsg = msg_event.getConstMessage();
+    std::string def = ssmsg->getMessageDefinition();
+    size_t length = ros::serialization::serializationLength(*ssmsg);
+    
+    // Check the message definition.
+    std::istringstream f(def);
+    std::string s;
+    bool flag = false;
+    while(std::getline(f, s, '\n')) {
+        if (!s.empty() && s.find("#") != 0) {
+            // Does not start with #, is not a comment.
+            if (s.find("Header ") == 0) {
+                flag = true;
+            }
+            break;
+        }
+    }
+    // If the header is not the first element in the message according to the definition, throw an error.
+    if (!flag) {
+        std::cout << std::endl << "WARNING: Rate control topic is bad, header is not first. MSG may be malformed." << std::endl;
+        return;
+    }
+
+    std::vector<uint8_t> buffer(length);
+    ros::serialization::OStream ostream(&buffer[0], length);
+    ros::serialization::Serializer<topic_tools::ShapeShifter>::write(ostream, *ssmsg);
+
+    // Assuming that the header is the first several bytes of the message.
+    //uint32_t header_sequence_id   = buffer[0] | (uint32_t)buffer[1] << 8 | (uint32_t)buffer[2] << 16 | (uint32_t)buffer[3] << 24;
+    int32_t header_timestamp_sec  = buffer[4] | (uint32_t)buffer[5] << 8 | (uint32_t)buffer[6] << 16 | (uint32_t)buffer[7] << 24;
+    int32_t header_timestamp_nsec = buffer[8] | (uint32_t)buffer[9] << 8 | (uint32_t)buffer[10] << 16 | (uint32_t)buffer[11] << 24;
+
+    last_rate_control_ = ros::Time(header_timestamp_sec, header_timestamp_nsec);
+}
+
+void Player::printTime()
+{
+    if (!options_.quiet) {
+
+        ros::Time current_time = time_publisher_.getTime();
+        ros::Duration d = current_time - start_time_;
+
+
+        if (paused_)
+        {
+            printf("\r [PAUSED ]  Bag Time: %13.6f   Duration: %.6f / %.6f               \r", time_publisher_.getTime().toSec(), d.toSec(), bag_length_.toSec());
+        }
+        else if (delayed_)
+        {
+            ros::Duration time_since_rate = std::max(ros::Time::now() - last_rate_control_, ros::Duration(0));
+            printf("\r [DELAYED]  Bag Time: %13.6f   Duration: %.6f / %.6f   Delay: %.2f \r", time_publisher_.getTime().toSec(), d.toSec(), bag_length_.toSec(), time_since_rate.toSec());
+        }
+        else
+        {
+            printf("\r [RUNNING]  Bag Time: %13.6f   Duration: %.6f / %.6f               \r", time_publisher_.getTime().toSec(), d.toSec(), bag_length_.toSec());
+        }
+        fflush(stdout);
+    }
+}
+
+bool Player::pauseCallback(std_srvs::SetBool::Request &req, std_srvs::SetBool::Response &res)
+{
+  pause_change_requested_ = (req.data != paused_);
+  requested_pause_state_ = req.data;
+
+  res.success = pause_change_requested_;
+
+  if (res.success)
+  {
+    res.message = std::string("Playback is now ") + (requested_pause_state_ ? "paused" : "resumed");
+  }
+  else
+  {
+    res.message = std::string("Bag is already ") + (requested_pause_state_ ? "paused." : "running.");
+  }
+
+  return true;
+}
+
+void Player::processPause(const bool paused, ros::WallTime &horizon)
+{
+  paused_ = paused;
+
+  if (paused_)
+  {
+    paused_time_ = ros::WallTime::now();
+  }
+  else
+  {
+    // Make sure time doesn't shift after leaving pause.
+    ros::WallDuration shift = ros::WallTime::now() - paused_time_;
+    paused_time_ = ros::WallTime::now();
+
+    time_translator_.shift(ros::Duration(shift.sec, shift.nsec));
+
+    horizon += shift;
+    time_publisher_.setWCHorizon(horizon);
+  }
+}
+
+void Player::waitForSubscribers() const
+{
+    bool all_topics_subscribed = false;
+    std::cout << "Waiting for subscribers." << std::endl;
+    while (!all_topics_subscribed) {
+        all_topics_subscribed = std::all_of(
+            std::begin(publishers_), std::end(publishers_),
+            [](const PublisherMap::value_type& pub) {
+                return pub.second.getNumSubscribers() > 0;
+            });
+        ros::WallDuration(0.1).sleep();
+    }
+    std::cout << "Finished waiting for subscribers." << std::endl;
+}
+
+void Player::advertise(const ConnectionInfo* c)
+{
+    ros::M_string::const_iterator header_iter = c->header->find("callerid");
+    std::string callerid = (header_iter != c->header->end() ? header_iter->second : string(""));
+
+    string callerid_topic = callerid + c->topic;
+
+    map<string, ros::Publisher>::iterator pub_iter = publishers_.find(callerid_topic);
+    if (pub_iter == publishers_.end()) {
+        ros::AdvertiseOptions opts = createAdvertiseOptions(c, options_.queue_size, options_.prefix);
+
+        ros::Publisher pub = node_handle_.advertise(opts);
+        publishers_.insert(publishers_.begin(), pair<string, ros::Publisher>(callerid_topic, pub));
+
+        pub_iter = publishers_.find(callerid_topic);
+    }
+}
+
+void Player::doPublish(MessageInstance const& m) {
+    string const& topic   = m.getTopic();
+    ros::Time const& time = m.getTime();
+    string callerid       = m.getCallerId();
+    
+    ros::Time translated = time_translator_.translate(time);
+    ros::WallTime horizon = ros::WallTime(translated.sec, translated.nsec);
+
+    time_publisher_.setHorizon(time);
+    time_publisher_.setWCHorizon(horizon);
+
+    string callerid_topic = callerid + topic;
+
+    map<string, ros::Publisher>::iterator pub_iter = publishers_.find(callerid_topic);
+    ROS_ASSERT(pub_iter != publishers_.end());
+
+    // Update subscribers.
+    ros::spinOnce();
+
+    // If immediate specified, play immediately
+    if (options_.at_once) {
+        time_publisher_.stepClock();
+        pub_iter->second.publish(m);
+        printTime();
+        return;
+    }
+
+    // If skip_empty is specified, skip this region and shift.
+    if (time - time_publisher_.getTime() > options_.skip_empty)
+    {
+      time_publisher_.stepClock();
+
+      ros::WallDuration shift = ros::WallTime::now() - horizon ;
+      time_translator_.shift(ros::Duration(shift.sec, shift.nsec));
+      horizon += shift;
+      time_publisher_.setWCHorizon(horizon);
+      (pub_iter->second).publish(m);
+      printTime();
+      return;
+    }
+
+    if (pause_for_topics_)
+    {
+        for (std::vector<std::string>::iterator i = options_.pause_topics.begin();
+             i != options_.pause_topics.end();
+             ++i)
+        {
+            if (topic == *i)
+            {
+                paused_ = true;
+                paused_time_ = ros::WallTime::now();
+            }
+        }
+    }
+
+    // Check if the rate control topic has posted recently enough to continue, or if a delay is needed.
+    // Delayed is separated from paused to allow more verbose printing.
+    if (rate_control_sub_ != NULL) {
+        if ((time_publisher_.getTime() - last_rate_control_).toSec() > options_.rate_control_max_delay) {
+            delayed_ = true;
+            paused_time_ = ros::WallTime::now();
+        }
+    }
+
+    while ((paused_ || delayed_ || !time_publisher_.horizonReached()) && node_handle_.ok())
+    {
+        bool charsleftorpaused = true;
+        while (charsleftorpaused && node_handle_.ok())
+        {
+            ros::spinOnce();
+
+            if (pause_change_requested_)
+            {
+              processPause(requested_pause_state_, horizon);
+              pause_change_requested_ = false;
+            }
+
+            switch (readCharFromStdin()){
+            case ' ':
+                processPause(!paused_, horizon);
+                break;
+            case 's':
+                if (paused_) {
+                    time_publisher_.stepClock();
+
+                    ros::WallDuration shift = ros::WallTime::now() - horizon ;
+                    paused_time_ = ros::WallTime::now();
+
+                    time_translator_.shift(ros::Duration(shift.sec, shift.nsec));
+
+                    horizon += shift;
+                    time_publisher_.setWCHorizon(horizon);
+            
+                    (pub_iter->second).publish(m);
+
+                    printTime();
+                    return;
+                }
+                break;
+            case 't':
+                pause_for_topics_ = !pause_for_topics_;
+                break;
+            case EOF:
+                if (paused_)
+                {
+                    printTime();
+                    time_publisher_.runStalledClock(ros::WallDuration(.1));
+                    ros::spinOnce();
+                }
+                else if (delayed_)
+                {
+                    printTime();
+                    time_publisher_.runStalledClock(ros::WallDuration(.1));
+                    ros::spinOnce();
+                    // You need to check the rate here too.
+                    if(rate_control_sub_ == NULL || (time_publisher_.getTime() - last_rate_control_).toSec() <= options_.rate_control_max_delay) {
+                        delayed_ = false;
+                        // Make sure time doesn't shift after leaving delay.
+                        ros::WallDuration shift = ros::WallTime::now() - paused_time_;
+                        paused_time_ = ros::WallTime::now();
+         
+                        time_translator_.shift(ros::Duration(shift.sec, shift.nsec));
+
+                        horizon += shift;
+                        time_publisher_.setWCHorizon(horizon);
+                    }
+                }
+                else
+                    charsleftorpaused = false;
+            }
+        }
+
+        printTime();
+        time_publisher_.runClock(ros::WallDuration(.1));
+        ros::spinOnce();
+    }
+
+    pub_iter->second.publish(m);
+}
+
+
+void Player::doKeepAlive() {
+    //Keep pushing ourself out in 10-sec increments (avoids fancy math dealing with the end of time)
+    ros::Time const& time = time_publisher_.getTime() + ros::Duration(10.0);
+
+    ros::Time translated = time_translator_.translate(time);
+    ros::WallTime horizon = ros::WallTime(translated.sec, translated.nsec);
+
+    time_publisher_.setHorizon(time);
+    time_publisher_.setWCHorizon(horizon);
+
+    if (options_.at_once) {
+        return;
+    }
+
+    // If we're done and just staying alive, don't watch the rate control topic. We aren't publishing anyway.
+    delayed_ = false;
+
+    while ((paused_ || !time_publisher_.horizonReached()) && node_handle_.ok())
+    {
+        bool charsleftorpaused = true;
+        while (charsleftorpaused && node_handle_.ok())
+        {
+            switch (readCharFromStdin()){
+            case ' ':
+                paused_ = !paused_;
+                if (paused_) {
+                    paused_time_ = ros::WallTime::now();
+                }
+                else
+                {
+                    // Make sure time doesn't shift after leaving pause.
+                    ros::WallDuration shift = ros::WallTime::now() - paused_time_;
+                    paused_time_ = ros::WallTime::now();
+         
+                    time_translator_.shift(ros::Duration(shift.sec, shift.nsec));
+
+                    horizon += shift;
+                    time_publisher_.setWCHorizon(horizon);
+                }
+                break;
+            case EOF:
+                if (paused_)
+                {
+                    printTime();
+                    time_publisher_.runStalledClock(ros::WallDuration(.1));
+                    ros::spinOnce();
+                }
+                else
+                    charsleftorpaused = false;
+            }
+        }
+
+        printTime();
+        time_publisher_.runClock(ros::WallDuration(.1));
+        ros::spinOnce();
+    }
+}
+
+
+
+void Player::setupTerminal() {
+    if (terminal_modified_)
+        return;
+
+#if defined(_MSC_VER)
+    input_handle = GetStdHandle(STD_INPUT_HANDLE);
+    if (input_handle == INVALID_HANDLE_VALUE)
+    {
+        std::cout << "Failed to set up standard input handle." << std::endl;
+        return;
+    }
+    if (! GetConsoleMode(input_handle, &stdin_set) )
+    {
+        std::cout << "Failed to save the console mode." << std::endl;
+        return;
+    }
+    // don't actually need anything but the default, alternatively try this
+    //DWORD event_mode = ENABLE_WINDOW_INPUT | ENABLE_MOUSE_INPUT;
+    //if (! SetConsoleMode(input_handle, event_mode) )
+    //{
+    // std::cout << "Failed to set the console mode." << std::endl;
+    // return;
+    //}
+    terminal_modified_ = true;
+#else
+    const int fd = fileno(stdin);
+    termios flags;
+    tcgetattr(fd, &orig_flags_);
+    flags = orig_flags_;
+    flags.c_lflag &= ~ICANON;      // set raw (unset canonical modes)
+    flags.c_cc[VMIN]  = 0;         // i.e. min 1 char for blocking, 0 chars for non-blocking
+    flags.c_cc[VTIME] = 0;         // block if waiting for char
+    tcsetattr(fd, TCSANOW, &flags);
+
+    FD_ZERO(&stdin_fdset_);
+    FD_SET(fd, &stdin_fdset_);
+    maxfd_ = fd + 1;
+    terminal_modified_ = true;
+#endif
+}
+
+void Player::restoreTerminal() {
+	if (!terminal_modified_)
+		return;
+
+#if defined(_MSC_VER)
+    SetConsoleMode(input_handle, stdin_set);
+#else
+    const int fd = fileno(stdin);
+    tcsetattr(fd, TCSANOW, &orig_flags_);
+#endif
+    terminal_modified_ = false;
+}
+
+int Player::readCharFromStdin() {
+#ifdef __APPLE__
+    fd_set testfd;
+    FD_COPY(&stdin_fdset_, &testfd);
+#elif !defined(_MSC_VER)
+    fd_set testfd = stdin_fdset_;
+#endif
+
+#if defined(_MSC_VER)
+    DWORD events = 0;
+    INPUT_RECORD input_record[1];
+    DWORD input_size = 1;
+    BOOL b = GetNumberOfConsoleInputEvents(input_handle, &events);
+    if (b && events > 0)
+    {
+        b = ReadConsoleInput(input_handle, input_record, input_size, &events);
+        if (b)
+        {
+            for (unsigned int i = 0; i < events; ++i)
+            {
+                if (input_record[i].EventType & KEY_EVENT & input_record[i].Event.KeyEvent.bKeyDown)
+                {
+                    CHAR ch = input_record[i].Event.KeyEvent.uChar.AsciiChar;
+                    return ch;
+                }
+            }
+        }
+    }
+    return EOF;
+#else
+    timeval tv;
+    tv.tv_sec  = 0;
+    tv.tv_usec = 0;
+    if (select(maxfd_, &testfd, NULL, NULL, &tv) <= 0)
+        return EOF;
+    return getc(stdin);
+#endif
+}
+
+TimePublisher::TimePublisher() : time_scale_(1.0)
+{
+  setPublishFrequency(-1.0);
+  time_pub_ = node_handle_.advertise<rosgraph_msgs::Clock>("clock",1);
+}
+
+void TimePublisher::setPublishFrequency(double publish_frequency)
+{
+  publish_frequency_ = publish_frequency;
+  
+  do_publish_ = (publish_frequency > 0.0);
+
+  wall_step_.fromSec(1.0 / publish_frequency);
+}
+
+void TimePublisher::setTimeScale(double time_scale)
+{
+    time_scale_ = time_scale;
+}
+
+void TimePublisher::setHorizon(const ros::Time& horizon)
+{
+    horizon_ = horizon;
+}
+
+void TimePublisher::setWCHorizon(const ros::WallTime& horizon)
+{
+  wc_horizon_ = horizon;
+}
+
+void TimePublisher::setTime(const ros::Time& time)
+{
+    current_ = time;
+}
+
+ros::Time const& TimePublisher::getTime() const
+{
+    return current_;
+}
+
+void TimePublisher::runClock(const ros::WallDuration& duration)
+{
+    if (do_publish_)
+    {
+        rosgraph_msgs::Clock pub_msg;
+
+        ros::WallTime t = ros::WallTime::now();
+        ros::WallTime done = t + duration;
+
+        while (t < done && t < wc_horizon_)
+        {
+            ros::WallDuration leftHorizonWC = wc_horizon_ - t;
+
+            ros::Duration d(leftHorizonWC.sec, leftHorizonWC.nsec);
+            d *= time_scale_;
+
+            current_ = horizon_ - d;
+
+            if (current_ >= horizon_)
+              current_ = horizon_;
+
+            if (t >= next_pub_)
+            {
+                pub_msg.clock = current_;
+                time_pub_.publish(pub_msg);
+                next_pub_ = t + wall_step_;
+            }
+
+            ros::WallTime target = done;
+            if (target > wc_horizon_)
+              target = wc_horizon_;
+            if (target > next_pub_)
+              target = next_pub_;
+
+            ros::WallTime::sleepUntil(target);
+
+            t = ros::WallTime::now();
+        }
+    } else {
+
+        ros::WallTime t = ros::WallTime::now();
+
+        ros::WallDuration leftHorizonWC = wc_horizon_ - t;
+
+        ros::Duration d(leftHorizonWC.sec, leftHorizonWC.nsec);
+        d *= time_scale_;
+
+        current_ = horizon_ - d;
+        
+        if (current_ >= horizon_)
+            current_ = horizon_;
+
+        ros::WallTime target = ros::WallTime::now() + duration;
+
+        if (target > wc_horizon_)
+            target = wc_horizon_;
+
+        ros::WallTime::sleepUntil(target);
+    }
+}
+
+void TimePublisher::stepClock()
+{
+    if (do_publish_)
+    {
+        current_ = horizon_;
+
+        rosgraph_msgs::Clock pub_msg;
+
+        pub_msg.clock = current_;
+        time_pub_.publish(pub_msg);
+
+        ros::WallTime t = ros::WallTime::now();
+        next_pub_ = t + wall_step_;
+    } else {
+        current_ = horizon_;
+    }
+}
+
+void TimePublisher::runStalledClock(const ros::WallDuration& duration)
+{
+    if (do_publish_)
+    {
+        rosgraph_msgs::Clock pub_msg;
+
+        ros::WallTime t = ros::WallTime::now();
+        ros::WallTime done = t + duration;
+
+        while ( t < done )
+        {
+            if (t > next_pub_)
+            {
+                pub_msg.clock = current_;
+                time_pub_.publish(pub_msg);
+                next_pub_ = t + wall_step_;
+            }
+
+            ros::WallTime target = done;
+
+            if (target > next_pub_)
+              target = next_pub_;
+
+            ros::WallTime::sleepUntil(target);
+
+            t = ros::WallTime::now();
+        }
+    } else {
+        duration.sleep();
+    }
+}
+
+bool TimePublisher::horizonReached()
+{
+  return ros::WallTime::now() > wc_horizon_;
+}
+
+} // namespace rosbag
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/src/record.cpp
@@ -0,0 +1,318 @@
+/*********************************************************************
+* Software License Agreement (BSD License)
+*
+*  Copyright (c) 2008, Willow Garage, Inc.
+*  All rights reserved.
+*
+*  Redistribution and use in source and binary forms, with or without
+*  modification, are permitted provided that the following conditions
+*  are met:
+*
+*   * Redistributions of source code must retain the above copyright
+*     notice, this list of conditions and the following disclaimer.
+*   * Redistributions in binary form must reproduce the above
+*     copyright notice, this list of conditions and the following
+*     disclaimer in the documentation and/or other materials provided
+*     with the distribution.
+*   * Neither the name of Willow Garage, Inc. nor the names of its
+*     contributors may be used to endorse or promote products derived
+*     from this software without specific prior written permission.
+*
+*  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+*  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+*  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+*  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+*  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+*  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+*  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+*  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+*  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+*  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+*  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+*  POSSIBILITY OF SUCH DAMAGE.
+********************************************************************/
+
+#include "rosbag/recorder.h"
+#include "rosbag/exceptions.h"
+
+#include "boost/program_options.hpp"
+#include <signal.h>
+#include <string>
+#include <sstream>
+
+namespace po = boost::program_options;
+
+//! Parse the command-line arguments for recorder options
+rosbag::RecorderOptions parseOptions(int argc, char** argv) {
+    rosbag::RecorderOptions opts;
+
+    po::options_description desc("Allowed options");
+
+    desc.add_options()
+      ("help,h", "produce help message")
+      ("all,a", "record all topics")
+      ("regex,e", "match topics using regular expressions")
+      ("exclude,x", po::value<std::string>(), "exclude topics matching regular expressions")
+      ("quiet,q", "suppress console output")
+      ("publish,p", "Publish a msg when the record begin")
+      ("output-prefix,o", po::value<std::string>(), "prepend PREFIX to beginning of bag name")
+      ("output-name,O", po::value<std::string>(), "record bagnamed NAME.bag")
+      ("buffsize,b", po::value<int>()->default_value(256), "Use an internal buffer of SIZE MB (Default: 256)")
+      ("chunksize", po::value<int>()->default_value(768), "Set chunk size of message data, in KB (Default: 768. Advanced)")
+      ("limit,l", po::value<int>()->default_value(0), "Only record NUM messages on each topic")
+      ("min-space,L", po::value<std::string>()->default_value("1G"), "Minimum allowed space on recording device (use G,M,k multipliers)")
+      ("bz2,j", "use BZ2 compression")
+      ("lz4", "use LZ4 compression")
+      ("split", po::value<int>()->implicit_value(0), "Split the bag file and continue recording when maximum size or maximum duration reached.")
+      ("max-splits", po::value<int>(), "Keep a maximum of N bag files, when reaching the maximum erase the oldest one to keep a constant number of files.")
+      ("topic", po::value< std::vector<std::string> >(), "topic to record")
+      ("size", po::value<uint64_t>(), "The maximum size of the bag to record in MB.")
+      ("duration", po::value<std::string>(), "Record a bag of maximum duration in seconds, unless 'm', or 'h' is appended.")
+      ("node", po::value<std::string>(), "Record all topics subscribed to by a specific node.")
+      ("tcpnodelay", "Use the TCP_NODELAY transport hint when subscribing to topics.")
+      ("udp", "Use the UDP transport hint when subscribing to topics.")
+      ("repeat-latched", "Repeat latched msgs at the start of each new bag file.");
+
+  
+    po::positional_options_description p;
+    p.add("topic", -1);
+    
+    po::variables_map vm;
+    
+    try 
+    {
+      po::store(po::command_line_parser(argc, argv).options(desc).positional(p).run(), vm);
+    } catch (const boost::program_options::invalid_command_line_syntax& e)
+    {
+      throw ros::Exception(e.what());
+    } catch (const boost::program_options::unknown_option& e)
+    {
+      throw ros::Exception(e.what());
+    }
+
+    if (vm.count("help")) {
+      std::cout << desc << std::endl;
+      exit(0);
+    }
+
+    if (vm.count("all"))
+      opts.record_all = true;
+    if (vm.count("regex"))
+      opts.regex = true;
+    if (vm.count("exclude"))
+    {
+      opts.do_exclude = true;
+      opts.exclude_regex = vm["exclude"].as<std::string>();
+    }
+    if (vm.count("quiet"))
+      opts.quiet = true;
+    if (vm.count("publish"))
+      opts.publish = true;
+    if (vm.count("repeat-latched"))
+      opts.repeat_latched = true;
+    if (vm.count("output-prefix"))
+    {
+      opts.prefix = vm["output-prefix"].as<std::string>();
+      opts.append_date = true;
+    }
+    if (vm.count("output-name"))
+    {
+      opts.prefix = vm["output-name"].as<std::string>();
+      opts.append_date = false;
+    }
+    if (vm.count("split"))
+    {
+      opts.split = true;
+
+      int S = vm["split"].as<int>();
+      if (S != 0)
+      {
+        ROS_WARN("Use of \"--split <MAX_SIZE>\" has been deprecated.  Please use --split --size <MAX_SIZE> or --split --duration <MAX_DURATION>");
+        if (S < 0)
+          throw ros::Exception("Split size must be 0 or positive");
+        opts.max_size = 1048576 * static_cast<uint64_t>(S);
+      }
+    }
+    if(vm.count("max-splits"))
+    {
+        if(!opts.split)
+        {
+            ROS_WARN("--max-splits is ignored without --split");
+        }
+        else
+        {
+            opts.max_splits = vm["max-splits"].as<int>();
+        }
+    }
+    if (vm.count("buffsize"))
+    {
+      int m = vm["buffsize"].as<int>();
+      if (m < 0)
+        throw ros::Exception("Buffer size must be 0 or positive");
+      opts.buffer_size = 1048576 * m;
+    }
+    if (vm.count("chunksize"))
+    {
+      int chnk_sz = vm["chunksize"].as<int>();
+      if (chnk_sz < 0)
+        throw ros::Exception("Chunk size must be 0 or positive");
+      opts.chunk_size = 1024 * chnk_sz;
+    }
+    if (vm.count("limit"))
+    {
+      opts.limit = vm["limit"].as<int>();
+    }
+    if (vm.count("min-space"))
+    {
+        std::string ms = vm["min-space"].as<std::string>();
+        long long int value = 1073741824ull;
+        char mul = 0;
+        // Sane default values, just in case
+        opts.min_space_str = "1G";
+        opts.min_space = value;
+        if (sscanf(ms.c_str(), " %lld%c", &value, &mul) > 0) {
+            opts.min_space_str = ms;
+            switch (mul) {
+                case 'G':
+                case 'g':
+                    opts.min_space = value * 1073741824ull;
+                    break;
+                case 'M':
+                case 'm':
+                    opts.min_space = value * 1048576ull;
+                    break;
+                case 'K':
+                case 'k':
+                    opts.min_space = value * 1024ull;
+                    break;
+                default:
+                    opts.min_space = value;
+                    break;
+            }
+        }
+        ROS_DEBUG("Rosbag using minimum space of %lld bytes, or %s", opts.min_space, opts.min_space_str.c_str());
+    }
+    if (vm.count("bz2") && vm.count("lz4"))
+    {
+      throw ros::Exception("Can only use one type of compression");
+    }
+    if (vm.count("bz2"))
+    {
+      opts.compression = rosbag::compression::BZ2;
+    }
+    if (vm.count("lz4"))
+    {
+      opts.compression = rosbag::compression::LZ4;
+    }
+    if (vm.count("duration"))
+    {
+      std::string duration_str = vm["duration"].as<std::string>();
+
+      double duration;
+      double multiplier = 1.0;
+      std::string unit("");
+
+      std::istringstream iss(duration_str);
+      if ((iss >> duration).fail())
+        throw ros::Exception("Duration must start with a floating point number.");
+
+      if ( (!iss.eof() && ((iss >> unit).fail())) )
+      {
+        throw ros::Exception("Duration unit must be s, m, or h");
+      }
+      if (unit == std::string(""))
+        multiplier = 1.0;
+      else if (unit == std::string("s"))
+        multiplier = 1.0;
+      else if (unit == std::string("m"))
+        multiplier = 60.0;
+      else if (unit == std::string("h"))
+        multiplier = 3600.0;
+      else
+        throw ros::Exception("Duration unit must be s, m, or h");
+
+      
+      opts.max_duration = ros::Duration(duration * multiplier);
+      if (opts.max_duration <= ros::Duration(0))
+        throw ros::Exception("Duration must be positive.");
+    }
+    if (vm.count("size"))
+    {
+      opts.max_size = vm["size"].as<uint64_t>() * 1048576;
+      if (opts.max_size <= 0)
+        throw ros::Exception("Split size must be 0 or positive");
+    }
+    if (vm.count("node"))
+    {
+      opts.node = vm["node"].as<std::string>();
+      std::cout << "Recording from: " << opts.node << std::endl;
+    }
+    if (vm.count("tcpnodelay"))
+    {
+      opts.transport_hints.tcpNoDelay();
+    }
+    if (vm.count("udp"))
+    {
+      opts.transport_hints.udp();
+    }
+
+    // Every non-option argument is assumed to be a topic
+    if (vm.count("topic"))
+    {
+      std::vector<std::string> bags = vm["topic"].as< std::vector<std::string> >();
+      std::sort(bags.begin(), bags.end());
+      bags.erase(std::unique(bags.begin(), bags.end()), bags.end());
+      for (std::vector<std::string>::iterator i = bags.begin();
+           i != bags.end();
+           i++)
+        opts.topics.push_back(*i);
+    }
+
+
+    // check that argument combinations make sense
+    if(opts.exclude_regex.size() > 0 &&
+            !(opts.record_all || opts.regex)) {
+        fprintf(stderr, "Warning: Exclusion regex given, but no topics to subscribe to.\n"
+                "Adding implicit 'record all'.");
+        opts.record_all = true;
+    }
+
+    return opts;
+}
+
+/**
+ * Handle SIGTERM to allow the recorder to cleanup by requesting a shutdown.
+ * \param signal
+ */
+void signal_handler(int signal)
+{
+  (void) signal;
+  ros::requestShutdown();
+}
+
+int main(int argc, char** argv) {
+    ros::init(argc, argv, "record", ros::init_options::AnonymousName);
+
+    // handle SIGTERM signals
+    signal(SIGTERM, signal_handler);
+
+    // Parse the command-line options
+    rosbag::RecorderOptions opts;
+    try {
+        opts = parseOptions(argc, argv);
+    }
+    catch (const ros::Exception& ex) {
+        ROS_ERROR("Error reading options: %s", ex.what());
+        return 1;
+    }
+    catch(const boost::regex_error& ex) {
+        ROS_ERROR("Error reading options: %s\n", ex.what());
+        return 1;
+    }
+
+    // Run the recorder
+    rosbag::Recorder recorder(opts);
+    int result = recorder.run();
+    
+    return result;
+}
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/src/recorder.cpp
@@ -0,0 +1,795 @@
+/*********************************************************************
+* Software License Agreement (BSD License)
+*
+*  Copyright (c) 2008, Willow Garage, Inc.
+*  All rights reserved.
+*
+*  Redistribution and use in source and binary forms, with or without
+*  modification, are permitted provided that the following conditions
+*  are met:
+*
+*   * Redistributions of source code must retain the above copyright
+*     notice, this list of conditions and the following disclaimer.
+*   * Redistributions in binary form must reproduce the above
+*     copyright notice, this list of conditions and the following
+*     disclaimer in the documentation and/or other materials provided
+*     with the distribution.
+*   * Neither the name of Willow Garage, Inc. nor the names of its
+*     contributors may be used to endorse or promote products derived
+*     from this software without specific prior written permission.
+*
+*  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+*  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+*  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+*  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+*  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+*  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+*  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+*  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+*  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+*  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+*  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+*  POSSIBILITY OF SUCH DAMAGE.
+********************************************************************/
+
+#include "rosbag/recorder.h"
+
+#include <sys/stat.h>
+#include <boost/filesystem.hpp>
+// Boost filesystem v3 is default in 1.46.0 and above
+// Fallback to original posix code (*nix only) if this is not true
+#if BOOST_FILESYSTEM_VERSION < 3
+  #include <sys/statvfs.h>
+#endif
+#include <time.h>
+
+#include <queue>
+#include <set>
+#include <sstream>
+#include <string>
+
+#include <boost/lexical_cast.hpp>
+#include <boost/regex.hpp>
+#include <boost/thread.hpp>
+#include <boost/thread/xtime.hpp>
+#include <boost/date_time/local_time/local_time.hpp>
+
+#include <ros/ros.h>
+#include <topic_tools/shape_shifter.h>
+
+#include "ros/network.h"
+#include "ros/xmlrpc_manager.h"
+#include "xmlrpcpp/XmlRpc.h"
+
+using std::cout;
+using std::endl;
+using std::set;
+using std::string;
+using std::vector;
+using boost::shared_ptr;
+using ros::Time;
+
+namespace rosbag {
+
+// OutgoingMessage
+
+OutgoingMessage::OutgoingMessage(string const& _topic, topic_tools::ShapeShifter::ConstPtr _msg, boost::shared_ptr<ros::M_string> _connection_header, Time _time) :
+    topic(_topic), msg(_msg), connection_header(_connection_header), time(_time)
+{
+}
+
+// OutgoingQueue
+
+OutgoingQueue::OutgoingQueue(string const& _filename, std::queue<OutgoingMessage>* _queue, Time _time) :
+    filename(_filename), queue(_queue), time(_time)
+{
+}
+
+// RecorderOptions
+
+RecorderOptions::RecorderOptions() :
+    trigger(false),
+    record_all(false),
+    regex(false),
+    do_exclude(false),
+    quiet(false),
+    append_date(true),
+    snapshot(false),
+    verbose(false),
+    publish(false),
+    repeat_latched(false),
+    compression(compression::Uncompressed),
+    prefix(""),
+    name(""),
+    exclude_regex(),
+    buffer_size(1048576 * 256),
+    chunk_size(1024 * 768),
+    limit(0),
+    split(false),
+    max_size(0),
+    max_splits(0),
+    max_duration(-1.0),
+    node(""),
+    min_space(1024 * 1024 * 1024),
+    min_space_str("1G")
+{
+}
+
+// Recorder
+
+Recorder::Recorder(RecorderOptions const& options) :
+    options_(options),
+    num_subscribers_(0),
+    exit_code_(0),
+    queue_size_(0),
+    split_count_(0),
+    writing_enabled_(true)
+{
+}
+
+int Recorder::run() {
+    if (options_.trigger) {
+        doTrigger();
+        return 0;
+    }
+
+    if (options_.topics.size() == 0) {
+        // Make sure limit is not specified with automatic topic subscription
+        if (options_.limit > 0) {
+            fprintf(stderr, "Specifying a count is not valid with automatic topic subscription.\n");
+            return 1;
+        }
+
+        // Make sure topics are specified
+        if (!options_.record_all && (options_.node == std::string(""))) {
+            fprintf(stderr, "No topics specified.\n");
+            return 1;
+        }
+    }
+
+    ros::NodeHandle nh;
+    if (!nh.ok())
+        return 0;
+
+    if (options_.publish)
+    {
+        pub_begin_write = nh.advertise<std_msgs::String>("begin_write", 1, true);
+    }
+
+    last_buffer_warn_ = Time();
+    queue_ = new std::queue<OutgoingMessage>;
+
+    // Subscribe to each topic
+    if (!options_.regex) {
+    	for (string const& topic : options_.topics)
+            subscribe(topic);
+    }
+
+    if (!ros::Time::waitForValid(ros::WallDuration(2.0)))
+      ROS_WARN("/use_sim_time set to true and no clock published.  Still waiting for valid time...");
+
+    ros::Time::waitForValid();
+
+    start_time_ = ros::Time::now();
+
+    // Don't bother doing anything if we never got a valid time
+    if (!nh.ok())
+        return 0;
+
+    ros::Subscriber trigger_sub;
+
+    // Spin up a thread for writing to the file
+    boost::thread record_thread;
+    if (options_.snapshot)
+    {
+        record_thread = boost::thread([this]() {
+          try
+          {
+            this->doRecordSnapshotter();
+          }
+          catch (const rosbag::BagException& ex)
+          {
+            ROS_ERROR_STREAM(ex.what());
+            exit_code_ = 1;
+          }
+          catch (const std::exception& ex)
+          {
+            ROS_ERROR_STREAM(ex.what());
+            exit_code_ = 2;
+          }
+          catch (...)
+          {
+            ROS_ERROR_STREAM("Unknown exception thrown while recording bag, exiting.");
+            exit_code_ = 3;
+          }
+        });
+
+        // Subscribe to the snapshot trigger
+        trigger_sub = nh.subscribe<std_msgs::Empty>("snapshot_trigger", 100, boost::bind(&Recorder::snapshotTrigger, this, boost::placeholders::_1));
+    }
+    else
+    {
+        record_thread = boost::thread([this]() {
+          try
+          {
+            this->doRecord();
+          }
+          catch (const rosbag::BagException& ex)
+          {
+            ROS_ERROR_STREAM(ex.what());
+            exit_code_ = 1;
+          }
+          catch (const std::exception& ex)
+          {
+            ROS_ERROR_STREAM(ex.what());
+            exit_code_ = 2;
+          }
+          catch (...)
+          {
+            ROS_ERROR_STREAM("Unknown exception thrown while recording bag, exiting.");
+            exit_code_ = 3;
+          }
+        });
+    }
+
+
+
+    ros::Timer check_master_timer;
+    if (options_.record_all || options_.regex || (options_.node != std::string("")))
+    {
+        // check for master first
+        doCheckMaster(ros::TimerEvent(), nh);
+        check_master_timer = nh.createTimer(ros::Duration(1.0), boost::bind(&Recorder::doCheckMaster, this, boost::placeholders::_1, boost::ref(nh)));
+    }
+
+    ros::AsyncSpinner s(10);
+    s.start();
+
+    record_thread.join();
+    queue_condition_.notify_all();
+    delete queue_;
+
+    return exit_code_;
+}
+
+shared_ptr<ros::Subscriber> Recorder::subscribe(string const& topic) {
+    ROS_INFO("Subscribing to %s", topic.c_str());
+
+    ros::NodeHandle nh;
+    shared_ptr<int> count(boost::make_shared<int>(options_.limit));
+    shared_ptr<ros::Subscriber> sub(boost::make_shared<ros::Subscriber>());
+
+    ros::SubscribeOptions ops;
+    ops.topic = topic;
+    ops.queue_size = 100;
+    ops.md5sum = ros::message_traits::md5sum<topic_tools::ShapeShifter>();
+    ops.datatype = ros::message_traits::datatype<topic_tools::ShapeShifter>();
+    ops.helper = boost::make_shared<ros::SubscriptionCallbackHelperT<
+        const ros::MessageEvent<topic_tools::ShapeShifter const> &> >(
+            boost::bind(&Recorder::doQueue, this, boost::placeholders::_1, topic, sub, count));
+    ops.transport_hints = options_.transport_hints;
+    *sub = nh.subscribe(ops);
+
+    currently_recording_.insert(topic);
+    num_subscribers_++;
+
+    return sub;
+}
+
+bool Recorder::isSubscribed(string const& topic) const {
+    return currently_recording_.find(topic) != currently_recording_.end();
+}
+
+bool Recorder::shouldSubscribeToTopic(std::string const& topic, bool from_node) {
+    // ignore already known topics
+    if (isSubscribed(topic)) {
+        return false;
+    }
+
+    // subtract exclusion regex, if any
+    if(options_.do_exclude && boost::regex_match(topic, options_.exclude_regex)) {
+        return false;
+    }
+
+    if(options_.record_all || from_node) {
+        return true;
+    }
+    
+    if (options_.regex) {
+        // Treat the topics as regular expressions
+	return std::any_of(
+            std::begin(options_.topics), std::end(options_.topics),
+            [&topic] (string const& regex_str){
+                boost::regex e(regex_str);
+                boost::smatch what;
+                return boost::regex_match(topic, what, e, boost::match_extra);
+            });
+    }
+
+    return std::find(std::begin(options_.topics), std::end(options_.topics), topic)
+	    != std::end(options_.topics);
+}
+
+template<class T>
+std::string Recorder::timeToStr(T ros_t)
+{
+    (void)ros_t;
+    std::stringstream msg;
+    const boost::posix_time::ptime now=
+        boost::posix_time::second_clock::local_time();
+    boost::posix_time::time_facet *const f=
+        new boost::posix_time::time_facet("%Y-%m-%d-%H-%M-%S");
+    msg.imbue(std::locale(msg.getloc(),f));
+    msg << now;
+    return msg.str();
+}
+
+//! Callback to be invoked to save messages into a queue
+void Recorder::doQueue(const ros::MessageEvent<topic_tools::ShapeShifter const>& msg_event, string const& topic, shared_ptr<ros::Subscriber> subscriber, shared_ptr<int> count) {
+    //void Recorder::doQueue(topic_tools::ShapeShifter::ConstPtr msg, string const& topic, shared_ptr<ros::Subscriber> subscriber, shared_ptr<int> count) {
+    Time rectime = Time::now();
+    
+    if (options_.verbose)
+        cout << "Received message on topic " << subscriber->getTopic() << endl;
+
+    OutgoingMessage out(topic, msg_event.getMessage(), msg_event.getConnectionHeaderPtr(), rectime);
+    
+    {
+        boost::mutex::scoped_lock lock(queue_mutex_);
+
+        queue_->push(out);
+        queue_size_ += out.msg->size();
+        
+        if (options_.repeat_latched)
+        {
+            ros::M_string::const_iterator it = out.connection_header->find("latching");
+            if ((it != out.connection_header->end()) && (it->second == "1"))
+            {
+                ros::M_string::const_iterator it2 = out.connection_header->find("callerid");
+                if (it2 != out.connection_header->end())
+                {
+                    latched_msgs_.insert({{subscriber->getTopic(), it2->second}, out});
+                }
+            }
+        }
+
+        // Check to see if buffer has been exceeded
+        while (options_.buffer_size > 0 && queue_size_ > options_.buffer_size) {
+            OutgoingMessage drop = queue_->front();
+            queue_->pop();
+            queue_size_ -= drop.msg->size();
+
+            if (!options_.snapshot) {
+                Time now = Time::now();
+                if (now > last_buffer_warn_ + ros::Duration(5.0)) {
+                    ROS_WARN("rosbag record buffer exceeded.  Dropping oldest queued message.");
+                    last_buffer_warn_ = now;
+                }
+            }
+        }
+    }
+  
+    if (!options_.snapshot)
+        queue_condition_.notify_all();
+
+    // If we are book-keeping count, decrement and possibly shutdown
+    if ((*count) > 0) {
+        (*count)--;
+        if ((*count) == 0) {
+            subscriber->shutdown();
+
+            num_subscribers_--;
+
+            if (num_subscribers_ == 0)
+                ros::shutdown();
+        }
+    }
+}
+
+void Recorder::updateFilenames() {
+    vector<string> parts;
+
+    std::string prefix = options_.prefix;
+    size_t ind = prefix.rfind(".bag");
+
+    if (ind != std::string::npos && ind == prefix.size() - 4)
+    {
+      prefix.erase(ind);
+    }
+
+    if (prefix.length() > 0)
+        parts.push_back(prefix);
+    if (options_.append_date)
+        parts.push_back(timeToStr(ros::WallTime::now()));
+    if (options_.split)
+        parts.push_back(boost::lexical_cast<string>(split_count_));
+
+    if (parts.size() == 0)
+    {
+      throw BagException("Bag filename is empty (neither of these was specified: prefix, append_date, split)");
+    }
+
+    target_filename_ = parts[0];
+    for (unsigned int i = 1; i < parts.size(); i++)
+        target_filename_ += string("_") + parts[i];
+
+    target_filename_ += string(".bag");
+    write_filename_ = target_filename_ + string(".active");
+}
+
+//! Callback to be invoked to actually do the recording
+void Recorder::snapshotTrigger(std_msgs::Empty::ConstPtr trigger) {
+    (void)trigger;
+    updateFilenames();
+    
+    ROS_INFO("Triggered snapshot recording with name '%s'.", target_filename_.c_str());
+    
+    {
+        boost::mutex::scoped_lock lock(queue_mutex_);
+        queue_queue_.push(OutgoingQueue(target_filename_, queue_, Time::now()));
+        queue_      = new std::queue<OutgoingMessage>;
+        queue_size_ = 0;
+    }
+
+    queue_condition_.notify_all();
+}
+
+void Recorder::startWriting() {
+    bag_.setCompression(options_.compression);
+    bag_.setChunkThreshold(options_.chunk_size);
+
+    updateFilenames();
+    try {
+        bag_.open(write_filename_, bagmode::Write);
+    }
+    catch (const rosbag::BagException& e) {
+        ROS_ERROR("Error writing: %s", e.what());
+        exit_code_ = 1;
+        ros::shutdown();
+    }
+    ROS_INFO("Recording to '%s'.", target_filename_.c_str());
+
+    if (options_.repeat_latched)
+    {
+        // Start each new bag file with copies of all latched messages.
+        ros::Time now = ros::Time::now();
+        for (auto const& out : latched_msgs_)
+        {
+            // Overwrite the original receipt time, otherwise the new bag will
+            // have a gap before the new messages start.
+            bag_.write(out.second.topic, now, *out.second.msg);
+        }
+    }
+
+    if (options_.publish)
+    {
+        std_msgs::String msg;
+        msg.data = target_filename_.c_str();
+        pub_begin_write.publish(msg);
+    }
+}
+
+void Recorder::stopWriting() {
+    ROS_INFO("Closing '%s'.", target_filename_.c_str());
+    bag_.close();
+    rename(write_filename_.c_str(), target_filename_.c_str());
+}
+
+void Recorder::checkNumSplits()
+{
+    if(options_.max_splits>0)
+    {
+        current_files_.push_back(target_filename_);
+        if(current_files_.size()>options_.max_splits)
+        {
+            int err = unlink(current_files_.front().c_str());
+            if(err != 0)
+            {
+                ROS_ERROR("Unable to remove %s: %s", current_files_.front().c_str(), strerror(errno));
+            }
+            current_files_.pop_front();
+        }
+    }
+}
+
+bool Recorder::checkSize()
+{
+    if (options_.max_size > 0)
+    {
+        if (bag_.getSize() > options_.max_size)
+        {
+            if (options_.split)
+            {
+                stopWriting();
+                split_count_++;
+                checkNumSplits();
+                startWriting();
+            } else {
+                ros::shutdown();
+                return true;
+            }
+        }
+    }
+    return false;
+}
+
+bool Recorder::checkDuration(const ros::Time& t)
+{
+    if (options_.max_duration > ros::Duration(0))
+    {
+        if (t - start_time_ > options_.max_duration)
+        {
+            if (options_.split)
+            {
+                while (start_time_ + options_.max_duration < t)
+                {
+                    stopWriting();
+                    split_count_++;
+                    checkNumSplits();
+                    start_time_ += options_.max_duration;
+                    startWriting();
+                }
+            } else {
+                ros::shutdown();
+                return true;
+            }
+        }
+    }
+    return false;
+}
+
+
+//! Thread that actually does writing to file.
+void Recorder::doRecord() {
+    // Open bag file for writing
+    startWriting();
+
+    // Schedule the disk space check
+    warn_next_ = ros::WallTime();
+
+    try
+    {
+        checkDisk();
+    }
+    catch (const rosbag::BagException& ex)
+    {
+        ROS_ERROR_STREAM(ex.what());
+        exit_code_ = 1;
+        stopWriting();
+        return;
+    }
+
+    check_disk_next_ = ros::WallTime::now() + ros::WallDuration().fromSec(20.0);
+
+    // Technically the queue_mutex_ should be locked while checking empty.
+    // Except it should only get checked if the node is not ok, and thus
+    // it shouldn't be in contention.
+    ros::NodeHandle nh;
+    while (nh.ok() || !queue_->empty()) {
+        boost::unique_lock<boost::mutex> lock(queue_mutex_);
+
+        bool finished = false;
+        while (queue_->empty()) {
+            if (!nh.ok()) {
+                lock.release()->unlock();
+                finished = true;
+                break;
+            }
+            boost::xtime xt;
+            boost::xtime_get(&xt, boost::TIME_UTC_);
+            xt.nsec += 250000000;
+            queue_condition_.timed_wait(lock, xt);
+            if (checkDuration(ros::Time::now()))
+            {
+                finished = true;
+                break;
+            }
+        }
+        if (finished)
+            break;
+
+        OutgoingMessage out = queue_->front();
+        queue_->pop();
+        queue_size_ -= out.msg->size();
+        
+        lock.release()->unlock();
+        
+        if (checkSize())
+            break;
+
+        if (checkDuration(out.time))
+            break;
+
+        try
+        {
+            if (scheduledCheckDisk() && checkLogging())
+                bag_.write(out.topic, out.time, *out.msg, out.connection_header);
+        }
+        catch (const rosbag::BagException& ex)
+        {
+            ROS_ERROR_STREAM(ex.what());
+            exit_code_ = 1;
+            break;
+        }
+    }
+
+    stopWriting();
+}
+
+void Recorder::doRecordSnapshotter() {
+    ros::NodeHandle nh;
+  
+    while (nh.ok() || !queue_queue_.empty()) {
+        boost::unique_lock<boost::mutex> lock(queue_mutex_);
+        while (queue_queue_.empty()) {
+            if (!nh.ok())
+                return;
+            queue_condition_.wait(lock);
+        }
+        
+        OutgoingQueue out_queue = queue_queue_.front();
+        queue_queue_.pop();
+        
+        lock.release()->unlock();
+        
+        string target_filename = out_queue.filename;
+        string write_filename  = target_filename + string(".active");
+        
+        try {
+            bag_.open(write_filename, bagmode::Write);
+        }
+        catch (const rosbag::BagException& ex) {
+            ROS_ERROR("Error writing: %s", ex.what());
+            return;
+        }
+
+        while (!out_queue.queue->empty()) {
+            OutgoingMessage out = out_queue.queue->front();
+            out_queue.queue->pop();
+
+            bag_.write(out.topic, out.time, *out.msg);
+        }
+
+        stopWriting();
+    }
+}
+
+void Recorder::doCheckMaster(ros::TimerEvent const& e, ros::NodeHandle& node_handle) {
+    (void)e;
+    (void)node_handle;
+    ros::master::V_TopicInfo topics;
+    if (ros::master::getTopics(topics)) {
+	for (ros::master::TopicInfo const& t : topics) {
+	    if (shouldSubscribeToTopic(t.name))
+	        subscribe(t.name);
+	}
+    }
+    
+    if (options_.node != std::string(""))
+    {
+
+      XmlRpc::XmlRpcValue req;
+      req[0] = ros::this_node::getName();
+      req[1] = options_.node;
+      XmlRpc::XmlRpcValue resp;
+      XmlRpc::XmlRpcValue payload;
+
+      if (ros::master::execute("lookupNode", req, resp, payload, true))
+      {
+        std::string peer_host;
+        uint32_t peer_port;
+
+        if (!ros::network::splitURI(static_cast<std::string>(resp[2]), peer_host, peer_port))
+        {
+          ROS_ERROR("Bad xml-rpc URI trying to inspect node at: [%s]", static_cast<std::string>(resp[2]).c_str());
+        } else {
+
+          XmlRpc::XmlRpcClient c(peer_host.c_str(), peer_port, "/");
+          XmlRpc::XmlRpcValue req2;
+          XmlRpc::XmlRpcValue resp2;
+          req2[0] = ros::this_node::getName();
+          c.execute("getSubscriptions", req2, resp2);
+          
+          if (!c.isFault() && resp2.valid() && resp2.size() > 0 && static_cast<int>(resp2[0]) == 1)
+          {
+            for(int i = 0; i < resp2[2].size(); i++)
+            {
+              if (shouldSubscribeToTopic(resp2[2][i][0], true))
+                subscribe(resp2[2][i][0]);
+            }
+          } else {
+            ROS_ERROR("Node at: [%s] failed to return subscriptions.", static_cast<std::string>(resp[2]).c_str());
+          }
+        }
+      }
+    }
+}
+
+void Recorder::doTrigger() {
+    ros::NodeHandle nh;
+    ros::Publisher pub = nh.advertise<std_msgs::Empty>("snapshot_trigger", 1, true);
+    pub.publish(std_msgs::Empty());
+
+    ros::Timer terminate_timer = nh.createTimer(ros::Duration(1.0), boost::bind(&ros::shutdown));
+    ros::spin();
+}
+
+bool Recorder::scheduledCheckDisk() {
+    boost::mutex::scoped_lock lock(check_disk_mutex_);
+
+    if (ros::WallTime::now() < check_disk_next_)
+        return true;
+
+    check_disk_next_ += ros::WallDuration().fromSec(20.0);
+    return checkDisk();
+}
+
+bool Recorder::checkDisk() {
+#if BOOST_FILESYSTEM_VERSION < 3
+    struct statvfs fiData;
+    if ((statvfs(bag_.getFileName().c_str(), &fiData)) < 0)
+    {
+        ROS_WARN("Failed to check filesystem stats.");
+        return true;
+    }
+    unsigned long long free_space = 0;
+    free_space = (unsigned long long) (fiData.f_bsize) * (unsigned long long) (fiData.f_bavail);
+    if (free_space < options_.min_space)
+    {
+        ROS_ERROR("Less than %s of space free on disk with '%s'.  Disabling recording.", options_.min_space_str.c_str(), bag_.getFileName().c_str());
+        writing_enabled_ = false;
+        return false;
+    }
+    else if (free_space < 5 * options_.min_space)
+    {
+        ROS_WARN("Less than 5 x %s of space free on disk with '%s'.", options_.min_space_str.c_str(), bag_.getFileName().c_str());
+    }
+    else
+    {
+        writing_enabled_ = true;
+    }
+#else
+    boost::filesystem::path p(boost::filesystem::system_complete(bag_.getFileName().c_str()));
+    p = p.parent_path();
+    boost::filesystem::space_info info;
+    try
+    {
+        info = boost::filesystem::space(p);
+    }
+    catch (const boost::filesystem::filesystem_error& e) 
+    { 
+        ROS_WARN("Failed to check filesystem stats [%s].", e.what());
+        writing_enabled_ = false;
+        return false;
+    }
+    if ( info.available < options_.min_space)
+    {
+        writing_enabled_ = false;
+        throw BagException("Less than " + options_.min_space_str + " of space free on disk with " + bag_.getFileName() + ". Disabling recording.");
+    }
+    else if (info.available < 5 * options_.min_space)
+    {
+        ROS_WARN("Less than 5 x %s of space free on disk with '%s'.", options_.min_space_str.c_str(), bag_.getFileName().c_str());
+        writing_enabled_ = true;
+    }
+    else
+    {
+        writing_enabled_ = true;
+    }
+#endif
+    return true;
+}
+
+bool Recorder::checkLogging() {
+    if (writing_enabled_)
+        return true;
+
+    ros::WallTime now = ros::WallTime::now();
+    if (now >= warn_next_) {
+        warn_next_ = now + ros::WallDuration().fromSec(5.0);
+        ROS_WARN("Not logging message because logging disabled.  Most likely cause is a full disk.");
+    }
+    return false;
+}
+
+} // namespace rosbag
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/src/rosbag/__init__.py
@@ -0,0 +1,37 @@
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2009, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+from .bag import Bag, Compression, ROSBagException, ROSBagFormatException, ROSBagUnindexedException
+
+# Import rosbag main to be used by the rosbag executable
+from .rosbag_main import rosbagmain
+
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/src/rosbag/bag.py
@@ -0,0 +1,2939 @@
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2010, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+"""
+The rosbag Python API.
+
+Provides serialization of bag files.
+"""
+
+from __future__ import print_function
+
+import bisect
+import bz2
+import collections
+import heapq
+import os
+import re
+import struct
+import sys
+import threading
+import time
+import yaml
+
+from Cryptodome.Cipher import AES
+from Cryptodome.Random import get_random_bytes
+
+import gnupg
+
+try:
+    from cStringIO import StringIO  # Python 2.x
+except ImportError:
+    from io import BytesIO as StringIO  # Python 3.x
+
+import genmsg
+import genpy
+import genpy.dynamic
+import genpy.message
+
+import roslib.names # still needed for roslib.names.canonicalize_name()
+import rospy
+try:
+    import roslz4
+    found_lz4 = True
+except ImportError:
+    rospy.logwarn(
+        'Failed to load Python extension for LZ4 support. '
+        'LZ4 compression will not be available.')
+    found_lz4 = False
+
+class ROSBagException(Exception):
+    """
+    Base class for exceptions in rosbag.
+    """
+    def __init__(self, value=None):
+        self.value = value
+        #fix for #1209. needed in Python 2.7.
+        # For details: https://stackoverflow.com/questions/41808912/cannot-unpickle-exception-subclass
+        self.args = (value,)
+
+    def __str__(self):
+        return self.value
+
+class ROSBagFormatException(ROSBagException):
+    """
+    Exceptions for errors relating to the bag file format.
+    """
+    def __init__(self, value):
+        ROSBagException.__init__(self, value)
+
+class ROSBagUnindexedException(ROSBagException):
+    """
+    Exception for unindexed bags.
+    """
+    def __init__(self, *args):
+        #*args needed for #1209
+        ROSBagException.__init__(self, 'Unindexed bag')
+
+class ROSBagEncryptNotSupportedException(ROSBagException):
+    """
+    Exception raised when encryption is not supported.
+    """
+    def __init__(self, value):
+        ROSBagException.__init__(self, value)
+
+class ROSBagEncryptException(ROSBagException):
+    """
+    Exception raised when encryption or decryption failed.
+    """
+    def __init__(self, value):
+        ROSBagException.__init__(self, value)
+
+class Compression:
+    """
+    Allowable compression types
+    """
+    NONE = 'none'
+    BZ2  = 'bz2'
+    LZ4  = 'lz4'
+
+BagMessage = collections.namedtuple('BagMessage', 'topic message timestamp')
+BagMessageWithConnectionHeader = collections.namedtuple('BagMessageWithConnectionHeader', 'topic message timestamp connection_header')
+
+class _ROSBagEncryptor(object):
+    """
+    Base class for bag encryptor.
+    """
+    _ENCRYPTOR_FIELD_NAME = 'encryptor'
+
+    def __init__(self):
+        pass
+
+class _ROSBagNoEncryptor(_ROSBagEncryptor):
+    """
+    Class for unencrypted bags.
+    """
+    def __init__(self):
+        super(_ROSBagNoEncryptor, self).__init__()
+
+    def initialize(self, _, __):
+        pass
+
+    def encrypt_chunk(self, chunk_size, _, __):
+        return chunk_size
+
+    def decrypt_chunk(self, chunk):
+        return chunk
+
+    def add_fields_to_file_header(self, _):
+        pass
+
+    def read_fields_from_file_header(self, _):
+        pass
+
+    def write_encrypted_header(self, write_header, f, header):
+        return write_header(f, header)
+
+    def read_encrypted_header(self, read_header, f, req_op=None):
+        return read_header(f, req_op)
+
+    def add_info_rows(self, rows):
+        pass
+
+    def get_info_str(self):
+        return ''
+
+class _ROSBagAesCbcEncryptor(_ROSBagEncryptor):
+    """
+    Class for AES-CBC-encrypted bags.
+    """
+    NAME = 'rosbag/AesCbcEncryptor'
+    _GPG_USER_FIELD_NAME = 'gpg_user'
+    _ENCRYPTED_KEY_FIELD_NAME = 'encrypted_key'
+
+    def __init__(self):
+        """
+        Create AES encryptor.
+        """
+        super(_ROSBagAesCbcEncryptor, self).__init__()
+        # User name of GPG key used for symmetric key encryption
+        self._gpg_key_user = None
+        # GPG passphrase
+        self._gpg_passphrase = None
+        # Symmetric key for encryption/decryption
+        self._symmetric_key = None
+        # Encrypted symmetric key
+        self._encrypted_symmetric_key = None
+
+    def initialize(self, bag, gpg_key_user, passphrase=None):
+        """
+        Initialize encryptor by composing AES symmetric key.
+        @param bag: bag to be encrypted/decrypted
+        @type  bag: Bag
+        @param gpg_key_user: user name of GPG key used for symmetric key encryption
+        @type  gpg_key_user: str
+        @raise ROSBagException: if GPG key user has already been set
+        """
+        if bag._mode != 'w':
+            self._gpg_passphrase = passphrase or os.getenv('ROSBAG_GPG_PASSPHRASE', None)
+            return
+        if self._gpg_key_user == gpg_key_user:
+            return
+        if not self._gpg_key_user:
+            self._gpg_key_user = gpg_key_user
+            self._build_symmetric_key()
+        else:
+            raise ROSBagException('Encryption user has already been set to {}'.format(self._gpg_key_user))
+
+    def encrypt_chunk(self, chunk_size, chunk_data_pos, f):
+        """
+        Read chunk from file, encrypt it, and write back to file.
+        @param chunk_size: size of chunk
+        @type  chunk_size: int
+        @param chunk_data_pos: position of chunk data portion
+        @type  chunk_data_pos: int
+        @param f: file stream
+        @type  f: file
+        @return: size of initialization vector and encrypted chunk
+        @rtype:  int
+        """
+        f.seek(chunk_data_pos)
+        chunk = _read(f, chunk_size)
+        # Encrypt chunk
+        iv = get_random_bytes(AES.block_size)
+        f.seek(chunk_data_pos)
+        f.write(iv)
+        cipher = AES.new(self._symmetric_key, AES.MODE_CBC, iv)
+        encrypted_chunk = cipher.encrypt(_add_padding(chunk))
+        # Write encrypted chunk
+        f.write(encrypted_chunk)
+        f.truncate(f.tell())
+        return AES.block_size + len(encrypted_chunk)
+
+    def decrypt_chunk(self, encrypted_chunk):
+        """
+        Decrypt chunk.
+        @param encrypted_chunk: chunk to decrypt
+        @type  encrypted_chunk: str
+        @return: decrypted chunk
+        @rtype:  str
+        @raise ROSBagFormatException: if size of input chunk is not multiple of AES block size
+        """
+        if len(encrypted_chunk) % AES.block_size != 0:
+            raise ROSBagFormatException('Error in encrypted chunk size: {}'.format(len(encrypted_chunk)))
+        if len(encrypted_chunk) < AES.block_size:
+            raise ROSBagFormatException('No initialization vector in encrypted chunk: {}'.format(len(encrypted_chunk)))
+
+        iv = encrypted_chunk[:AES.block_size]
+        cipher = AES.new(self._symmetric_key, AES.MODE_CBC, iv)
+        decrypted_chunk = cipher.decrypt(encrypted_chunk[AES.block_size:])
+        return _remove_padding(decrypted_chunk)
+
+    def add_fields_to_file_header(self, header):
+        """
+        Add encryptor information to bag file header.
+        @param header: bag file header
+        @type  header: dict
+        """
+        header[self._ENCRYPTOR_FIELD_NAME] = self.NAME
+        header[self._GPG_USER_FIELD_NAME] = self._gpg_key_user
+        header[self._ENCRYPTED_KEY_FIELD_NAME] = self._encrypted_symmetric_key
+
+    def read_fields_from_file_header(self, header):
+        """
+        Read encryptor information from bag file header.
+        @param header: bag file header
+        @type  header: dict
+        @raise ROSBagFormatException: if GPG key user is not found in header
+        """
+        try:
+            self._encrypted_symmetric_key = _read_bytes_field(header, self._ENCRYPTED_KEY_FIELD_NAME)
+        except ROSBagFormatException:
+            raise ROSBagFormatException('Encrypted symmetric key is not found in header')
+        try:
+            self._gpg_key_user = _read_str_field(header, self._GPG_USER_FIELD_NAME)
+        except ROSBagFormatException:
+            raise ROSBagFormatException('GPG key user is not found in header')
+        try:
+            self._symmetric_key = _decrypt_string_gpg(self._encrypted_symmetric_key, self._gpg_passphrase)
+        except ROSBagFormatException:
+            raise
+
+    def write_encrypted_header(self, _, f, header):
+        """
+        Write encrypted header to bag file.
+        @param f: file stream
+        @type  f: file
+        @param header: unencrypted header
+        @type  header: dict
+        @return: encrypted string representing header
+        @rtype:  str
+        """
+        header_str = b''
+        equal = b'='
+        for k, v in header.items():
+            if not isinstance(k, bytes):
+                k = k.encode()
+            if not isinstance(v, bytes):
+                v = v.encode()
+            header_str += _pack_uint32(len(k) + 1 + len(v)) + k + equal + v
+
+        iv = get_random_bytes(AES.block_size)
+        enc_str = iv
+        cipher = AES.new(self._symmetric_key, AES.MODE_CBC, iv)
+        enc_str += cipher.encrypt(_add_padding(header_str))
+        _write_sized(f, enc_str)
+        return enc_str
+
+    def read_encrypted_header(self, _, f, req_op=None):
+        """
+        Read encrypted header from bag file.
+        @param f: file stream
+        @type  f: file
+        @param req_op: expected header op code
+        @type  req_op: int
+        @return: decrypted header
+        @rtype:  dict
+        @raise ROSBagFormatException: if error occurs while decrypting/reading header
+        """
+        # Read header
+        try:
+            header = self._decrypt_encrypted_header(f)
+        except ROSBagException as ex:
+            raise ROSBagFormatException('Error reading header: %s' % str(ex))
+
+        return _build_header_from_str(header, req_op)
+
+    def add_info_rows(self, rows):
+        """
+        Add rows for rosbag info.
+        @param rows: information on bag encryption
+        @type  rows: list of tuples
+        """
+        rows.append(('encryption', self.NAME))
+        rows.append(('GPG key user', self._gpg_key_user))
+
+    def get_info_str(self):
+        """
+        Return string for rosbag info.
+        @return: information on bag encryption
+        @rtype:  str
+        """
+        return 'encryption: %s\nGPG key user: %s\n' % (self.NAME, self._gpg_key_user)
+
+    def _build_symmetric_key(self):
+        if not self._gpg_key_user:
+            return
+        self._symmetric_key = get_random_bytes(AES.block_size)
+        self._encrypted_symmetric_key = _encrypt_string_gpg(self._gpg_key_user, self._symmetric_key)
+
+    def _decrypt_encrypted_header(self, f):
+        try:
+            size = _read_uint32(f)
+        except struct.error as ex:
+            raise ROSBagFormatException('error unpacking uint32: %s' % str(ex))
+
+        if size % AES.block_size != 0:
+            raise ROSBagFormatException('Error in encrypted header size: {}'.format(size))
+        if size < AES.block_size:
+            raise ROSBagFormatException('No initialization vector in encrypted header: {}'.format(size))
+
+        iv = _read(f, AES.block_size)
+        size -= AES.block_size
+        encrypted_header = _read(f, size)
+        cipher = AES.new(self._symmetric_key, AES.MODE_CBC, iv)
+        header = cipher.decrypt(encrypted_header)
+        return _remove_padding(header)
+
+def _add_padding(input_bytes):
+    # Add PKCS#7 padding to input string
+    padding_num = AES.block_size - len(input_bytes) % AES.block_size
+    return input_bytes + bytes((padding_num,) * padding_num)
+
+def _remove_padding(input_str):
+    # Remove PKCS#7 padding from input string
+    return input_str[:-ord(input_str[len(input_str) - 1:])]
+
+def _encrypt_string_gpg(key_user, input):
+    gpg = gnupg.GPG()
+    enc_data = gpg.encrypt(input, [key_user], always_trust=True)
+    if not enc_data.ok:
+        raise ROSBagEncryptException('Failed to encrypt bag: {}.  Have you installed a required public key?'.format(enc_data.status))
+    return str(enc_data)
+
+def _decrypt_string_gpg(input, passphrase=None):
+    gpg = gnupg.GPG()
+    dec_data = gpg.decrypt(input, passphrase=passphrase)
+    if not dec_data.ok:
+        raise ROSBagEncryptException('Failed to decrypt bag: {}.  Have you installed a required private key?'.format(dec_data.status))
+    return dec_data.data
+
+class Bag(object):
+    """
+    Bag serialize messages to and from a single file on disk using the bag format.
+    """
+    def __init__(self, f, mode='r', compression=Compression.NONE, chunk_threshold=768 * 1024, allow_unindexed=False, options=None, skip_index=False):
+        """
+        Open a bag file.  The mode can be 'r', 'w', or 'a' for reading (default),
+        writing or appending.  The file will be created if it doesn't exist
+        when opened for writing or appending; it will be truncated when opened
+        for writing.  Simultaneous reading and writing is allowed when in writing
+        or appending mode.
+        @param f: filename of bag to open or a stream to read from
+        @type  f: str or file
+        @param mode: mode, either 'r', 'w', or 'a'
+        @type  mode: str
+        @param compression: compression mode, see U{rosbag.Compression} for valid modes
+        @type  compression: str
+        @param chunk_threshold: minimum number of uncompressed bytes per chunk
+        @type  chunk_threshold: int
+        @param allow_unindexed: if True, allow opening unindexed bags
+        @type  allow_unindexed: bool
+        @param options: the bag options (currently: compression and chunk_threshold)
+        @type  options: dict
+        @param skip_index: if True, don't read the connection index records on open [2.0+]
+        @type  skip_index: bool
+        @raise ValueError: if any argument is invalid
+        @raise ROSBagException: if an error occurs opening file
+        @raise ROSBagFormatException: if bag format is corrupted
+        """
+        if options is not None:
+            if type(options) is not dict:
+                raise ValueError('options must be of type dict')                
+            if 'compression' in options:
+                compression = options['compression']
+            if 'chunk_threshold' in options:
+                chunk_threshold = options['chunk_threshold']
+
+        self._file     = None
+        self._filename = None
+        self._version  = None
+
+        allowed_compressions = [Compression.NONE, Compression.BZ2]
+        if found_lz4:
+            allowed_compressions.append(Compression.LZ4)
+        if compression not in allowed_compressions:
+            raise ValueError('compression must be one of: %s' % ', '.join(allowed_compressions))  
+        self._compression = compression      
+
+        if chunk_threshold < 0:
+            raise ValueError('chunk_threshold must be greater than or equal to zero')        
+        self._chunk_threshold = chunk_threshold
+
+        self._skip_index = skip_index
+
+        self._reader          = None
+
+        self._file_header_pos = None
+        self._index_data_pos  = 0       # (1.2+)
+
+        self._clear_index()
+
+        self._buffer = StringIO()        
+
+        self._curr_compression = Compression.NONE
+        
+        self._encryptor = _ROSBagNoEncryptor()
+
+        self._open(f, mode, allow_unindexed)
+
+        self._output_file = self._file
+
+    def __iter__(self):
+        return self.read_messages()
+
+    def __enter__(self):
+        return self
+        
+    def __exit__(self, exc_type, exc_value, traceback):
+        self.close()
+
+    @property
+    def options(self):
+        """Get the options."""
+        return { 'compression' : self._compression, 'chunk_threshold' : self._chunk_threshold }
+    
+    @property
+    def filename(self):
+        """Get the filename."""
+        return self._filename
+    
+    @property
+    def version(self):
+        """Get the version."""
+        return self._version
+    
+    @property
+    def mode(self):
+        """Get the mode."""
+        return self._mode
+
+    @property
+    def size(self):
+        """Get the size in bytes."""
+        if not self._file:
+            raise ValueError('I/O operation on closed bag')
+        
+        pos = self._file.tell()
+        self._file.seek(0, os.SEEK_END)
+        size = self._file.tell()
+        self._file.seek(pos)
+        return size
+
+    # compression
+        
+    def _get_compression(self):
+        """Get the compression method to use for writing."""
+        return self._compression
+    
+    def _set_compression(self, compression):
+        """Set the compression method to use for writing."""
+        allowed_compressions = [Compression.NONE, Compression.BZ2]
+        if found_lz4:
+            allowed_compressions.append(Compression.LZ4)
+        if compression not in allowed_compressions:
+            raise ValueError('compression must be one of: %s' % ', '.join(allowed_compressions))        
+        
+        self.flush()
+        self._compression = compression
+        
+    compression = property(_get_compression, _set_compression)
+    
+    # chunk_threshold
+    
+    def _get_chunk_threshold(self):
+        """Get the chunk threshold to use for writing."""
+        return self._chunk_threshold
+    
+    def _set_chunk_threshold(self, chunk_threshold):
+        """Set the chunk threshold to use for writing."""
+        if chunk_threshold < 0:
+            raise ValueError('chunk_threshold must be greater than or equal to zero')
+
+        self.flush()
+        self._chunk_threshold = chunk_threshold
+        
+    chunk_threshold = property(_get_chunk_threshold, _set_chunk_threshold)
+
+    def read_messages(self, topics=None, start_time=None, end_time=None, connection_filter=None, raw=False, return_connection_header=False):
+        """
+        Read messages from the bag, optionally filtered by topic, timestamp and connection details.
+        @param topics: list of topics or a single topic. if an empty list is given all topics will be read [optional]
+        @type  topics: list(str) or str
+        @param start_time: earliest timestamp of message to return [optional]
+        @type  start_time: U{genpy.Time}
+        @param end_time: latest timestamp of message to return [optional]
+        @type  end_time: U{genpy.Time}
+        @param connection_filter: function to filter connections to include [optional]
+        @type  connection_filter: function taking (topic, datatype, md5sum, msg_def, header) and returning bool
+        @param raw: if True, then generate tuples of (datatype, (data, md5sum, position), pytype)
+        @type  raw: bool
+        @return: generator of BagMessage(topic, message, timestamp) namedtuples for each message in the bag file
+        @rtype:  generator of tuples of (str, U{genpy.Message}, U{genpy.Time}) [not raw] or (str, (str, str, str, tuple, class), U{genpy.Time}) [raw]
+        """
+        self.flush()
+
+        if topics and type(topics) is str:
+            topics = [topics]
+        
+        return self._reader.read_messages(topics, start_time, end_time, connection_filter, raw, return_connection_header)
+
+    def flush(self):
+        """
+        Write the open chunk to disk so subsequent reads will read all messages.
+        @raise ValueError: if bag is closed 
+        """
+        if not self._file:
+            raise ValueError('I/O operation on closed bag')
+
+        if self._chunk_open:
+            self._stop_writing_chunk()
+
+    def write(self, topic, msg, t=None, raw=False, connection_header=None):
+        """
+        Write a message to the bag.
+        @param topic: name of topic
+        @type  topic: str
+        @param msg: message to add to bag, or tuple (if raw)
+        @type  msg: Message or tuple of raw message data
+        @param t: ROS time of message publication, if None specified, use current time [optional]
+        @type  t: U{genpy.Time}
+        @param raw: if True, msg is in raw format, i.e. (msg_type, serialized_bytes, md5sum, pytype)
+        @type  raw: bool
+        @raise ValueError: if arguments are invalid or bag is closed
+        """
+        if not self._file:
+            raise ValueError('I/O operation on closed bag')
+
+        if not topic:
+            raise ValueError('topic is invalid')
+        if not msg:
+            raise ValueError('msg is invalid')
+
+        if t is None:
+            t = genpy.Time.from_sec(time.time())
+
+        # Seek to end (in case previous operation was a read)
+        self._file.seek(0, os.SEEK_END)
+
+        # Open a chunk, if needed
+        if not self._chunk_open:
+            self._start_writing_chunk(t)
+
+        # Unpack raw
+        if raw:
+            if len(msg) == 5:
+                msg_type, serialized_bytes, md5sum, pos, pytype = msg
+            elif len(msg) == 4:
+                msg_type, serialized_bytes, md5sum, pytype = msg
+            else:
+                raise ValueError('msg must be of length 4 or 5')
+
+        # Write connection record, if necessary (currently using a connection per topic; ignoring message connection header)
+        if topic in self._topic_connections:
+            connection_info = self._topic_connections[topic]
+            conn_id = connection_info.id
+        else:
+            conn_id = len(self._connections)
+
+            if raw:
+                if pytype is None:
+                    try:
+                        pytype = genpy.message.get_message_class(msg_type)
+                    except Exception:
+                        pytype = None
+                if pytype is None:
+                    raise ROSBagException('cannot locate message class and no message class provided for [%s]' % msg_type)
+    
+                if pytype._md5sum != md5sum:
+                    print('WARNING: md5sum of loaded type [%s] does not match that specified' % msg_type, file=sys.stderr)
+                    #raise ROSBagException('md5sum of loaded type does not match that of data being recorded')
+
+                header = connection_header if connection_header is not None else {
+                    'topic': topic,
+                    'type': msg_type,
+                    'md5sum': md5sum,
+                    'message_definition': pytype._full_text
+                }
+            else:
+                header = connection_header if connection_header is not None else {
+                    'topic': topic,
+                    'type': msg.__class__._type,
+                    'md5sum': msg.__class__._md5sum,
+                    'message_definition': msg._full_text
+                }
+
+            connection_info = _ConnectionInfo(conn_id, topic, header)
+            # No need to encrypt connection records in chunk (encrypt=False)
+            self._write_connection_record(connection_info, False)
+
+            self._connections[conn_id] = connection_info
+            self._topic_connections[topic] = connection_info
+
+        # Create an index entry
+        index_entry = _IndexEntry200(t, self._curr_chunk_info.pos, self._get_chunk_offset())
+
+        # Update the indexes and current chunk info 
+        if conn_id not in self._curr_chunk_connection_indexes:
+            # This is the first message on this connection in the chunk
+            self._curr_chunk_connection_indexes[conn_id] = [index_entry]
+            self._curr_chunk_info.connection_counts[conn_id] = 1
+        else:
+            curr_chunk_connection_index = self._curr_chunk_connection_indexes[conn_id]
+            if index_entry >= curr_chunk_connection_index[-1]:
+                # Test if we're writing chronologically.  Can skip binary search if so.
+                curr_chunk_connection_index.append(index_entry)
+            else:
+                bisect.insort_right(curr_chunk_connection_index, index_entry)
+
+            self._curr_chunk_info.connection_counts[conn_id] += 1
+
+        if conn_id not in self._connection_indexes:
+            self._connection_indexes[conn_id] = [index_entry]
+        else:
+            bisect.insort_right(self._connection_indexes[conn_id], index_entry)
+
+        # Update the chunk start/end times
+        if t > self._curr_chunk_info.end_time:
+            self._curr_chunk_info.end_time = t
+        elif t < self._curr_chunk_info.start_time:
+            self._curr_chunk_info.start_time = t
+
+        if not raw:
+            # Serialize the message to the buffer
+            self._buffer.seek(0)
+            self._buffer.truncate(0)
+            msg.serialize(self._buffer)
+            serialized_bytes = self._buffer.getvalue()
+
+        # Write message data record
+        self._write_message_data_record(conn_id, t, serialized_bytes)
+        
+        # Check if we want to stop this chunk
+        chunk_size = self._get_chunk_offset()
+        if chunk_size > self._chunk_threshold:
+            self._stop_writing_chunk()
+
+    def reindex(self):
+        """
+        Reindexes the bag file.  Yields position of each chunk for progress.
+        """
+        self._clear_index()
+        return self._reader.reindex()
+
+    def close(self):
+        """
+        Close the bag file.  Closing an already closed bag does nothing.
+        """
+        if self._file:
+            if self._mode in 'wa':
+                self._stop_writing()
+            
+            self._close_file()
+            
+    def get_compression_info(self):
+        """
+        Returns information about the compression of the bag
+        @return: generator of CompressionTuple(compression, uncompressed, compressed) describing the
+            type of compression used, size of the uncompressed data in Bytes, size of the compressed data in Bytes. If
+            no compression is used then uncompressed and compressed data will be equal.
+        @rtype: generator of CompressionTuple of (str, int, int)
+        """
+        
+        compression = self.compression
+        uncompressed = 0
+        compressed = 0
+        
+        if self._chunk_headers:
+            compression_counts = {}
+            compression_uncompressed = {}
+            compression_compressed = {}
+            
+            # the rest of this is determine which compression algorithm is dominant and
+            # to add up the uncompressed and compressed Bytes
+            for chunk_header in self._chunk_headers.values():
+                if chunk_header.compression not in compression_counts:
+                    compression_counts[chunk_header.compression] = 0
+                if chunk_header.compression not in compression_uncompressed:
+                    compression_uncompressed[chunk_header.compression] = 0
+                if chunk_header.compression not in compression_compressed:
+                    compression_compressed[chunk_header.compression] = 0
+                    
+                compression_counts[chunk_header.compression] += 1
+                compression_uncompressed[chunk_header.compression] += chunk_header.uncompressed_size
+                uncompressed += chunk_header.uncompressed_size
+                compression_compressed[chunk_header.compression] += chunk_header.compressed_size
+                compressed += chunk_header.compressed_size
+                
+            chunk_count = len(self._chunk_headers)
+
+            main_compression_count, main_compression = sorted([(v, k) for k, v in compression_counts.items()], reverse=True)[0]
+            compression = str(main_compression)
+
+        return collections.namedtuple("CompressionTuple", ["compression",
+                                                           "uncompressed", "compressed"])(compression=compression,
+                                                                                          uncompressed=uncompressed, compressed=compressed)
+    
+    def get_message_count(self, topic_filters=None):
+        """
+        Returns the number of messages in the bag. Can be filtered by Topic
+        @param topic_filters: One or more topics to filter by
+        @type topic_filters: Could be either a single str or a list of str.
+        @return: The number of messages in the bag, optionally filtered by topic
+        @rtype: int
+        """
+        
+        num_messages = 0
+        
+        if topic_filters is not None:
+            info = self.get_type_and_topic_info(topic_filters=topic_filters)
+            for topic in info.topics.values():
+                num_messages += topic.message_count
+        else:
+            if self._chunks:
+                for c in self._chunks:
+                    for counts in c.connection_counts.values():
+                        num_messages += counts
+            else:
+                num_messages = sum([len(index) for index in self._connection_indexes.values()])
+            
+        return num_messages
+    
+    def get_start_time(self):
+        """
+        Returns the start time of the bag.
+        @return: a timestamp of the start of the bag
+        @rtype: float, timestamp in seconds, includes fractions of a second
+        """
+        
+        if self._chunks:
+            start_stamp = self._chunks[0].start_time.to_sec()
+        else:
+            if not self._connection_indexes:
+                raise ROSBagException('Bag contains no message')
+            start_stamps = [index[0].time.to_sec() for index in self._connection_indexes.values() if index]
+            start_stamp = min(start_stamps) if start_stamps else 0
+        
+        return start_stamp
+    
+    def get_end_time(self):
+        """
+        Returns the end time of the bag.
+        @return: a timestamp of the end of the bag
+        @rtype: float, timestamp in seconds, includes fractions of a second
+        """
+        
+        if self._chunks:
+            end_stamp = self._chunks[-1].end_time.to_sec()
+        else:
+            if not self._connection_indexes:
+                raise ROSBagException('Bag contains no message')
+            end_stamps = [index[-1].time.to_sec() for index in self._connection_indexes.values() if index]
+            end_stamp = max(end_stamps) if end_stamps else 0
+        
+        return end_stamp
+    
+    def get_type_and_topic_info(self, topic_filters=None):
+        """
+        Coallates info about the type and topics in the bag.
+        
+        Note, it would be nice to filter by type as well, but there appear to be some limitations in the current architecture
+        that prevent that from working when more than one message type is written on the same topic.
+        
+        @param topic_filters: specify one or more topic to filter by.
+        @type topic_filters: either a single str or a list of str.
+        @return: generator of TypesAndTopicsTuple(types{key:type name, val: md5hash}, 
+            topics{type: msg type (Ex. "std_msgs/String"),
+                message_count: the number of messages of the particular type,
+                connections: the number of connections,
+                frequency: the data frequency,
+                key: type name,
+                val: md5hash}) describing the types of messages present
+            and information about the topics
+        @rtype: TypesAndTopicsTuple(dict(str, str), 
+            TopicTuple(str, int, int, float, str, str))
+        """
+        
+        datatypes = set()
+        datatype_infos = []
+        
+        for connection in self._connections.values():
+            if connection.datatype in datatypes:
+                continue
+            
+            datatype_infos.append((connection.datatype, connection.md5sum, connection.msg_def))
+            datatypes.add(connection.datatype)
+            
+        topics = []
+        # load our list of topics and optionally filter
+        if topic_filters is not None:
+            if not isinstance(topic_filters, list):
+                topic_filters = [topic_filters]
+                
+            topics = topic_filters
+        else:
+            topics = [c.topic for c in self._get_connections()]
+            
+        topics = sorted(set(topics))
+            
+        topic_datatypes = {}
+        topic_conn_counts = {}
+        topic_msg_counts = {}
+        topic_freqs_median = {}
+        
+        for topic in topics:
+            connections = list(self._get_connections(topic))
+            
+            if not connections:
+                continue
+                
+            topic_datatypes[topic] = connections[0].datatype
+            topic_conn_counts[topic] = len(connections)
+
+            msg_count = 0
+            for connection in connections:
+                for chunk in self._chunks:
+                    msg_count += chunk.connection_counts.get(connection.id, 0)
+                    
+            topic_msg_counts[topic] = msg_count
+
+            if self._connection_indexes_read:
+                stamps = [entry.time.to_sec() for entry in self._get_entries(connections)]
+                if len(stamps) > 1:
+                    periods = [s1 - s0 for s1, s0 in zip(stamps[1:], stamps[:-1])]
+                    med_period = _median(periods)
+                    if med_period > 0.0:
+                        topic_freqs_median[topic] = 1.0 / med_period
+
+        # process datatypes       
+        types = {}
+        for datatype, md5sum, msg_def in sorted(datatype_infos):
+            types[datatype] = md5sum
+            
+        # process topics
+        topics_t = {}
+        TopicTuple = collections.namedtuple("TopicTuple", ["msg_type", "message_count", "connections", "frequency"])
+        for topic in sorted(topic_datatypes.keys()):
+            topic_msg_count = topic_msg_counts[topic]
+            frequency = topic_freqs_median[topic] if topic in topic_freqs_median else None
+            topics_t[topic] = TopicTuple(msg_type=topic_datatypes[topic], 
+                                            message_count=topic_msg_count,
+                                            connections=topic_conn_counts[topic], 
+                                            frequency=frequency)
+            
+        return collections.namedtuple("TypesAndTopicsTuple", ["msg_types", "topics"])(msg_types=types, topics=topics_t)
+
+    def set_encryptor(self, encryptor=None, param=None):
+        if self._chunks:
+            raise ROSBagException('Cannot set encryptor after chunks are written')
+        if encryptor is None:
+            self._encryptor = _ROSBagNoEncryptor()
+        elif encryptor == _ROSBagAesCbcEncryptor.NAME:
+            if sys.platform == 'win32':
+                raise ROSBagEncryptNotSupportedException('AES CBC encryptor is not supported for Windows')
+            else:
+                self._encryptor = _ROSBagAesCbcEncryptor()
+        else:
+            self._encryptor = _ROSBagNoEncryptor()
+        self._encryptor.initialize(self, param)
+
+    def __str__(self):
+        rows = []
+
+        try:
+            if self._filename:
+                rows.append(('path', self._filename))
+
+            if self._version == 102 and type(self._reader) == _BagReader102_Unindexed:
+                rows.append(('version', '1.2 (unindexed)'))
+            else:
+                rows.append(('version', '%d.%d' % (int(self._version / 100), self._version % 100)))
+
+            if not self._connection_indexes and not self._chunks:
+                rows.append(('size', _human_readable_size(self.size)))
+            else:
+                if self._chunks:
+                    start_stamp = self._chunks[ 0].start_time.to_sec()
+                    end_stamp   = self._chunks[-1].end_time.to_sec()
+                else:
+                    start_stamps = [index[0].time.to_sec() for index in self._connection_indexes.values() if index]
+                    start_stamp = min(start_stamps) if start_stamps else 0
+                    end_stamps = [index[-1].time.to_sec() for index in self._connection_indexes.values() if index]
+                    end_stamp = max(end_stamps) if end_stamps else 0
+    
+                # Show duration
+                duration = end_stamp - start_stamp
+                dur_secs = duration % 60
+                dur_mins = int(duration / 60)
+                dur_hrs  = int(dur_mins / 60)
+                if dur_hrs > 0:
+                    dur_mins = dur_mins % 60
+                    duration_str = '%dhr %d:%02ds (%ds)' % (dur_hrs, dur_mins, dur_secs, duration)
+                elif dur_mins > 0:
+                    duration_str = '%d:%02ds (%ds)' % (dur_mins, dur_secs, duration)
+                else:
+                    duration_str = '%.1fs' % duration   
+
+                rows.append(('duration', duration_str))
+        
+                # Show start and end times
+                rows.append(('start', '%s (%.2f)' % (_time_to_str(start_stamp), start_stamp)))
+                rows.append(('end',   '%s (%.2f)' % (_time_to_str(end_stamp),   end_stamp)))
+    
+                rows.append(('size', _human_readable_size(self.size)))
+
+                if self._chunks:
+                    num_messages = 0
+                    for c in self._chunks:
+                        for counts in c.connection_counts.values():
+                            num_messages += counts
+                else:
+                    num_messages = sum([len(index) for index in self._connection_indexes.values()])
+                rows.append(('messages', str(num_messages)))
+
+                # Show compression information
+                if len(self._chunk_headers) == 0:
+                    rows.append(('compression', 'none'))
+                else:
+                    compression_counts       = {}
+                    compression_uncompressed = {}
+                    compression_compressed   = {}
+                    for chunk_header in self._chunk_headers.values():
+                        if chunk_header.compression not in compression_counts:
+                            compression_counts[chunk_header.compression] = 1
+                            compression_uncompressed[chunk_header.compression] = chunk_header.uncompressed_size
+                            compression_compressed[chunk_header.compression] = chunk_header.compressed_size
+                        else:
+                            compression_counts[chunk_header.compression] += 1
+                            compression_uncompressed[chunk_header.compression] += chunk_header.uncompressed_size
+                            compression_compressed[chunk_header.compression] += chunk_header.compressed_size
+    
+                    chunk_count = len(self._chunk_headers)
+    
+                    compressions = []
+                    for count, compression in reversed(sorted([(v, k) for k, v in compression_counts.items()])):
+                        if compression != Compression.NONE:
+                            fraction = (100.0 * compression_compressed[compression]) / compression_uncompressed[compression]
+                            compressions.append('%s [%d/%d chunks; %.2f%%]' % (compression, count, chunk_count, fraction))
+                        else:
+                            compressions.append('%s [%d/%d chunks]' % (compression, count, chunk_count))
+                    rows.append(('compression', ', '.join(compressions)))
+    
+                    all_uncompressed = (sum([count for c, count in compression_counts.items() if c != Compression.NONE]) == 0)
+                    if not all_uncompressed:    
+                        total_uncompressed_size = sum((h.uncompressed_size for h in self._chunk_headers.values()))
+                        total_compressed_size   = sum((h.compressed_size   for h in self._chunk_headers.values()))
+                        
+                        total_uncompressed_size_str = _human_readable_size(total_uncompressed_size)
+                        total_compressed_size_str   = _human_readable_size(total_compressed_size)
+                        total_size_str_length = max([len(total_uncompressed_size_str), len(total_compressed_size_str)])
+
+                        if duration > 0:
+                            uncompressed_rate_str = _human_readable_size(total_uncompressed_size / duration)
+                            compressed_rate_str   = _human_readable_size(total_compressed_size / duration)
+                            rate_str_length = max([len(uncompressed_rate_str), len(compressed_rate_str)])
+
+                            rows.append(('uncompressed', '%*s @ %*s/s' %
+                                         (total_size_str_length, total_uncompressed_size_str, rate_str_length, uncompressed_rate_str)))
+                            rows.append(('compressed',   '%*s @ %*s/s (%.2f%%)' %
+                                         (total_size_str_length, total_compressed_size_str,   rate_str_length, compressed_rate_str, (100.0 * total_compressed_size) / total_uncompressed_size)))
+                        else:
+                            rows.append(('uncompressed', '%*s' % (total_size_str_length, total_uncompressed_size_str)))
+                            rows.append(('compressed',   '%*s' % (total_size_str_length, total_compressed_size_str)))
+
+                self._encryptor.add_info_rows(rows)
+
+                datatypes = set()
+                datatype_infos = []
+                for connection in self._connections.values():
+                    if connection.datatype in datatypes:
+                        continue
+                    datatype_infos.append((connection.datatype, connection.md5sum, connection.msg_def))
+                    datatypes.add(connection.datatype)
+                    
+                topics = sorted(set([c.topic for c in self._get_connections()]))
+                topic_datatypes    = {}
+                topic_conn_counts  = {}
+                topic_msg_counts   = {}
+                topic_freqs_median = {}
+                for topic in topics:
+                    connections = list(self._get_connections(topic))
+
+                    topic_datatypes[topic] = connections[0].datatype
+                    topic_conn_counts[topic] = len(connections)
+
+                    msg_count = 0
+                    for connection in connections:
+                        if self._chunks:
+                            for chunk in self._chunks:
+                                msg_count += chunk.connection_counts.get(connection.id, 0)
+                        else:
+                            msg_count += len(self._connection_indexes.get(connection.id, []))
+                    topic_msg_counts[topic] = msg_count
+
+                    if self._connection_indexes_read:
+                        stamps = [entry.time.to_sec() for entry in self._get_entries(connections)]
+                        if len(stamps) > 1:
+                            periods = [s1 - s0 for s1, s0 in zip(stamps[1:], stamps[:-1])]
+                            med_period = _median(periods)
+                            if med_period > 0.0:
+                                topic_freqs_median[topic] = 1.0 / med_period
+
+                topics = sorted(topic_datatypes.keys())
+                max_topic_len       = max([len(topic) for topic in topics])
+                max_datatype_len    = max([len(datatype) for datatype in datatypes])
+                max_msg_count_len   = max([len('%d' % msg_count) for msg_count in topic_msg_counts.values()])
+                max_freq_median_len = max([len(_human_readable_frequency(freq)) for freq in topic_freqs_median.values()]) if len(topic_freqs_median) > 0 else 0
+
+                # Show datatypes       
+                for i, (datatype, md5sum, msg_def) in enumerate(sorted(datatype_infos)):
+                    s = '%-*s [%s]' % (max_datatype_len, datatype, md5sum)
+                    if i == 0:
+                        rows.append(('types', s))
+                    else:
+                        rows.append(('', s))
+                    
+                # Show topics
+                for i, topic in enumerate(topics):
+                    topic_msg_count = topic_msg_counts[topic]
+                    
+                    s = '%-*s   %*d %s' % (max_topic_len, topic, max_msg_count_len, topic_msg_count, 'msgs' if topic_msg_count > 1 else 'msg ')
+                    if topic in topic_freqs_median:
+                        s += ' @ %*s' % (max_freq_median_len, _human_readable_frequency(topic_freqs_median[topic]))
+                    else:
+                        s += '   %*s' % (max_freq_median_len, '')
+
+                    s += ' : %-*s' % (max_datatype_len, topic_datatypes[topic])
+                    if topic_conn_counts[topic] > 1:
+                        s += ' (%d connections)' % topic_conn_counts[topic]
+        
+                    if i == 0:
+                        rows.append(('topics', s))
+                    else:
+                        rows.append(('', s))
+        
+        except Exception as ex:
+            raise
+
+        first_column_width = max([len(field) for field, _ in rows]) + 1
+
+        s = ''
+        for (field, value) in rows:
+            if field:
+                s += '%-*s %s\n' % (first_column_width, field + ':', value)
+            else:
+                s += '%-*s %s\n' % (first_column_width, '', value)
+
+        return s.rstrip()
+
+    def _get_yaml_info(self, key=None):
+        s = ''
+
+        try:
+            if self._filename:
+                s += 'path: %s\n' % self._filename
+
+            if self._version == 102 and type(self._reader) == _BagReader102_Unindexed:
+                s += 'version: 1.2 (unindexed)\n'
+            else:
+                s += 'version: %d.%d\n' % (int(self._version / 100), self._version % 100)
+
+            if not self._connection_indexes and not self._chunks:
+                s += 'size: %d\n' % self.size
+                s += 'indexed: False\n'
+            else:
+                if self._chunks:
+                    start_stamp = self._chunks[ 0].start_time.to_sec()
+                    end_stamp   = self._chunks[-1].end_time.to_sec()
+                else:
+                    start_stamps = [index[0].time.to_sec() for index in self._connection_indexes.values() if index]
+                    start_stamp = min(start_stamps) if start_stamps else 0
+                    end_stamps = [index[-1].time.to_sec() for index in self._connection_indexes.values() if index]
+                    end_stamp = max(end_stamps) if end_stamps else 0
+                
+                duration = end_stamp - start_stamp
+                s += 'duration: %.6f\n' % duration
+                s += 'start: %.6f\n' % start_stamp
+                s += 'end: %.6f\n' % end_stamp
+                s += 'size: %d\n' % self.size
+                if self._chunks:
+                    num_messages = 0
+                    for c in self._chunks:
+                        for counts in c.connection_counts.values():
+                            num_messages += counts
+                else:
+                    num_messages = sum([len(index) for index in self._connection_indexes.values()])
+                s += 'messages: %d\n' % num_messages
+                s += 'indexed: True\n'
+
+                # Show compression information
+                if len(self._chunk_headers) == 0:
+                    s += 'compression: none\n'
+                else:
+                    compression_counts       = {}
+                    compression_uncompressed = {}
+                    compression_compressed   = {}
+                    for chunk_header in self._chunk_headers.values():
+                        if chunk_header.compression not in compression_counts:
+                            compression_counts[chunk_header.compression] = 1
+                            compression_uncompressed[chunk_header.compression] = chunk_header.uncompressed_size
+                            compression_compressed[chunk_header.compression] = chunk_header.compressed_size
+                        else:
+                            compression_counts[chunk_header.compression] += 1
+                            compression_uncompressed[chunk_header.compression] += chunk_header.uncompressed_size
+                            compression_compressed[chunk_header.compression] += chunk_header.compressed_size
+    
+                    chunk_count = len(self._chunk_headers)
+    
+                    main_compression_count, main_compression = list(reversed(sorted([(v, k) for k, v in compression_counts.items()])))[0]
+                    s += 'compression: %s\n' % str(main_compression)
+    
+                    all_uncompressed = (sum([count for c, count in compression_counts.items() if c != Compression.NONE]) == 0)
+                    if not all_uncompressed:    
+                        s += 'uncompressed: %d\n' % sum((h.uncompressed_size for h in self._chunk_headers.values()))
+                        s += 'compressed: %d\n' % sum((h.compressed_size for h in self._chunk_headers.values()))
+
+                s += self._encryptor.get_info_str()
+
+                datatypes = set()
+                datatype_infos = []
+                for connection in self._connections.values():
+                    if connection.datatype in datatypes:
+                        continue
+                    datatype_infos.append((connection.datatype, connection.md5sum, connection.msg_def))
+                    datatypes.add(connection.datatype)
+                    
+                topics = sorted(set([c.topic for c in self._get_connections()]))
+                topic_datatypes    = {}
+                topic_conn_counts  = {}
+                topic_msg_counts   = {}
+                topic_freqs_median = {}
+                for topic in topics:
+                    connections = list(self._get_connections(topic))
+
+                    topic_datatypes[topic] = connections[0].datatype
+                    topic_conn_counts[topic] = len(connections)
+
+                    msg_count = 0
+                    for connection in connections:
+                        for chunk in self._chunks:
+                            msg_count += chunk.connection_counts.get(connection.id, 0)
+                    topic_msg_counts[topic] = msg_count
+
+                    if self._connection_indexes_read:
+                        stamps = [entry.time.to_sec() for entry in self._get_entries(connections)]
+                        if len(stamps) > 1:
+                            periods = [s1 - s0 for s1, s0 in zip(stamps[1:], stamps[:-1])]
+                            med_period = _median(periods)
+                            if med_period > 0.0:
+                                topic_freqs_median[topic] = 1.0 / med_period
+
+                topics = sorted(topic_datatypes.keys())
+                max_topic_len       = max([len(topic) for topic in topics])
+                max_datatype_len    = max([len(datatype) for datatype in datatypes])
+                max_msg_count_len   = max([len('%d' % msg_count) for msg_count in topic_msg_counts.values()])
+                max_freq_median_len = max([len(_human_readable_frequency(freq)) for freq in topic_freqs_median.values()]) if len(topic_freqs_median) > 0 else 0
+
+                # Show datatypes       
+                s += 'types:\n'
+                for i, (datatype, md5sum, msg_def) in enumerate(sorted(datatype_infos)):
+                    s += '    - type: %s\n' % datatype
+                    s += '      md5: %s\n' % md5sum
+                    
+                # Show topics
+                s += 'topics:\n'
+                for i, topic in enumerate(topics):
+                    topic_msg_count = topic_msg_counts[topic]
+                    
+                    s += '    - topic: %s\n' % topic
+                    s += '      type: %s\n' % topic_datatypes[topic]
+                    s += '      messages: %d\n' % topic_msg_count
+                        
+                    if topic_conn_counts[topic] > 1:
+                        s += '      connections: %d\n' % topic_conn_counts[topic]
+
+                    if topic in topic_freqs_median:
+                        s += '      frequency: %.4f\n' % topic_freqs_median[topic]
+
+            if not key:
+                return s
+            
+            class DictObject(object):
+                def __init__(self, d):
+                    for a, b in d.items():
+                        if isinstance(b, (list, tuple)):
+                           setattr(self, a, [DictObject(x) if isinstance(x, dict) else x for x in b])
+                        else:
+                           setattr(self, a, DictObject(b) if isinstance(b, dict) else b)
+
+            obj = DictObject(yaml.safe_load(s))
+            try:
+                val = eval('obj.' + key)
+            except Exception as ex:
+                print('Error getting key "%s"' % key, file=sys.stderr)
+                return None
+
+            def print_yaml(val, indent=0):
+                indent_str = '  ' * indent
+
+                if type(val) is list:
+                    s = ''
+                    for item in val:
+                        s += '%s- %s\n' % (indent_str, print_yaml(item, indent + 1))
+                    return s
+                elif type(val) is DictObject:
+                    s = ''
+                    for i, (k, v) in enumerate(val.__dict__.items()):
+                        if i != 0:
+                            s += indent_str
+                        s += '%s: %s' % (k, str(v))
+                        if i < len(val.__dict__) - 1:
+                            s += '\n'
+                    return s
+                else:
+                    return indent_str + str(val)
+
+            return print_yaml(val)
+
+        except Exception as ex:
+            raise
+
+    ### Internal API ###
+
+    @property
+    def _has_compressed_chunks(self):
+        if not self._chunk_headers:
+            return False
+
+        return any((h.compression != Compression.NONE for h in self._chunk_headers.values()))
+
+    @property
+    def _uncompressed_size(self):
+        if not self._chunk_headers:
+            return self.size
+
+        return sum((h.uncompressed_size for h in self._chunk_headers.values()))
+
+    def _read_message(self, position, raw=False, return_connection_header=False):
+        """
+        Read the message from the given position in the file.
+        """
+        self.flush()
+        return self._reader.seek_and_read_message_data_record(position, raw, return_connection_header)
+
+    # Index accessing
+
+    def _get_connections(self, topics=None, connection_filter=None):
+        """
+        Yield the connections, optionally filtering by topic and/or connection information.
+        """
+        if topics:
+            if type(topics) is str:
+                topics = set([roslib.names.canonicalize_name(topics)])
+            else:
+                topics = set([roslib.names.canonicalize_name(t) for t in topics])
+
+        for c in self._connections.values():
+            if topics and c.topic not in topics and roslib.names.canonicalize_name(c.topic) not in topics:
+                continue
+            if connection_filter and not connection_filter(c.topic, c.datatype, c.md5sum, c.msg_def, c.header):
+                continue
+            yield c
+
+    def _get_entries(self, connections=None, start_time=None, end_time=None):
+        """
+        Yield index entries on the given connections in the given time range.
+        """
+        for entry in heapq.merge(*self._get_indexes(connections), key=lambda x: x.time.to_nsec()):
+            if start_time and entry.time < start_time:
+                continue
+            if end_time and entry.time > end_time:
+                return
+            yield entry
+
+    def _get_entries_reverse(self, connections=None, start_time=None, end_time=None):
+        """
+        Yield index entries on the given connections in the given time range in reverse order.
+        """
+        for entry in heapq.merge(*(reversed(index) for index in self._get_indexes(connections)),
+                                 key=lambda x: x.time.to_nsec(), reverse=True):
+            if end_time and entry.time > end_time:
+                continue
+            if start_time and entry.time < start_time:
+                return
+            yield entry
+
+    def _get_entry(self, t, connections=None):
+        """
+        Return the first index entry on/before the given time on the given connections
+        """
+        indexes = self._get_indexes(connections)
+
+        entry = _IndexEntry(t)
+
+        first_entry = None
+
+        for index in indexes:
+            i = bisect.bisect_right(index, entry) - 1
+            if i >= 0:
+                index_entry = index[i]
+                if first_entry is None or index_entry > first_entry:
+                    first_entry = index_entry
+
+        return first_entry
+    
+    def _get_entry_after(self, t, connections=None):
+        """
+        Return the first index entry after the given time on the given connections
+        """
+        indexes = self._get_indexes(connections)
+
+        entry = _IndexEntry(t)
+
+        first_entry = None
+
+        for index in indexes:
+            i = bisect.bisect_right(index, entry) 
+            if i <= len(index) - 1:
+                index_entry = index[i]
+                if first_entry is None or index_entry < first_entry:
+                    first_entry = index_entry
+
+        return first_entry
+
+    def _get_indexes(self, connections):
+        """
+        Get the indexes for the given connections.
+        """
+        if not self._connection_indexes_read:
+            self._reader._read_connection_index_records()
+
+        if connections is None:
+            return self._connection_indexes.values()
+        else:
+            return (self._connection_indexes[c.id] for c in connections)
+
+    ### Implementation ###
+
+    def _clear_index(self):
+        self._connection_indexes_read = False
+        self._connection_indexes      = {}    # id    -> IndexEntry[] (1.2+)
+
+        self._topic_connections  = {}    # topic -> connection_id
+        self._connections        = {}    # id -> ConnectionInfo
+        self._connection_count   = 0     # (2.0)
+        self._chunk_count        = 0     # (2.0)
+        self._chunks             = []    # ChunkInfo[] (2.0)
+        self._chunk_headers      = {}    # chunk_pos -> ChunkHeader (2.0)
+
+        self._chunk_open                    = False
+        self._curr_chunk_info               = None
+        self._curr_chunk_data_pos           = None
+        self._curr_chunk_connection_indexes = {}
+    
+    def _open(self, f, mode, allow_unindexed):
+        if not f:
+            raise ValueError('filename (or stream) is invalid')
+
+        try:
+            if   mode == 'r': self._open_read(f, allow_unindexed)
+            elif mode == 'w': self._open_write(f)
+            elif mode == 'a': self._open_append(f, allow_unindexed)
+            else:
+                raise ValueError('mode "%s" is invalid' % mode)
+            if 'b' not in self._file.mode and not isinstance('', bytes):
+                raise ROSBagException('In Python 3 the bag file must be opened in binary mode')
+        except struct.error:
+            raise ROSBagFormatException('error with bag')
+
+    def _is_file(self, f):
+        try:
+            return isinstance(f, file)  # Python 2
+        except NameError:
+            import io
+            return isinstance(f, io.IOBase)  # Python 3...this will return false in Python 2 always
+
+    def _open_read(self, f, allow_unindexed):
+        if self._is_file(f):
+            self._file     = f
+            self._filename = None
+        else:
+            self._file     = open(f, 'rb')
+            self._filename = f        
+
+        self._mode = 'r'
+
+        try:
+            self._version = self._read_version()
+        except:
+            self._version = None
+            self._close_file()
+            raise
+
+        try:
+            self._create_reader()
+            self._reader.start_reading()
+        except ROSBagUnindexedException as ex:
+            if not allow_unindexed:
+                self._close_file()
+                raise
+        except:
+            self._close_file()
+            raise
+
+    def _open_write(self, f):
+        if self._is_file(f):
+            self._file     = f
+            self._filename = None
+        else:
+            self._file     = open(f, 'w+b')
+            self._filename = f
+
+        self._mode = 'w'
+
+        self._version = 200
+
+        try:
+            self._create_reader()
+            self._start_writing()
+        except:
+            self._close_file()
+            raise
+
+    def _open_append(self, f, allow_unindexed):
+        if self._is_file(f):
+            self._file     = f
+            self._filename = None
+        else:        
+            try:
+                # Test if the file already exists
+                open(f, 'r').close()
+
+                # File exists: open in read with update mode
+                self._file = open(f, 'r+b')
+            except IOError:
+                # File doesn't exist: open in write mode
+                self._file = open(f, 'w+b')
+        
+            self._filename = f
+
+        self._mode = 'a'
+
+        try:
+            self._version = self._read_version()
+        except:
+            self._version = None
+            self._close_file()
+            raise
+
+        if self._version != 200:
+            self._file.close()
+            raise ROSBagException('bag version %d is unsupported for appending' % self._version)
+
+        try:
+            self._start_appending()
+        except ROSBagUnindexedException:
+            if not allow_unindexed:
+                self._close_file()
+                raise
+        except:
+            self._close_file()
+            raise
+
+    def _close_file(self):
+        self._file.close()
+        self._file = None
+
+    def _create_reader(self):
+        """
+        @raise ROSBagException: if the bag version is unsupported
+        """
+        major_version, minor_version = int(self._version / 100), self._version % 100
+        if major_version == 2:
+            self._reader = _BagReader200(self)
+        elif major_version == 1:
+            if minor_version == 1:
+                raise ROSBagException('unsupported bag version %d. Please convert bag to newer format.' % self._version)
+            else:
+                # Get the op code of the first record.  If it's FILE_HEADER, then we have an indexed 1.2 bag.
+                first_record_pos = self._file.tell()
+                header = _read_header(self._file)
+                op = _read_uint8_field(header, 'op')
+                self._file.seek(first_record_pos)
+    
+                if op == _OP_FILE_HEADER:
+                    self._reader = _BagReader102_Indexed(self)
+                else:
+                    self._reader = _BagReader102_Unindexed(self)
+        else:
+            raise ROSBagException('unknown bag version %d' % self._version)
+
+    def _read_version(self):
+        """
+        @raise ROSBagException: if the file is empty, or the version line can't be parsed
+        """
+        version_line = self._file.readline().rstrip().decode()
+        if len(version_line) == 0:
+            raise ROSBagException('empty file')
+        
+        matches = re.match("#ROS(.*) V(\d).(\d)", version_line)
+        if matches is None or len(matches.groups()) != 3:
+            raise ROSBagException('This does not appear to be a bag file')
+        
+        version_type, major_version_str, minor_version_str = matches.groups()
+
+        version = int(major_version_str) * 100 + int(minor_version_str)
+        
+        return version
+
+    def _start_writing(self):        
+        version = _VERSION + '\n'
+        version = version.encode()
+        self._file.write(version)
+        self._file_header_pos = self._file.tell()
+        self._write_file_header_record(0, 0, 0)
+
+    def _stop_writing(self):
+        # Write the open chunk (if any) to file
+        self.flush()
+
+        # When read and write operations are mixed (e.g. bags in 'a' mode might be opened in 'r+b' mode)
+        # it could cause problems on Windows:
+        # https://stackoverflow.com/questions/14279658/mixing-read-and-write-on-python-files-in-windows
+        # to fix this, f.seek(0, os.SEEK_CUR) needs to be added after a read() before the next write()
+        self._file.seek(0, os.SEEK_CUR)
+
+        # Remember this location as the start of the index
+        self._index_data_pos = self._file.tell()
+
+        # Write connection infos
+        for connection_info in self._connections.values():
+            # Encrypt connection records in index data (encrypt: True)
+            self._write_connection_record(connection_info, True)
+
+        # Write chunk infos
+        for chunk_info in self._chunks:
+            self._write_chunk_info_record(chunk_info)
+
+        # Re-write the file header
+        self._file.seek(self._file_header_pos)
+        self._write_file_header_record(self._index_data_pos, len(self._connections), len(self._chunks))
+
+    def _start_appending(self):
+        self._file_header_pos = self._file.tell()
+
+        self._create_reader()
+        self._reader.start_reading()
+
+        # Truncate the file to chop off the index
+        self._file.truncate(self._index_data_pos)
+        self._reader.index_data_pos = 0
+    
+        # Rewrite the file header, clearing the index position (so we know if the index is invalid)
+        self._file.seek(self._file_header_pos);
+        self._write_file_header_record(0, 0, 0)
+
+        # Seek to the end of the file
+        self._file.seek(0, os.SEEK_END)
+
+    def _start_writing_chunk(self, t):
+        self._curr_chunk_info = _ChunkInfo(self._file.tell(), t, t)
+        self._write_chunk_header(_ChunkHeader(self._compression, 0, 0))
+        self._curr_chunk_data_pos = self._file.tell()
+        self._set_compression_mode(self._compression)
+        self._chunk_open = True
+    
+    def _get_chunk_offset(self):
+        if self._compression == Compression.NONE:
+            return self._file.tell() - self._curr_chunk_data_pos
+        else:
+            return self._output_file.compressed_bytes_in
+
+    def _stop_writing_chunk(self):
+        # Add this chunk to the index
+        self._chunks.append(self._curr_chunk_info)
+
+        # Get the uncompressed and compressed sizes
+        uncompressed_size = self._get_chunk_offset()
+        self._set_compression_mode(Compression.NONE)
+        compressed_size = self._file.tell() - self._curr_chunk_data_pos
+
+        # When encryption is on, compressed_size represents encrypted chunk size;
+        # When decrypting, the actual compressed size can be deduced from the decrypted chunk
+        compressed_size = self._encryptor.encrypt_chunk(compressed_size, self._curr_chunk_data_pos, self._file)
+
+        # Rewrite the chunk header with the size of the chunk (remembering current offset)
+        end_of_chunk_pos = self._file.tell()
+        self._file.seek(self._curr_chunk_info.pos)
+        chunk_header = _ChunkHeader(self._compression, compressed_size, uncompressed_size, self._curr_chunk_data_pos)
+        self._write_chunk_header(chunk_header)
+        self._chunk_headers[self._curr_chunk_info.pos] = chunk_header
+
+        # Write out the connection indexes and clear them
+        self._file.seek(end_of_chunk_pos)
+        for connection_id, entries in self._curr_chunk_connection_indexes.items():
+            self._write_connection_index_record(connection_id, entries)
+        self._curr_chunk_connection_indexes.clear()
+
+        # Flag that we're starting a new chunk
+        self._chunk_open = False
+
+    def _set_compression_mode(self, compression):
+        # Flush the compressor, if needed
+        if self._curr_compression != Compression.NONE:
+            self._output_file.flush()
+        
+        # Create the compressor
+        if compression == Compression.BZ2:
+            self._output_file = _CompressorFileFacade(self._file, bz2.BZ2Compressor())
+        elif compression == Compression.LZ4 and found_lz4:
+            self._output_file = _CompressorFileFacade(self._file, roslz4.LZ4Compressor())
+        elif compression == Compression.NONE:
+            self._output_file = self._file
+        else:
+            raise ROSBagException('unsupported compression type: %s' % compression)
+
+        self._curr_compression = compression
+
+    def _write_file_header_record(self, index_pos, connection_count, chunk_count):
+        header = {
+            'op':          _pack_uint8(_OP_FILE_HEADER),
+            'index_pos':   _pack_uint64(index_pos),
+            'conn_count':  _pack_uint32(connection_count),
+            'chunk_count': _pack_uint32(chunk_count)
+        }
+        self._encryptor.add_fields_to_file_header(header)
+        _write_record(self._file, header, padded_size=_FILE_HEADER_LENGTH)
+
+    def _write_connection_record(self, connection_info, encrypt):
+        header = {
+            'op':    _pack_uint8(_OP_CONNECTION),
+            'topic': connection_info.topic,
+            'conn':  _pack_uint32(connection_info.id)
+        }
+        if encrypt:
+            self._encryptor.write_encrypted_header(_write_header, self._output_file, header)
+        else:
+            _write_header(self._output_file, header)
+
+        if encrypt:
+            self._encryptor.write_encrypted_header(_write_header, self._output_file, connection_info.header)
+        else:
+            _write_header(self._output_file, connection_info.header)
+
+    def _write_message_data_record(self, connection_id, t, serialized_bytes):
+        header = {
+            'op':   _pack_uint8(_OP_MSG_DATA),
+            'conn': _pack_uint32(connection_id),
+            'time': _pack_time(t)
+        }
+        _write_record(self._output_file, header, serialized_bytes)
+
+    def _write_chunk_header(self, chunk_header):
+        header = {
+            'op':          _pack_uint8(_OP_CHUNK),
+            'compression': chunk_header.compression,
+            'size':        _pack_uint32(chunk_header.uncompressed_size)
+        }
+        _write_header(self._file, header)
+
+        self._file.write(_pack_uint32(chunk_header.compressed_size))
+
+    def _write_connection_index_record(self, connection_id, entries):        
+        header = {
+            'op':    _pack_uint8(_OP_INDEX_DATA),
+            'conn':  _pack_uint32(connection_id),
+            'ver':   _pack_uint32(_INDEX_VERSION),
+            'count': _pack_uint32(len(entries))
+        }
+
+        buffer = self._buffer
+        buffer.seek(0)
+        buffer.truncate(0)            
+        for entry in entries:
+            buffer.write(_pack_time  (entry.time))
+            buffer.write(_pack_uint32(entry.offset))
+            
+        _write_record(self._file, header, buffer.getvalue())            
+
+    def _write_chunk_info_record(self, chunk_info):
+        header = {
+            'op':         _pack_uint8 (_OP_CHUNK_INFO),
+            'ver':        _pack_uint32(_CHUNK_INDEX_VERSION),
+            'chunk_pos':  _pack_uint64(chunk_info.pos),
+            'start_time': _pack_time(chunk_info.start_time),
+            'end_time':   _pack_time(chunk_info.end_time),
+            'count':      _pack_uint32(len(chunk_info.connection_counts))
+        }
+        
+        buffer = self._buffer
+        buffer.seek(0)
+        buffer.truncate(0)
+        for connection_id, count in chunk_info.connection_counts.items():
+            buffer.write(_pack_uint32(connection_id))
+            buffer.write(_pack_uint32(count))
+
+        _write_record(self._file, header, buffer.getvalue())    
+
+### Implementation ###
+
+_message_types = {}   # md5sum -> type
+
+_OP_MSG_DEF     = 0x01
+_OP_MSG_DATA    = 0x02
+_OP_FILE_HEADER = 0x03
+_OP_INDEX_DATA  = 0x04
+_OP_CHUNK       = 0x05
+_OP_CHUNK_INFO  = 0x06
+_OP_CONNECTION  = 0x07
+
+_OP_CODES = {
+    _OP_MSG_DEF:     'MSG_DEF',
+    _OP_MSG_DATA:    'MSG_DATA',
+    _OP_FILE_HEADER: 'FILE_HEADER',
+    _OP_INDEX_DATA:  'INDEX_DATA',
+    _OP_CHUNK:       'CHUNK',
+    _OP_CHUNK_INFO:  'CHUNK_INFO',
+    _OP_CONNECTION:  'CONNECTION'
+}
+
+_VERSION             = '#ROSBAG V2.0'
+_FILE_HEADER_LENGTH  = 4096
+_INDEX_VERSION       = 1
+_CHUNK_INDEX_VERSION = 1
+
+class _ConnectionInfo(object):
+    def __init__(self, id, topic, header):
+        try:
+            datatype = _read_str_field(header, 'type')
+            md5sum   = _read_str_field(header, 'md5sum')
+            msg_def  = _read_str_field(header, 'message_definition')
+        except KeyError as ex:
+            raise ROSBagFormatException('connection header field %s not found' % str(ex))
+
+        self.id       = id
+        self.topic    = topic
+        self.datatype = datatype
+        self.md5sum   = md5sum
+        self.msg_def  = msg_def
+        self.header   = header
+
+    def __str__(self):
+        return '%d on %s: %s' % (self.id, self.topic, str(self.header))
+
+class _ChunkInfo(object):
+    def __init__(self, pos, start_time, end_time):
+        self.pos        = pos
+        self.start_time = start_time
+        self.end_time   = end_time
+        
+        self.connection_counts = {}
+
+    def __str__(self):
+        s  = 'chunk_pos:   %d\n' % self.pos
+        s += 'start_time:  %s\n' % str(self.start_time)
+        s += 'end_time:    %s\n' % str(self.end_time)
+        s += 'connections: %d\n' % len(self.connection_counts)
+        s += '\n'.join(['  - [%4d] %d' % (connection_id, count) for connection_id, count in self.connection_counts.items()])
+        return s
+
+class _ChunkHeader(object):
+    def __init__(self, compression, compressed_size, uncompressed_size, data_pos=0):
+        self.compression       = compression
+        self.compressed_size   = compressed_size
+        self.uncompressed_size = uncompressed_size
+        self.data_pos          = data_pos
+
+    def __str__(self):
+        if self.uncompressed_size > 0:
+            ratio = 100 * (float(self.compressed_size) / self.uncompressed_size)
+            return 'compression: %s, size: %d, uncompressed: %d (%.2f%%)' % (self.compression, self.compressed_size, self.uncompressed_size, ratio)
+        else:
+            return 'compression: %s, size: %d, uncompressed: %d' % (self.compression, self.compressed_size, self.uncompressed_size)
+
+class ComparableMixin(object):
+    __slots__ = []
+
+    def _compare(self, other, method):
+        try:
+            return method(self._cmpkey(), other._cmpkey())
+        except (AttributeError, TypeError):
+            # _cmpkey not implemented or return different type
+            # so can not compare with other
+            return NotImplemented
+
+    def __lt__(self, other):
+        return self._compare(other, lambda s, o: s < o)
+
+    def __le__(self, other):
+        return self._compare(other, lambda s, o: s <= o)
+
+    def __eq__(self, other):
+        return self._compare(other, lambda s, o: s == o)
+
+    def __ge__(self, other):
+        return self._compare(other, lambda s, o: s >= o)
+
+    def __gt__(self, other):
+        return self._compare(other, lambda s, o: s > o)
+
+    def __ne__(self, other):
+        return self._compare(other, lambda s, o: s != o)
+
+class _IndexEntry(ComparableMixin):
+    __slots__ = ['time']
+
+    def __init__(self, time):
+        self.time = time
+
+    def _cmpkey(self):
+        return self.time
+
+class _IndexEntry102(_IndexEntry):
+    __slots__ = ['offset']
+
+    def __init__(self, time, offset):
+        self.time   = time
+        self.offset = offset
+        
+    @property
+    def position(self):
+        return self.offset
+        
+    def __str__(self):
+        return '%d.%d: %d' % (self.time.secs, self.time.nsecs, self.offset)
+
+class _IndexEntry200(_IndexEntry):
+    __slots__ = ['chunk_pos', 'offset']
+
+    def __init__(self, time, chunk_pos, offset):
+        self.time      = time
+        self.chunk_pos = chunk_pos
+        self.offset    = offset
+
+    @property
+    def position(self):
+        return (self.chunk_pos, self.offset)
+
+    def __str__(self):
+        return '%d.%d: %d+%d' % (self.time.secs, self.time.nsecs, self.chunk_pos, self.offset)
+    
+def _get_message_type(info):
+    message_type = _message_types.get(info.md5sum)
+    if message_type is None:
+        try:
+            message_type = genpy.dynamic.generate_dynamic(info.datatype, info.msg_def)[info.datatype]
+            if (message_type._md5sum != info.md5sum):
+                print('WARNING: For type [%s] stored md5sum [%s] does not match message definition [%s].\n  Try: "rosrun rosbag fix_msg_defs.py old_bag new_bag."'%(info.datatype, info.md5sum, message_type._md5sum), file=sys.stderr)
+        except genmsg.InvalidMsgSpec:
+            message_type = genpy.dynamic.generate_dynamic(info.datatype, "")[info.datatype]
+            print('WARNING: For type [%s] stored md5sum [%s] has invalid message definition."'%(info.datatype, info.md5sum), file=sys.stderr)
+        except genmsg.MsgGenerationException as ex:
+            raise ROSBagException('Error generating datatype %s: %s' % (info.datatype, str(ex)))
+
+        _message_types[info.md5sum] = message_type
+
+    return message_type
+
+def _read_uint8 (f): return _unpack_uint8 (f.read(1))
+def _read_uint32(f): return _unpack_uint32(f.read(4))
+def _read_uint64(f): return _unpack_uint64(f.read(8))
+def _read_time  (f): return _unpack_time  (f.read(8))
+
+def _decode_bytes(v):  return v
+def _decode_str(v):    return v if type(v) is str else v.decode()
+def _unpack_uint8(v):  return struct.unpack('<B', v)[0]
+def _unpack_uint32(v): return struct.unpack('<L', v)[0]
+def _unpack_uint64(v): return struct.unpack('<Q', v)[0]
+def _unpack_time(v):   return rospy.Time(*struct.unpack('<LL', v))
+
+def _pack_uint8(v):  return struct.pack('<B', v)
+def _pack_uint32(v): return struct.pack('<L', v)
+def _pack_uint64(v): return struct.pack('<Q', v)
+def _pack_time(v):   return _pack_uint32(v.secs) + _pack_uint32(v.nsecs)
+
+def _read(f, size):
+    data = f.read(size)
+    if len(data) != size:
+        raise ROSBagException('expecting %d bytes, read %d' % (size, len(data)))   
+    return data
+
+def _skip_record(f):
+    _skip_sized(f)  # skip header
+    _skip_sized(f)  # skip data
+
+def _skip_sized(f):
+    size = _read_uint32(f)
+    f.seek(size, os.SEEK_CUR)
+
+def _read_sized(f):
+    try:
+        size = _read_uint32(f)
+    except struct.error as ex:
+        raise ROSBagFormatException('error unpacking uint32: %s' % str(ex))
+    return _read(f, size)
+
+def _write_sized(f, v):
+    if not isinstance(v, bytes):
+        v = v.encode()
+    f.write(_pack_uint32(len(v)))
+    f.write(v)
+
+def _read_field(header, field, unpack_fn):
+    if field not in header:
+        raise ROSBagFormatException('expected "%s" field in record' % field)
+    
+    try:
+        value = unpack_fn(header[field])
+    except Exception as ex:
+        raise ROSBagFormatException('error reading field "%s": %s' % (field, str(ex)))
+    
+    return value
+
+def _read_bytes_field (header, field): return _read_field(header, field, _decode_bytes)
+def _read_str_field   (header, field): return _read_field(header, field, _decode_str)
+def _read_uint8_field (header, field): return _read_field(header, field, _unpack_uint8)
+def _read_uint32_field(header, field): return _read_field(header, field, _unpack_uint32)
+def _read_uint64_field(header, field): return _read_field(header, field, _unpack_uint64)
+def _read_time_field  (header, field): return _read_field(header, field, _unpack_time)
+
+def _write_record(f, header, data='', padded_size=None):
+    header_str = _write_header(f, header)
+
+    if padded_size is not None:
+        header_len = len(header_str)
+        if header_len < padded_size:
+            data = ' ' * (padded_size - header_len)
+        else:
+            data = ''
+
+    _write_sized(f, data)
+
+def _write_header(f, header):
+    header_str = b''
+    equal = b'='
+    for k, v in header.items():
+        if not isinstance(k, bytes):
+            k = k.encode()
+        if not isinstance(v, bytes):
+            v = v.encode()
+        header_str += _pack_uint32(len(k) + 1 + len(v)) + k + equal + v
+    _write_sized(f, header_str)
+    return header_str
+
+def _read_header(f, req_op=None):
+    bag_pos = f.tell()
+
+    # Read header
+    try:
+        header = _read_sized(f)
+    except ROSBagException as ex:
+        raise ROSBagFormatException('Error reading header: %s' % str(ex))
+
+    return _build_header_from_str(header, req_op)
+
+def _build_header_from_str(header, req_op):
+    # Parse header into a dict
+    header_dict = {}
+    while header != b'':
+        # Read size
+        if len(header) < 4:
+            raise ROSBagFormatException('Error reading header field')           
+        (size,) = struct.unpack('<L', header[:4])                          # @todo reindex: catch struct.error
+        header = header[4:]
+
+        # Read bytes
+        if len(header) < size:
+            raise ROSBagFormatException('Error reading header field: expected %d bytes, read %d' % (size, len(header)))
+        (name, sep, value) = header[:size].partition(b'=')
+        if sep == b'':
+            raise ROSBagFormatException('Error reading header field')
+
+        name = name.decode()
+        header_dict[name] = value                                          # @todo reindex: raise exception on empty name
+        
+        header = header[size:]
+
+    # Check the op code of the header, if supplied
+    if req_op is not None:
+        op = _read_uint8_field(header_dict, 'op')
+        if req_op != op:
+            raise ROSBagFormatException('Expected op code: %s, got %s' % (_OP_CODES[req_op], _OP_CODES[op]))
+
+    return header_dict
+
+def _peek_next_header_op(f):
+    pos = f.tell()
+    header = _read_header(f)
+    op = _read_uint8_field(header, 'op')
+    f.seek(pos)
+    return op
+
+def _read_record_data(f):
+    try:
+        record_data = _read_sized(f)
+    except ROSBagException as ex:
+        raise ROSBagFormatException('Error reading record data: %s' % str(ex))
+
+    return record_data
+
+class _BagReader(object):
+    def __init__(self, bag):
+        self.bag = bag
+        
+    def start_reading(self):
+        raise NotImplementedError()
+
+    def read_messages(self, topics, start_time, end_time, connection_filter, raw, return_connection_header=False):
+        raise NotImplementedError()
+
+    def reindex(self):
+        raise NotImplementedError()
+
+class _BagReader102_Unindexed(_BagReader):
+    """
+    Support class for reading unindexed v1.2 bag files.
+    """
+    def __init__(self, bag):
+        _BagReader.__init__(self, bag)
+        
+    def start_reading(self):
+        self.bag._file_header_pos = self.bag._file.tell()
+
+    def reindex(self):
+        """Generates all bag index information by rereading the message records."""
+        f = self.bag._file
+        
+        total_bytes = self.bag.size
+        
+        # Re-read the file header to get to the start of the first message
+        self.bag._file.seek(self.bag._file_header_pos)
+
+        offset = f.tell()
+
+        # Read message definition and data records
+        while offset < total_bytes:
+            yield offset
+            
+            op = _peek_next_header_op(f)
+
+            if op == _OP_MSG_DEF:
+                connection_info = self.read_message_definition_record()
+    
+                if connection_info.topic not in self.bag._topic_connections:
+                    self.bag._topic_connections[connection_info.topic] = connection_info.id
+                    self.bag._connections[connection_info.id]          = connection_info
+                    self.bag._connection_indexes[connection_info.id]   = []
+
+            elif op == _OP_MSG_DATA:
+                # Read the topic and timestamp from the header
+                header = _read_header(f)
+                
+                topic = _read_str_field(header, 'topic')
+                secs  = _read_uint32_field(header, 'sec')
+                nsecs = _read_uint32_field(header, 'nsec')
+                t = genpy.Time(secs, nsecs)
+
+                if topic not in self.bag._topic_connections:
+                    datatype = _read_str_field(header, 'type')
+                    self._create_connection_info_for_datatype(topic, datatype)
+
+                connection_id = self.bag._topic_connections[topic]
+                info = self.bag._connections[connection_id]
+
+                # Skip over the message content
+                _skip_sized(f)
+
+                # Insert the message entry (in order) into the connection index
+                bisect.insort_right(self.bag._connection_indexes[connection_id], _IndexEntry102(t, offset))
+            
+            offset = f.tell()
+
+    def read_messages(self, topics, start_time, end_time, topic_filter, raw, return_connection_header=False):
+        f = self.bag._file
+
+        f.seek(self.bag._file_header_pos)
+
+        while True:
+            # Read MSG_DEF records
+            while True:
+                position = f.tell()
+                
+                try:
+                    header = _read_header(f)
+                except Exception:
+                    return
+
+                op = _read_uint8_field(header, 'op')
+                if op != _OP_MSG_DEF:
+                    break
+
+                connection_info = self.read_message_definition_record(header)
+                
+                if connection_info.topic not in self.bag._topic_connections:
+                    self.bag._topic_connections[connection_info.topic] = connection_info.id
+
+                self.bag._connections[connection_info.id] = connection_info
+
+            # Check that we have a MSG_DATA record
+            if op != _OP_MSG_DATA:
+                raise ROSBagFormatException('Expecting OP_MSG_DATA, got %d' % op)
+
+            topic = _read_str_field(header, 'topic')
+            
+            if topic not in self.bag._topic_connections:
+                datatype = _read_str_field(header, 'type')
+                self._create_connection_info_for_datatype(topic, datatype)
+
+            connection_id = self.bag._topic_connections[topic]
+            info = self.bag._connections[connection_id]
+    
+            # Get the message type
+            try:
+                msg_type = _get_message_type(info)
+            except KeyError:
+                raise ROSBagException('Cannot deserialize messages of type [%s].  Message was not preceded in bagfile by definition' % info.datatype)
+
+            # Get the timestamp
+            secs  = _read_uint32_field(header, 'sec')
+            nsecs = _read_uint32_field(header, 'nsec')
+            t = genpy.Time(secs, nsecs)
+
+            # Read the message content
+            data = _read_record_data(f)
+            
+            if raw:
+                msg = (info.datatype, data, info.md5sum, position, msg_type)
+            else:
+                # Deserialize the message
+                msg = msg_type()
+                msg.deserialize(data)
+
+            if return_connection_header:
+                yield BagMessageWithConnectionHeader(topic, msg, t, info.header)
+            else:
+                yield BagMessage(topic, msg, t)
+
+        self.bag._connection_indexes_read = True
+
+    def _create_connection_info_for_datatype(self, topic, datatype):
+        for c in self.bag._connections.values():
+            if c.datatype == datatype:
+                connection_id     = len(self.bag._connections)
+                connection_header = { 'topic' : topic, 'type' : c.header['type'], 'md5sum' : c.header['md5sum'], 'message_definition' : c.header['message_definition'] }
+                connection_info   = _ConnectionInfo(connection_id, topic, connection_header)
+
+                self.bag._topic_connections[topic]          = connection_id
+                self.bag._connections[connection_id]        = connection_info
+                self.bag._connection_indexes[connection_id] = []
+                return
+
+        raise ROSBagFormatException('Topic %s of datatype %s not preceded by message definition' % (topic, datatype))
+
+    def read_message_definition_record(self, header=None):
+        if not header:
+            header = _read_header(self.bag._file, _OP_MSG_DEF)
+
+        topic    = _read_str_field(header, 'topic')
+        datatype = _read_str_field(header, 'type')
+        md5sum   = _read_str_field(header, 'md5')
+        msg_def  = _read_str_field(header, 'def')
+
+        _skip_sized(self.bag._file)  # skip the record data
+
+        connection_header = { 'topic' : topic, 'type' : datatype, 'md5sum' : md5sum, 'message_definition' : msg_def }
+
+        id = len(self.bag._connections)
+
+        return _ConnectionInfo(id, topic, connection_header)
+
+class _BagReader102_Indexed(_BagReader102_Unindexed):
+    """
+    Support class for reading indexed v1.2 bag files.
+    """
+    def __init__(self, bag):
+        _BagReader.__init__(self, bag)
+
+    def read_messages(self, topics, start_time, end_time, connection_filter, raw):
+        connections = self.bag._get_connections(topics, connection_filter)
+        for entry in self.bag._get_entries(connections, start_time, end_time):
+            yield self.seek_and_read_message_data_record(entry.offset, raw)
+
+    def reindex(self):
+        """Generates all bag index information by rereading the message records."""
+        f = self.bag._file
+        
+        total_bytes = self.bag.size
+        
+        # Re-read the file header to get to the start of the first message
+        self.bag._file.seek(self.bag._file_header_pos)
+        self.read_file_header_record()
+
+        offset = f.tell()
+
+        # Read message definition and data records
+        while offset < total_bytes:
+            yield offset
+            
+            op = _peek_next_header_op(f)
+
+            if op == _OP_MSG_DEF:
+                connection_info = self.read_message_definition_record()
+    
+                if connection_info.topic not in self.bag._topic_connections:
+                    self.bag._topic_connections[connection_info.topic] = connection_info.id
+                    self.bag._connections[connection_info.id] = connection_info
+                    self.bag._connection_indexes[connection_info.id] = []
+
+            elif op == _OP_MSG_DATA:
+                # Read the topic and timestamp from the header
+                header = _read_header(f)
+                
+                topic = _read_str_field(header, 'topic')
+                secs  = _read_uint32_field(header, 'sec')
+                nsecs = _read_uint32_field(header, 'nsec')
+                t = genpy.Time(secs, nsecs)
+
+                if topic not in self.bag._topic_connections:
+                    datatype = _read_str_field(header, 'type')
+                    self._create_connection_info_for_datatype(topic, datatype)
+
+                connection_id = self.bag._topic_connections[topic]
+                info = self.bag._connections[connection_id]
+
+                # Skip over the message content
+                _skip_sized(f)
+
+                # Insert the message entry (in order) into the connection index
+                bisect.insort_right(self.bag._connection_indexes[connection_id], _IndexEntry102(t, offset))
+
+            elif op == _OP_INDEX_DATA:
+                _skip_record(f)
+
+            offset = f.tell()
+
+    def start_reading(self):
+        try:
+            # Read the file header
+            self.read_file_header_record()
+            
+            total_bytes = self.bag.size
+    
+            # Check if the index position has been written, i.e. the bag was closed successfully
+            if self.bag._index_data_pos == 0:
+                raise ROSBagUnindexedException()
+    
+            # Seek to the beginning of the topic index records
+            self.bag._file.seek(self.bag._index_data_pos)
+
+            # Read the topic indexes
+            topic_indexes = {}
+            while True:
+                pos = self.bag._file.tell()
+                if pos >= total_bytes:
+                    break
+
+                topic, index = self.read_topic_index_record()
+
+                topic_indexes[topic] = index
+
+            # Read the message definition records (one for each topic)
+            for topic, index in topic_indexes.items():
+                self.bag._file.seek(index[0].offset)
+    
+                connection_info = self.read_message_definition_record()
+    
+                if connection_info.topic not in self.bag._topic_connections:
+                    self.bag._topic_connections[connection_info.topic] = connection_info.id
+                self.bag._connections[connection_info.id] = connection_info
+    
+                self.bag._connection_indexes[connection_info.id] = index
+
+            self.bag._connection_indexes_read = True
+
+        except Exception as ex:
+            raise ROSBagUnindexedException()
+
+    def read_file_header_record(self):
+        self.bag._file_header_pos = self.bag._file.tell()
+
+        header = _read_header(self.bag._file, _OP_FILE_HEADER)
+
+        self.bag._index_data_pos = _read_uint64_field(header, 'index_pos')
+
+        _skip_sized(self.bag._file)  # skip the record data, i.e. padding
+
+    def read_topic_index_record(self):
+        f = self.bag._file
+
+        header = _read_header(f, _OP_INDEX_DATA)
+
+        index_version = _read_uint32_field(header, 'ver')
+        topic         = _read_str_field   (header, 'topic')
+        count         = _read_uint32_field(header, 'count')
+        
+        if index_version != 0:
+            raise ROSBagFormatException('expecting index version 0, got %d' % index_version)
+    
+        _read_uint32(f) # skip the record data size
+
+        topic_index = []
+                
+        for i in range(count):
+            time   = _read_time  (f)
+            offset = _read_uint64(f)
+            
+            topic_index.append(_IndexEntry102(time, offset))
+            
+        return (topic, topic_index)
+
+    def seek_and_read_message_data_record(self, position, raw, return_connection_header=False):
+        f = self.bag._file
+
+        # Seek to the message position
+        f.seek(position)
+
+        # Skip any MSG_DEF records
+        while True:
+            header = _read_header(f)
+            op = _read_uint8_field(header, 'op')
+            if op != _OP_MSG_DEF:
+                break
+            _skip_sized(f)
+
+        # Check that we have a MSG_DATA record
+        if op != _OP_MSG_DATA:
+            raise ROSBagFormatException('Expecting OP_MSG_DATA, got %d' % op)
+        
+        topic = _read_str_field(header, 'topic')
+
+        connection_id = self.bag._topic_connections[topic]
+        info = self.bag._connections[connection_id]
+
+        # Get the message type
+        try:
+            msg_type = _get_message_type(info)
+        except KeyError:
+            raise ROSBagException('Cannot deserialize messages of type [%s].  Message was not preceded in bagfile by definition' % info.datatype)
+
+        # Get the timestamp
+        secs  = _read_uint32_field(header, 'sec')
+        nsecs = _read_uint32_field(header, 'nsec')
+        t = genpy.Time(secs, nsecs)
+
+        # Read the message content
+        data = _read_record_data(f)
+        
+        if raw:
+            msg = info.datatype, data, info.md5sum, position, msg_type
+        else:
+            # Deserialize the message
+            msg = msg_type()
+            msg.deserialize(data)
+        
+        if return_connection_header:
+            return BagMessageWithConnectionHeader(topic, msg, t, header)
+        else:
+            return BagMessage(topic, msg, t)
+
+class _BagReader200(_BagReader):
+    """
+    Support class for reading v2.0 bag files.
+    """
+    def __init__(self, bag):
+        _BagReader.__init__(self, bag)
+        
+        self.decompressed_chunk_pos = None
+        self.decompressed_chunk     = None
+        self.decompressed_chunk_io  = None
+
+    def reindex(self):
+        """
+        Generates all bag index information by rereading the chunks.
+        Assumes the file header has been read.
+        """
+        f = self.bag._file
+
+        f.seek(0, os.SEEK_END)
+        total_bytes = f.tell()
+
+        # Read any connection records from after the chunk section.
+        # This is to workaround a bug in rosbag record --split (fixed in r10390)
+        # where connection records weren't always being written inside the chunk.
+        self._read_terminal_connection_records()
+
+        # Re-read the file header to get to the start of the first chunk
+        self.bag._file.seek(self.bag._file_header_pos)
+        self.read_file_header_record()
+
+        trunc_pos = None
+
+        while True:
+            chunk_pos = f.tell()
+            if chunk_pos >= total_bytes:
+                break
+            yield chunk_pos
+
+            try:
+                self._reindex_read_chunk(f, chunk_pos, total_bytes)
+            except Exception as ex:
+                break
+            
+            trunc_pos = f.tell()
+
+        if trunc_pos and trunc_pos < total_bytes:
+            f.truncate(trunc_pos)
+            f.seek(trunc_pos)
+
+    def _reindex_read_chunk(self, f, chunk_pos, total_bytes):
+        # Read the chunk header
+        chunk_header = self.read_chunk_header()
+
+        # If the chunk header size is 0, then the chunk wasn't correctly terminated - we're done
+        if chunk_header.compressed_size == 0:
+            raise ROSBagException('unterminated chunk at %d' % chunk_pos)
+
+        if chunk_header.compression == Compression.NONE:
+            encrypted_chunk = _read(f, chunk_header.compressed_size)
+
+            chunk = self.bag._encryptor.decrypt_chunk(encrypted_chunk)
+
+            if self.decompressed_chunk_io:
+                self.decompressed_chunk_io.close()
+            self.decompressed_chunk_io = StringIO(chunk)
+
+            chunk_file = self.decompressed_chunk_io
+
+        else:
+            # Read the chunk, and decrypt/decompress it
+            encrypted_chunk = _read(f, chunk_header.compressed_size)
+
+            compressed_chunk = self.bag._encryptor.decrypt_chunk(encrypted_chunk)
+
+            # Decompress it
+            if chunk_header.compression == Compression.BZ2:
+                self.decompressed_chunk = bz2.decompress(compressed_chunk)
+            elif chunk_header.compression == Compression.LZ4 and found_lz4:
+                self.decompressed_chunk = roslz4.decompress(compressed_chunk)
+            else:
+                raise ROSBagException('unsupported compression type: %s' % chunk_header.compression)
+
+            if self.decompressed_chunk_io:
+                self.decompressed_chunk_io.close()
+            self.decompressed_chunk_io = StringIO(self.decompressed_chunk)
+
+            chunk_file = self.decompressed_chunk_io
+
+        # Read chunk connection and message records
+        self.bag._curr_chunk_info = None
+
+        offset = chunk_file.tell()
+
+        expected_index_length = 0
+
+        while offset < chunk_header.uncompressed_size:
+            op = _peek_next_header_op(chunk_file)
+            if op == _OP_CONNECTION:
+                # Connection records in chunk are not encrypted (encrypt: False)
+                connection_info = self.read_connection_record(chunk_file, False)
+
+                if connection_info.id not in self.bag._connections:
+                    self.bag._connections[connection_info.id] = connection_info
+                if connection_info.id not in self.bag._connection_indexes:
+                    self.bag._connection_indexes[connection_info.id] = []
+
+            elif op == _OP_MSG_DATA:
+                # Read the connection id and timestamp from the header
+                header = _read_header(chunk_file)
+
+                connection_id = _read_uint32_field(header, 'conn')
+                t             = _read_time_field  (header, 'time')
+
+                # Update the chunk info with this timestamp
+                if not self.bag._curr_chunk_info:
+                    self.bag._curr_chunk_info = _ChunkInfo(chunk_pos, t, t)
+                else:
+                    if t > self.bag._curr_chunk_info.end_time:
+                        self.bag._curr_chunk_info.end_time = t
+                    elif t < self.bag._curr_chunk_info.start_time:
+                        self.bag._curr_chunk_info.start_time = t
+                if connection_id in self.bag._curr_chunk_info.connection_counts:
+                    self.bag._curr_chunk_info.connection_counts[connection_id] += 1
+                else:
+                    self.bag._curr_chunk_info.connection_counts[connection_id] = 1
+
+                # Skip over the message content
+                _skip_sized(chunk_file)
+
+                # Insert the message entry (in order) into the connection index
+                if connection_id not in self.bag._connection_indexes:
+                    raise ROSBagException('connection id (id=%d) in chunk at position %d not preceded by connection record' % (connection_id, chunk_pos))
+                bisect.insort_right(self.bag._connection_indexes[connection_id], _IndexEntry200(t, chunk_pos, offset)) 
+
+                expected_index_length += 1
+
+            else:
+                # Unknown record type so skip
+                _skip_record(chunk_file)
+
+            offset = chunk_file.tell()
+
+        # Skip over index records, connection records and chunk info records
+        next_op = _peek_next_header_op(f)
+        
+        total_index_length = 0
+        
+        while next_op != _OP_CHUNK:
+            if next_op == _OP_INDEX_DATA:
+                # Bug workaround: C Turtle bags (pre-1.1.15) were written with an incorrect data length
+                _, index = self.read_connection_index_record()
+                total_index_length += len(index)
+            else:
+                _skip_record(f)
+
+            if f.tell() >= total_bytes:
+                if total_index_length != expected_index_length:
+                    raise ROSBagException('index shorter than expected (%d vs %d)' % (total_index_length, expected_index_length))
+                break
+
+            next_op = _peek_next_header_op(f)
+
+        # Chunk was read correctly - store info
+        self.bag._chunk_headers[chunk_pos] = chunk_header
+        self.bag._chunks.append(self.bag._curr_chunk_info)
+
+    def _read_terminal_connection_records(self):
+        b, f, r = self.bag, self.bag._file, self.bag._reader
+
+        # Seek to the first record after FILE_HEADER
+        f.seek(b._file_header_pos)
+        r.read_file_header_record()
+
+        # Advance to the first CONNECTION
+        if self._advance_to_next_record(_OP_CONNECTION):
+            # Read the CONNECTION records
+            while True:
+                connection_info = r.read_connection_record(f, False)
+
+                b._connections[connection_info.id] = connection_info
+                b._connection_indexes[connection_info.id] = []
+
+                next_op = _peek_next_header_op(f)
+                if next_op != _OP_CONNECTION:
+                    break
+
+    def _advance_to_next_record(self, op):
+        b, f = self.bag, self.bag._file
+
+        try:
+            while True:
+                next_op = _peek_next_header_op(f)
+                if next_op == op:
+                    break
+
+                if next_op == _OP_INDEX_DATA:
+                    # Workaround the possible invalid data length in INDEX_DATA records
+
+                    # read_connection_index_record() requires _curr_chunk_info is set
+                    if b._curr_chunk_info is None:
+                        b._curr_chunk_info = _ChunkInfo(0, rospy.Time(0, 1), rospy.Time(0, 1))
+
+                    b._reader.read_connection_index_record()
+                else:
+                    _skip_record(f)
+
+            return True
+
+        except Exception as ex:
+            return False
+
+    def start_reading(self):
+        try:
+            # Read the file header
+            self.read_file_header_record()
+    
+            # Check if the index position has been written, i.e. the bag was closed successfully
+            if self.bag._index_data_pos == 0:
+                raise ROSBagUnindexedException()
+    
+            # Seek to the end of the chunks
+            self.bag._file.seek(self.bag._index_data_pos)
+
+            # Read the connection records
+            self.bag._connection_indexes = {}
+            for i in range(self.bag._connection_count):
+                # Connection records in index data are encrypted (encrypt: True)
+                connection_info = self.read_connection_record(self.bag._file, True)
+                self.bag._connections[connection_info.id] = connection_info
+                self.bag._connection_indexes[connection_info.id] = []
+
+            # Read the chunk info records
+            self.bag._chunks = [self.read_chunk_info_record() for i in range(self.bag._chunk_count)]
+    
+            # Read the chunk headers
+            self.bag._chunk_headers = {}
+            for chunk_info in self.bag._chunks:
+                self.bag._file.seek(chunk_info.pos)
+                self.bag._chunk_headers[chunk_info.pos] = self.read_chunk_header()
+
+            if not self.bag._skip_index:
+                self._read_connection_index_records()
+
+        except ROSBagEncryptNotSupportedException:
+            raise
+        except ROSBagEncryptException:
+            raise
+        except Exception as ex:
+            raise ROSBagUnindexedException()
+
+    def _read_connection_index_records(self):
+        for chunk_info in self.bag._chunks:
+            self.bag._file.seek(chunk_info.pos)
+            _skip_record(self.bag._file)
+
+            self.bag._curr_chunk_info = chunk_info
+            for i in range(len(chunk_info.connection_counts)):
+                connection_id, index = self.read_connection_index_record()
+                self.bag._connection_indexes[connection_id].extend(index)
+
+        # Remove any connections with no entries
+        # This is a workaround for a bug where connection records were being written for
+        # connections which had no messages in the bag
+        orphan_connection_ids = [id for id, index in self.bag._connection_indexes.items() if not index]
+        for id in orphan_connection_ids:
+            del self.bag._connections[id]
+            del self.bag._connection_indexes[id]
+
+        self.bag._connection_indexes_read = True
+
+    def read_messages(self, topics, start_time, end_time, connection_filter, raw, return_connection_header=False):
+        connections = self.bag._get_connections(topics, connection_filter)
+        for entry in self.bag._get_entries(connections, start_time, end_time):
+            yield self.seek_and_read_message_data_record((entry.chunk_pos, entry.offset), raw, return_connection_header)
+
+    ###
+
+    def read_file_header_record(self):
+        self.bag._file_header_pos = self.bag._file.tell()
+
+        header = _read_header(self.bag._file, _OP_FILE_HEADER)
+
+        self.bag._index_data_pos   = _read_uint64_field(header, 'index_pos')
+        self.bag._chunk_count      = _read_uint32_field(header, 'chunk_count')
+        self.bag._connection_count = _read_uint32_field(header, 'conn_count')
+        try:
+            encryptor = _read_str_field(header, 'encryptor')
+            self.bag.set_encryptor(encryptor)
+            self.bag._encryptor.read_fields_from_file_header(header)
+        except ROSBagFormatException:
+            # If encryptor header is not found, keep going
+            pass
+
+        _skip_sized(self.bag._file)  # skip over the record data, i.e. padding
+
+    def read_connection_record(self, f, encrypt):
+        if encrypt:
+            header = self.bag._encryptor.read_encrypted_header(_read_header, f, _OP_CONNECTION)
+        else:
+            header = _read_header(f, _OP_CONNECTION)
+
+        conn_id = _read_uint32_field(header, 'conn')
+        topic   = _read_str_field   (header, 'topic')
+
+        if encrypt:
+            connection_header = self.bag._encryptor.read_encrypted_header(_read_header, f)
+        else:
+            connection_header = _read_header(f)
+
+        return _ConnectionInfo(conn_id, topic, connection_header)
+
+    def read_chunk_info_record(self):
+        f = self.bag._file
+        
+        header = _read_header(f, _OP_CHUNK_INFO)
+
+        chunk_info_version = _read_uint32_field(header, 'ver')
+        
+        if chunk_info_version == 1:
+            chunk_pos        = _read_uint64_field(header, 'chunk_pos')
+            start_time       = _read_time_field  (header, 'start_time')
+            end_time         = _read_time_field  (header, 'end_time')
+            connection_count = _read_uint32_field(header, 'count') 
+
+            chunk_info = _ChunkInfo(chunk_pos, start_time, end_time)
+
+            _read_uint32(f)   # skip the record data size
+
+            for i in range(connection_count):
+                connection_id = _read_uint32(f)
+                count         = _read_uint32(f)
+    
+                chunk_info.connection_counts[connection_id] = count
+                
+            return chunk_info
+        else:
+            raise ROSBagFormatException('Unknown chunk info record version: %d' % chunk_info_version)
+
+    def read_chunk_header(self):
+        header = _read_header(self.bag._file, _OP_CHUNK)
+
+        compression       = _read_str_field   (header, 'compression')
+        uncompressed_size = _read_uint32_field(header, 'size')
+
+        compressed_size = _read_uint32(self.bag._file)  # read the record data size
+        
+        data_pos = self.bag._file.tell()
+
+        return _ChunkHeader(compression, compressed_size, uncompressed_size, data_pos)
+
+    def read_connection_index_record(self):
+        f = self.bag._file
+
+        header = _read_header(f, _OP_INDEX_DATA)
+        
+        index_version = _read_uint32_field(header, 'ver')
+        connection_id = _read_uint32_field(header, 'conn')
+        count         = _read_uint32_field(header, 'count')
+        
+        if index_version != 1:
+            raise ROSBagFormatException('expecting index version 1, got %d' % index_version)
+    
+        record_size = _read_uint32(f) # skip the record data size
+
+        index = []
+                
+        for i in range(count):
+            time   = _read_time  (f)
+            offset = _read_uint32(f)
+            
+            index.append(_IndexEntry200(time, self.bag._curr_chunk_info.pos, offset))
+
+        return (connection_id, index)
+
+    def seek_and_read_message_data_record(self, position, raw, return_connection_header=False):
+        chunk_pos, offset = position
+
+        chunk_header = self.bag._chunk_headers.get(chunk_pos)
+        if chunk_header is None:
+            raise ROSBagException('no chunk at position %d' % chunk_pos)
+
+        if chunk_header.compression == Compression.NONE:
+            if self.decompressed_chunk_pos != chunk_pos:
+                f = self.bag._file
+                f.seek(chunk_header.data_pos)
+                encrypted_chunk = _read(f, chunk_header.compressed_size)
+
+                chunk = self.bag._encryptor.decrypt_chunk(encrypted_chunk)
+
+                self.decompressed_chunk_pos = chunk_pos
+
+                if self.decompressed_chunk_io:
+                    self.decompressed_chunk_io.close()
+                self.decompressed_chunk_io = StringIO(chunk)
+        else:
+            if self.decompressed_chunk_pos != chunk_pos:
+                # Seek to the chunk data, read and decompress
+                self.bag._file.seek(chunk_header.data_pos)
+                encrypted_chunk = _read(self.bag._file, chunk_header.compressed_size)
+
+                compressed_chunk = self.bag._encryptor.decrypt_chunk(encrypted_chunk)
+
+                if chunk_header.compression == Compression.BZ2:
+                    self.decompressed_chunk = bz2.decompress(compressed_chunk)
+                elif chunk_header.compression == Compression.LZ4 and found_lz4:
+                    self.decompressed_chunk = roslz4.decompress(compressed_chunk)
+                else:
+                    raise ROSBagException('unsupported compression type: %s' % chunk_header.compression)
+                
+                self.decompressed_chunk_pos = chunk_pos
+
+                if self.decompressed_chunk_io:
+                    self.decompressed_chunk_io.close()
+                self.decompressed_chunk_io = StringIO(self.decompressed_chunk)
+
+        f = self.decompressed_chunk_io
+        f.seek(offset)
+
+        # Skip any CONNECTION records
+        while True:
+            header = _read_header(f)
+            op = _read_uint8_field(header, 'op')
+            if op != _OP_CONNECTION:
+                break
+            _skip_sized(f)
+
+        # Check that we have a MSG_DATA record
+        if op != _OP_MSG_DATA:
+            raise ROSBagFormatException('Expecting OP_MSG_DATA, got %d' % op)
+
+        connection_id = _read_uint32_field(header, 'conn')
+        t             = _read_time_field  (header, 'time')
+
+        # Get the message type
+        connection_info = self.bag._connections[connection_id]
+        try:
+            msg_type = _get_message_type(connection_info)
+        except KeyError:
+            raise ROSBagException('Cannot deserialize messages of type [%s].  Message was not preceded in bag by definition' % connection_info.datatype)
+
+        # Read the message content
+        data = _read_record_data(f)
+        
+        # Deserialize the message
+        if raw:
+            msg = connection_info.datatype, data, connection_info.md5sum, (chunk_pos, offset), msg_type
+        else:
+            msg = msg_type()
+            msg.deserialize(data)
+
+        if return_connection_header:
+            return BagMessageWithConnectionHeader(connection_info.topic, msg, t, connection_info.header)
+        else:
+            return BagMessage(connection_info.topic, msg, t)
+
+def _time_to_str(secs):
+    secs_frac = secs - int(secs) 
+    secs_frac_str = ('%.2f' % secs_frac)[1:]
+
+    return time.strftime('%b %d %Y %H:%M:%S', time.localtime(secs)) + secs_frac_str
+
+def _human_readable_size(size):
+    multiple = 1024.0
+    for suffix in ['KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB']:
+        size /= multiple
+        if size < multiple:
+            return '%.1f %s' % (size, suffix)
+
+    return '-'
+
+def _human_readable_frequency(freq):
+    multiple = 1000.0
+    for suffix in ['Hz', 'kHz', 'MHz', 'GHz', 'THz', 'PHz', 'EHz', 'ZHz', 'YHz']:
+        if freq < multiple:
+            return '%.1f %s' % (freq, suffix)
+        freq /= multiple
+
+    return '-'
+
+class _CompressorFileFacade(object):
+    """
+    A file facade for sequential compressors (e.g., bz2.BZ2Compressor).
+    """
+    def __init__(self, file, compressor):
+        self.file                = file
+        self.compressor          = compressor
+        self.compressed_bytes_in = 0
+    
+    def write(self, data):
+        compressed = self.compressor.compress(data)
+        if len(compressed) > 0:
+            self.file.write(compressed)
+        self.compressed_bytes_in += len(data)
+    
+    def flush(self):
+        compressed = self.compressor.flush()
+        if len(compressed) > 0:
+            self.file.write(compressed)
+
+def _median(values):
+    values_len = len(values)
+    if values_len == 0:
+        return float('nan')
+
+    sorted_values = sorted(values)
+    if values_len % 2 == 1:
+        return sorted_values[int(values_len / 2)]
+
+    lower = sorted_values[int(values_len / 2) - 1]
+    upper = sorted_values[int(values_len / 2)]
+
+    return float(lower + upper) / 2
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/src/rosbag/migration.py
@@ -0,0 +1,1358 @@
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2009, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+from __future__ import print_function
+
+import collections
+import copy
+try:
+    from cStringIO import StringIO  # Python 2.x
+except ImportError:
+    from io import BytesIO as StringIO  # Python 3.x
+import inspect
+import itertools
+import os
+import string
+import sys
+import traceback
+
+import genmsg.msgs
+import genpy
+import genpy.dynamic
+
+import rospkg
+
+import rosbag
+
+# Anything outside the scope of these primitives is a submessage
+#_PRIMITIVES = ['bool', 'byte','int8','int16','int32','int64','char','uint8','uint16','uint32','uint64','float32','float64','string','time']
+
+class BagMigrationException(Exception):
+    pass
+
+def checkbag(migrator, inbag):
+    """
+    Check whether a bag file can be played in the current system.
+    @param migrator: message migrator to use
+    @param inbag name of the bag to be checked.
+    @returns A list of tuples for each type in the bag file.  The first
+    element of each tuple is the full migration path for the type.  The
+    second element of the tuple is the expanded list of invalid rules
+    for that particular path.
+    """
+    checked = set()
+    migrations = []
+
+    bag = rosbag.Bag(inbag, 'r')
+
+    for topic, msg, t in bag.read_messages(raw=True):
+        key = get_message_key(msg[4])
+        if key not in checked:
+            target = migrator.find_target(msg[4])
+            # Even in the case of a zero-length path (matching md5sums), we still want
+            # to migrate in the event of a type change (message move).
+            path = migrator.find_path(msg[4], target)
+            if len(path) > 0:
+                migrations.append((path, [r for r in migrator.expand_rules([sn.rule for sn in path]) if r.valid == False]))
+
+            checked.add(key)
+
+    bag.close()
+            
+    return migrations
+
+def checkmessages(migrator, messages):
+    """
+    Check whether a bag file can be played in the current system.
+    @param migrator The message migrator to use
+    @param message_list A list of message classes.
+    @returns A list of tuples for each type in the bag file.  The first
+    element of each tuple is the full migration path for the type.  The
+    second element of the tuple is the expanded list of invalid rules
+    for that particular path.
+    """
+    
+    checked = set()
+    migrations = []
+
+    for msg in messages:
+        key = get_message_key(msg)
+        if key not in checked:
+            target = migrator.find_target(msg)
+            # Even in the case of a zero-length path (matching md5sums), we still want
+            # to migrate in the event of a type change (message move).
+            path = migrator.find_path(msg, target)
+            if len(path) > 0:
+                migrations.append((path, [r for r in migrator.expand_rules([sn.rule for sn in path]) if r.valid == False]))
+
+            checked.add(key)
+            
+    return migrations
+
+def _migrate_connection_header(conn_header, new_msg_type):
+    conn_header['type'] = new_msg_type._type
+    conn_header['md5sum'] = new_msg_type._md5sum
+    conn_header['message_definition'] = new_msg_type._full_text
+
+    return conn_header
+
+## Fix a bag so that it can be played in the current system
+#
+# @param migrator The message migrator to use
+# @param inbag Name of the bag to be fixed.
+# @param outbag Name of the bag to be saved.
+# @returns True if migration was successful.
+def fixbag(migrator, inbag, outbag):
+    # This checks/builds up rules for the given migrator
+    res = checkbag(migrator, inbag)
+
+    # Deserializing all messages is inefficient, but we can speed this up later
+    if not False in [m[1] == [] for m in res]:
+        bag = rosbag.Bag(inbag, 'r')
+        rebag = rosbag.Bag(outbag, 'w', options=bag.options)
+        for topic, msg, t, conn_header in bag.read_messages(raw=True, return_connection_header=True):
+            new_msg_type = migrator.find_target(msg[4])
+            mig_msg = migrator.migrate_raw(msg, (new_msg_type._type, None, new_msg_type._md5sum, None, new_msg_type))
+            new_conn_header = _migrate_connection_header(conn_header, new_msg_type)
+            rebag.write(topic, mig_msg, t, connection_header=new_conn_header, raw=True)
+        rebag.close()
+        bag.close()
+        return True
+    else:
+        return False
+
+## Fix a bag so that it can be played in the current system
+#
+# @param migrator The message migrator to use
+# @param inbag Name of the bag to be fixed.
+# @param outbag Name of the bag to be saved.
+# @returns [] if bag could be migrated, otherwise, it returns the list of necessary migration paths
+def fixbag2(migrator, inbag, outbag, force=False):
+    # This checks/builds up rules for the given migrator
+    res = checkbag(migrator, inbag)
+
+    migrations = [m for m in res if len(m[1]) > 0]
+
+    # Deserializing all messages is inefficient, but we can speed this up later
+    if len(migrations) == 0 or force:
+        bag = rosbag.Bag(inbag, 'r')
+        rebag = rosbag.Bag(outbag, 'w', options=bag.options)
+        for topic, msg, t, conn_header in bag.read_messages(raw=True, return_connection_header=True):
+            new_msg_type = migrator.find_target(msg[4])
+            if new_msg_type != None:
+                mig_msg = migrator.migrate_raw(msg, (new_msg_type._type, None, new_msg_type._md5sum, None, new_msg_type))
+                new_conn_header = _migrate_connection_header(conn_header, new_msg_type)
+                rebag.write(topic, mig_msg, t, connection_header=new_conn_header, raw=True)
+            else:
+                rebag.write(topic, msg, t, connection_header=conn_header, raw=True)
+        rebag.close()
+        bag.close()
+
+    if force:
+        return []
+    else:
+        return migrations
+
+## Helper function to strip out roslib and package name from name usages.
+# 
+# There is some inconsistency in whether a fully-qualified path is
+# used for sub-messages within a given message.  This function is
+# useful for stripping out the package name in a fully qualified
+# sub-message.
+#
+# @param name      The name to clean.
+# @param top_name  The name of the top-level type
+# @returns         The cleaned version of the name.
+def clean_name(name, top_name):
+    name_split = name.split('/')
+    try:
+        name_split.remove('std_msgs')
+    except ValueError:
+        pass
+    try:
+        name_split.remove(top_name.split('/')[0])
+    except ValueError:
+        pass
+    new_name = '/'.join(name_split)
+    return new_name
+
+## Helper function to ensure we end up with a qualified name
+# 
+# There is some inconsistency in whether a fully-qualified path is
+# used for sub-messages within a given message.  This function is
+# useful for ensuring that a name is fully qualified correctly.
+#
+# @param name      The name to qualify
+# @param top_name  The name of the top-level type
+# @returns         The qualified version of the name.
+def qualified_name(name, top_name):
+    # First clean the name, to make everything else more deterministic
+    tmp_name = clean_name(name, top_name)
+
+    if len(tmp_name.split('/')) == 2 or (genmsg.msgs.is_builtin(tmp_name)):
+        return tmp_name
+    elif tmp_name == 'Header':
+        return 'std_msgs/Header'
+    else:
+        return top_name.split('/')[0] + '/' + tmp_name
+
+## Helper function to return a key from a given class
+#
+# For now, we choose the tuple (type,md5sum) as a unique key for the
+# class.  However, this is subject to change and assumptions about keys
+# should not be made other than their uniqueness.
+#
+# @param c  The message class or instance to get a key for
+# @returns The unique key
+def get_message_key(c):
+    try:
+        return (c._type, c._md5sum)
+    except:
+        return None
+
+## Helper function to return a key for a given path
+#
+# For now, we choose the tuple ((type1,md5sum1),(type2,md5sum2)) as a
+# unique key for the path.  However, this is subject to change and
+# assumptions about keys should not be made other than their
+# uniqueness.
+#
+# @param c1  The start point of the path
+# @param c1  The stop point of the path
+# @returns The unique key
+def get_path_key(c1, c2):
+    try:
+        return (get_message_key(c1), get_message_key(c2))
+    except:
+        return None
+
+## Base class for all message update rules
+class MessageUpdateRule(object):
+    old_type = ''
+    old_full_text = ''
+    new_type = ''
+    new_full_text = ''
+    migrated_types = []
+
+    order = -1
+
+    valid = False
+
+    class EmptyType(Exception):
+        pass
+
+    ## Initialize class
+    def __init__(self, migrator, location):
+        # Every rule needs to hang onto the migrator so we can potentially use it
+        self.migrator = migrator
+        self.location = location
+
+        if (self.old_type != self.new_type):
+            self.rename_rule = True
+        else:
+            self.rename_rule = False
+
+        # Instantiate types dynamically based on definition
+        try:
+            if self.old_type == "":
+                raise self.EmptyType
+            self.old_types = genpy.dynamic.generate_dynamic(self.old_type, self.old_full_text)
+            self.old_class = self.old_types[self.old_type]
+            self.old_md5sum = self.old_class._md5sum
+        except Exception as e:
+            if not isinstance(e, self.EmptyType):
+                traceback.print_exc(file=sys.stderr)
+            self.old_types = {}
+            self.old_class = None
+            self.old_md5sum = ""
+        try:
+            if self.new_type == "":
+                raise self.EmptyType
+            self.new_types = genpy.dynamic.generate_dynamic(self.new_type, self.new_full_text)
+            self.new_class = self.new_types[self.new_type]
+            self.new_md5sum = self.new_class._md5sum
+        except Exception as e:
+            if not isinstance(e, self.EmptyType):
+                traceback.print_exc(file=sys.stderr)
+            self.new_types = {}
+            self.new_class = None
+            self.new_md5sum = ""
+
+        # We have not populated our sub rules (and ideally should
+        # wait until the full scaffold exists before doing this)
+        self.sub_rules_done = False
+        self.sub_rules_valid = False
+        self.sub_rules = []
+
+    ## Find all of the sub paths
+    # 
+    # For any migrated type the user might want to use, we must make
+    # sure the migrator had found a path for it.  To facilitated this
+    # check we require that all migrated types must be listed as pairs
+    # in the migrated_types field.
+    #
+    # It would be nice not to need these through performing some kind
+    # of other inspection of the update rule itself.
+    def find_sub_paths(self):
+        self.sub_rules_valid = True
+        for (t1, t2) in self.migrated_types:
+            try:
+                tmp_old_class = self.get_old_class(t1)
+            except KeyError:
+                print("WARNING: Within rule [%s], specified migrated type [%s] not found in old message types" % (self.location, t1), file=sys.stderr)
+                self.sub_rules_valid = False
+                continue
+            try:
+                tmp_new_class = self.get_new_class(t2)
+            except KeyError:
+                print("WARNING: Within rule [%s], specified migrated type [%s] not found in new message types" % (self.location, t2), file=sys.stderr)
+                self.sub_rules_valid = False
+                continue
+
+            # If a rule instantiates itself as a subrule (because the
+            # author knows the md5sums match), we don't Want to end up
+            # with an infinite recursion.
+            if (get_message_key(tmp_old_class) != get_message_key(self.old_class)) or (get_message_key(tmp_new_class) != get_message_key(self.new_class)):
+                path = self.migrator.find_path(tmp_old_class, tmp_new_class)
+                rules = [sn.rule for sn in path]
+                self.sub_rules.extend(rules)
+
+            if False in [r.valid for r in self.sub_rules]:
+                print("WARNING: Within rule [%s] cannot migrate from subtype [%s] to [%s].." % (self.location, t1, t2), file=sys.stderr)
+                self.sub_rules_valid = False
+                continue
+        self.sub_rules = self.migrator.filter_rules_unique(self.sub_rules)
+        self.sub_rules_done = True
+
+    ## Helper function to get the class of a submsg for the new type
+    #
+    # This function should be used inside of update to access new classes.
+    #
+    # @param t The subtype to return the class of
+    # @returns The class of the new sub type
+    def get_new_class(self,t):
+        try:
+            try:
+                return self.new_types[t]
+            except KeyError:                
+                return self.new_types['std_msgs/' + t]
+        except KeyError:
+            return self.new_types[self.new_type.split('/')[0] + '/' + t]
+
+    ## Helper function to get the class of a submsg for the old type
+    #
+    # This function should be used inside of update to access old classes.
+    #
+    # @param t The subtype to return the class of
+    # @returns The class of the old sub type
+    def get_old_class(self,t):
+        try:
+            try:
+                return self.old_types[t]
+            except KeyError:                
+                return self.old_types['std_msgs/' + t]
+        except KeyError:
+            return self.old_types[self.old_type.split('/')[0] + '/' + t]
+
+    ## Actually migrate one sub_type to another
+    #
+    # This function should be used inside of update to migrate sub msgs.
+    #
+    # @param msg_from A message instance of the old message type
+    # @param msg_to   A message instance of a new message type to be populated
+    def migrate(self, msg_from, msg_to):
+        tmp_msg_from = clean_name(msg_from._type, self.old_type)
+        tmp_msg_to = clean_name(msg_to._type, self.new_type)
+        if (tmp_msg_from, tmp_msg_to) not in self.migrated_types:
+            raise BagMigrationException("Rule [%s] tried to perform a migration from old [%s] to new [%s] not listed in migrated_types"%(self.location, tmp_msg_from, tmp_msg_to))
+        self.migrator.migrate(msg_from, msg_to)
+
+    ## Helper function to migrate a whole array of messages
+    #
+    # This function should be used inside of update to migrate arrays of sub msgs.
+    #
+    # @param msg_from_array An array of messages of the old message type
+    # @param msg_to_array An array of messages of the new message type (this will be emptied if not already)
+    # @param msg_to_class  The name of the new message type since msg_to_array may be an empty array.
+    def migrate_array(self, msg_from_array, msg_to_array, msg_to_name):
+        msg_to_class = self.get_new_class(msg_to_name)
+
+        while len(msg_to_array) > 0:
+            msg_to_array.pop()
+
+        if (len(msg_from_array) == 0):
+            return
+
+        tmp_msg_from = clean_name(msg_from_array[0]._type, self.old_type)
+        tmp_msg_to = clean_name(msg_to_class._type, self.new_type)
+        if (tmp_msg_from, tmp_msg_to) not in self.migrated_types:
+            raise BagMigrationException("Rule [%s] tried to perform a migration from old [%s] to new [%s] not listed in migrated_types"%(self.location, tmp_msg_from, tmp_msg_to))
+
+        msg_to_array.extend( [msg_to_class() for i in range(len(msg_from_array))] )
+
+        self.migrator.migrate_array(msg_from_array, msg_to_array)
+
+    ## A helper function to print out the definition of autogenerated messages.
+    def get_class_def(self):
+        pass
+
+    ## The function actually called by the message migrator 
+    #
+    # @param old_msg An instance of the old message type.
+    # @returns An instance of a new message type
+    def apply(self, old_msg):
+        if not self.valid:
+            raise BagMigrationException("Attempted to apply an invalid rule")
+        if not self.sub_rules_done:
+            raise BagMigrationException("Attempted to apply a rule without building up its sub rules")
+        if not self.sub_rules_valid:
+            raise BagMigrationException("Attempted to apply a rule without valid sub-rules")
+        if (get_message_key(old_msg) != get_message_key(self.old_class)):
+            raise BagMigrationException("Attempted to apply rule to incorrect class %s %s."%(get_message_key(old_msg),get_message_key(self.old_class)))
+
+        # Apply update rule
+        new_msg = self.new_class()
+        self.update(old_msg, new_msg)
+
+        return new_msg
+    
+    ## The function which a user overrides to actually perform the message update
+    #
+    # @param msg_from A message instance of the old message type
+    # @param msg_to   A message instance of a new message type to be populated
+    def update(self, old_msg, new_msg):
+        raise BagMigrationException("Tried to use rule without update overidden")
+
+
+## A class for book-keeping about rule-chains.
+#
+# Rule chains define the ordered set of update rules, indexed by
+# typename, terminated by a rename rule.  This class is only used
+# temporarily to help us get the ordering right, until all explicit
+# rules have been loaded (possibly out of order) and the proper
+# scaffold can be built.
+class RuleChain(object):
+    def __init__(self):
+        self.chain = []
+        self.order_keys = set()
+        self.rename = None
+ 
+
+## A class for arranging the ordered rules
+#
+# They provide a scaffolding (essentially a linked list) over which we
+# assume we can migrate messages forward.  This allows us to verify a
+# path exists before actually creating all of the necessary implicit
+# rules (mostly migration of sub-messages) that such a path
+# necessitates.
+class ScaffoldNode(object):
+    def __init__(self, old_class, new_class, rule):
+        self.old_class = old_class
+        self.new_class = new_class
+        self.rule = rule
+        self.next = None
+
+## A class to actually migrate messages
+#
+# This is the big class that actually handles all of the fancy
+# migration work.  Better documentation to come later.
+class MessageMigrator(object):
+    def __init__(self, input_rule_files=[], plugins=True):
+        # We use the rulechains to scaffold our initial creation of
+        # implicit rules.  Each RuleChain is keyed off of a type and
+        # consists of an ordered set of update rules followed by an
+        # optional rename rule.  For the system rule definitions to be
+        # valid, all members of a rulechains must be connectable via
+        # implicit rules and all rulechains must terminate in a known
+        # system type which is also reachable by an implicit rule.
+        self.rulechains = collections.defaultdict(RuleChain)
+        
+        # The list of all nodes that we can iterate through in the
+        # future when making sure all rules have been constructed.
+        self.base_nodes = []
+
+        # The list of extra (non-scaffolded) nodes that we can use
+        # when determining if all rules are valid and printing invalid
+        # rules.
+        self.extra_nodes = []
+
+        # A map from typename to the first node of a particular type
+        self.first_type = {}
+                
+        # A map from a typename to all other typenames for which
+        # rename rules exist.  This is necessary to determine whether
+        # an appropriate implicit rule can actually be constructed.
+        self.rename_map = {}
+
+        # The cached set of all found paths, keyed by:
+        # ((old_type, old_md5), (new_type, new_md5))
+        self.found_paths = {}
+        self.found_targets = {}
+
+        # Temporary list of the terminal nodes
+        terminal_nodes = []
+
+        # Temporary list of rule modules we are loading
+        rule_dicts = []
+
+        self.false_rule_loaded = False
+        
+        # To make debugging easy we can pass in a list of local
+        # rulefiles.
+        for r in input_rule_files:
+            try:
+                scratch_locals = {'MessageUpdateRule':MessageUpdateRule}
+                with open(r, 'r') as f:
+                    exec(f.read(), scratch_locals)
+                rule_dicts.append((scratch_locals, r))
+            except:
+                print("Cannot load rule file [%s] in local package" % r, file=sys.stderr)
+
+        # Alternatively the preferred method is to load definitions
+        # from the migration ruleset export flag.
+        if plugins:
+            rospack = rospkg.RosPack()
+            for dep,export in [('rosbagmigration','rule_file'),('rosbag','migration_rule_file'),('rosbag_migration_rule','rule_file')]:
+                for pkg in rospack.get_depends_on(dep, implicit=False):
+                    m = rospack.get_manifest(pkg)
+                    p_rules = m.get_export(dep,export)
+                    pkg_dir = rospack.get_path(pkg)
+                    for r in p_rules:
+                        if dep == 'rosbagmigration':
+                            print("""WARNING: The package: [%s] is using a deprecated rosbagmigration export.
+    The export in the manifest should be changed to:
+    <rosbag migration_rule_file="%s"/>
+""" % (pkg, r), file=sys.stderr)
+                        try:
+                            scratch_locals = {'MessageUpdateRule':MessageUpdateRule}
+                            exec(open(pkg_dir + "/" + r).read(), scratch_locals)
+                            rule_dicts.append((scratch_locals, r))
+                        except ImportError:
+                            print("Cannot load rule file [%s] in package [%s]" % (r, pkg), file=sys.stderr)
+
+
+        for (rule_dict, location_base) in rule_dicts:
+            for (n,c) in rule_dict.items():
+                if inspect.isclass(c):
+                    if (not c == MessageUpdateRule) and issubclass(c, MessageUpdateRule):
+                        self.add_update_rule(c(self, location_base + ':' + n))
+                
+        if self.false_rule_loaded:
+            raise BagMigrationException("Cannot instantiate MessageMigrator with invalid rules")
+
+        # Now, go through and build up a better scaffolded
+        # representation, deferring implicit rule generation until
+        # complete, since the implicit rule generation and sub-rule
+        # population makes use of the scaffold.
+
+        # First we each particular type chain (now including implicit
+        # rules).  Additionally, we build up our name remapping lists.
+
+
+        # For Each rulechain
+        for (type,rulechain) in self.rulechains.items():
+            first = True
+            sn = None
+            prev_sn = None
+
+            # Find name remapping list
+            rename_set = set([type])
+            tmp = rulechain.rename
+            while tmp:
+                rename_set.add(tmp.new_type)
+                if tmp.new_type in self.rulechains:
+                    tmp = self.rulechains[tmp.new_type].rename
+                else:
+                    break
+                    
+            self.rename_map[type] = rename_set
+
+            # For each element in the rulechain chain, 
+            for r in rulechain.chain:
+                # Create a scaffoldnode
+                sn = ScaffoldNode(r.old_class, r.new_class, r)
+                self.base_nodes.append(sn)
+                # If it's the first one, stick it in our first_type map
+                if first:
+                    self.first_type[type] = sn
+                    first = False
+                # If there was a previous node, link them if keys
+                # match, or else create an implicit SN
+                if prev_sn:
+                    if get_message_key(prev_sn.new_class) == get_message_key(sn.old_class):
+                        prev_sn.next = sn
+                    else:
+                        implicit_sn = ScaffoldNode(prev_sn.new_class, sn.old_class, None)
+                        self.base_nodes.append(implicit_sn)
+                        prev_sn.next = implicit_sn
+                        implicit_sn.next = sn
+                # The just-created node now becomes the previous
+                prev_sn = sn
+
+            # If there is a rename rule
+            if rulechain.rename:
+                # Create a scaffoldnode
+                sn = ScaffoldNode(rulechain.rename.old_class, rulechain.rename.new_class, rulechain.rename)
+                self.base_nodes.append(sn)
+
+                # Same rules apply here as when we created each node
+                # from chain.  Link if possible, otherwise create
+                # implicit
+                if first:
+                    self.first_type[type] = sn
+                    first = False
+                if prev_sn:
+                    if get_message_key(prev_sn.new_class) == get_message_key(sn.old_class):
+                        prev_sn.next = sn
+                    else:
+                        implicit_sn = ScaffoldNode(prev_sn.new_class, sn.old_class, None)
+                        self.base_nodes.append(implicit_sn)
+                        prev_sn.next = implicit_sn
+                        implicit_sn.next = sn                        
+                prev_sn = sn
+                terminal_nodes.append(sn)
+            # If there was not a rename rule, this must be a terminal node
+            else:
+                if prev_sn:
+                    terminal_nodes.append(prev_sn)
+        
+        # Between our partial scaffold and name remapping list, we can
+        # now GENERATE rules, though we cannot yet populate the
+        # subrules.
+
+        for sn in terminal_nodes:
+            key = get_message_key(sn.new_class)
+
+            renamed = (sn.old_class._type != sn.new_class._type)
+
+            sys_class = genpy.message.get_message_class(sn.new_class._type)
+
+            # If we map directly to a system-defined class we're done
+            if sys_class:
+                new_rule = self.make_update_rule(sn.new_class, sys_class)
+                R = new_rule(self, 'GENERATED.' + new_rule.__name__)
+                if R.valid:
+                    sn.next = ScaffoldNode(sn.new_class, sys_class, R)
+                    self.base_nodes.append(sn.next)
+
+            if renamed:
+                tmp_sns = self.scaffold_range(sn.new_class._type, sn.new_class._type)
+
+                # If we don't map to a scaffold range, we appear to be done
+                if tmp_sns == []:
+                    if sys_class is not None:
+                        sn.next = ScaffoldNode(sn.new_class, sys_class, None)
+                        self.base_nodes.append(sn.next)
+                        continue
+
+                # Otherwise look for trivial bridges
+                for tmp_sn in reversed(tmp_sns):
+                    tmp_key = get_message_key(tmp_sn.old_class)
+                    if (key == tmp_key):
+                        sn.next = tmp_sn
+                        break
+
+                # If we did not find a trivial bridge, we instead need
+                # to create the right implicit rule ourselves.  This
+                # is based on the ability to create a valid implicit
+                # rule as LATE in the chain as possible.  We do this
+                # to avoid extra conversions in some boundary
+                # circumstances.
+                if (sn.next is None):
+                    for tmp_sn in reversed(tmp_sns):
+                        new_rule = self.make_update_rule(sn.new_class, tmp_sn.old_class)
+                        R = new_rule(self, 'GENERATED.' + new_rule.__name__)
+                        if R.valid:
+                            sn.next = ScaffoldNode(sn.new_class, tmp_sn.old_class, R)
+                            self.base_nodes.append(sn.next)
+                            break
+
+            
+            # If we have still failed we need to create a placeholder.  
+            if (sn.next is None):
+                if sys_class:
+                    new_rule = self.make_update_rule(sn.new_class, sys_class)
+                else:
+                    new_rule = self.make_old_half_rule(sn.new_class)
+                R = new_rule(self, 'GENERATED.' + new_rule.__name__)
+                sn.next = ScaffoldNode(sn.new_class, None, R)
+                self.base_nodes.append(sn.next)
+                    
+
+        # Now that our scaffolding is actually complete, we iterate
+        # through all of our rules and generate the rules for which we
+        # have scaffoldnodes, but no rule yet
+        for sn in self.base_nodes:
+            if (sn.rule is None):
+                new_rule = self.make_update_rule(sn.old_class, sn.new_class)
+                sn.rule = new_rule(self, 'GENERATED.' + new_rule.__name__)
+
+        # Finally, we go through and try to find sub_paths for every
+        # rule in the system so far
+        for sn in self.base_nodes:
+            sn.rule.find_sub_paths()
+
+        # Construction should be done, we can now use the system in
+        # the event that we don't have invalid update rules.
+
+        self.class_dict = {}
+
+        for sn in self.base_nodes + self.extra_nodes:
+            self.class_dict[get_message_key(sn.old_class)] = sn.old_class
+            self.class_dict[get_message_key(sn.new_class)] = sn.new_class
+
+
+    def lookup_type(self, key):
+        if key in self.class_dict:
+            return self.class_dict[key]
+        else:
+            return None
+
+    # Add an update rule to our set of rule chains
+    def add_update_rule(self, r):
+        if r.valid == False:
+            print("ERROR: Update rule [%s] has valid set to False." % (r.location), file=sys.stderr)
+            self.false_rule_loaded = True
+            return
+
+        rulechain = self.rulechains[r.old_type]
+
+        if r.rename_rule:
+            if (rulechain.rename != None):
+                print("WARNING: Update rules [%s] and [%s] both attempting to rename type [%s]. Ignoring [%s]" % (rulechain.rename.location, r.location, r.old_type, r.location), file=sys.stderr)
+                return
+
+            # Search forward to make sure we havn't created a cycle
+            cycle = []
+            tmp = r
+            while tmp:
+                cycle.append(tmp)
+                if (tmp.new_type == r.old_type):
+                    print("WARNING: Update rules %s introduce a renaming cycle. Ignoring [%s]" % ([x.location for x in cycle], r.location), file=sys.stderr)
+                    return
+                if tmp.new_type in self.rulechains:
+                    tmp = self.rulechains[tmp.new_type].rename
+                else:
+                    break
+
+
+            if rulechain.chain and (r.order <= rulechain.chain[-1].order):
+                print("WARNING: Update rule [%s] which performs rename does not have largest order number. Ignoring" % r.location, file=sys.stderr)
+                return
+
+            rulechain.rename = r
+
+        else:
+            if r.order in rulechain.order_keys:
+                otherind = [x.order for x in rulechain.chain].index(r.order)
+                print("WARNING: Update rules [%s] and [%s] for type [%s] have the same order number. Ignoring [%s]" % (rulechain.chain[otherind].location, r.location, r.old_type, r.location), file=sys.stderr)
+                return
+            else:
+                if rulechain.rename and (r.order >= rulechain.rename.order):
+                    print("WARNING: Update rule [%s] has order number larger than rename rule [%s]. Ignoring" % (r.location, rulechain.rename.location), file=sys.stderr)
+                    return
+                # Insert the rule into a rule chain
+                rulechain.order_keys.add(r.order)
+                rulechain.chain.append(r)
+                rulechain.chain.sort(key=lambda x: x.order)
+                
+    # Helper function to determine if all rules are valid
+    def all_rules_valid(self):
+        base_valid  = not False in [sn.rule.valid for sn in self.base_nodes]
+        extra_valid = not False in [sn.rule.valid for sn in self.extra_nodes]
+        return base_valid and extra_valid
+
+    # Helper function to print out the definitions for all invalid rules (which include definitions)
+    def get_invalid_rules(self):
+        invalid_rules = []
+        invalid_rule_cache = []
+        for sn in self.base_nodes:
+            if not sn.rule.valid:
+                path_key = get_path_key(sn.old_class, sn.new_class)
+                if (path_key not in invalid_rule_cache):
+                    invalid_rules.append(sn.rule)
+                    invalid_rule_cache.append(path_key)
+        for sn in self.extra_nodes:
+            if not sn.rule.valid:
+                path_key = get_path_key(sn.old_class, sn.new_class)
+                if (path_key not in invalid_rule_cache):
+                    invalid_rules.append(sn.rule)
+                    invalid_rule_cache.append(path_key)
+        return invalid_rules
+
+    # Helper function to remove non-unique rules
+    def filter_rules_unique(self, rules):
+        rule_cache = []
+        new_rules = []
+        for r in rules:
+            path_key = get_path_key(r.old_class, r.new_class)
+            if (path_key not in rule_cache):
+                new_rules.append(r)
+        return new_rules
+
+    # Helper function to expand a list of rules to include subrules
+    def expand_rules(self, rules):
+        filtered = self.filter_rules_unique(rules)
+        expanded = []
+        for r in filtered:
+            expanded.append(r)
+            #print "For rule %s --> %s"%(r.old_class._type, r.new_class._type)
+            expanded.extend(self.expand_rules(r.sub_rules))
+        filtered = self.filter_rules_unique(expanded)
+        return filtered
+
+    def scaffold_range(self, old_type, new_type):
+        try:
+            first_sn = self.first_type[old_type]
+            
+            sn_range = [first_sn]
+
+            found_new_type = False
+
+            tmp_sn = first_sn
+
+            while (tmp_sn.next is not None and tmp_sn.next.new_class is not None):
+#                print sn_range
+                tmp_sn = tmp_sn.next
+                if (tmp_sn != first_sn):
+                    sn_range.append(tmp_sn)
+                if (tmp_sn.new_class._type == new_type):
+                    found_new_type = True
+                if (found_new_type and tmp_sn.new_class._type != new_type):
+                    break
+
+            return sn_range
+
+        except KeyError:
+            return []
+
+
+    def find_target(self, old_class):
+        key = get_message_key(old_class)
+
+        last_class = old_class
+
+        try:
+            return self.found_targets[key]
+        except KeyError:
+
+            sys_class = genpy.message.get_message_class(old_class._type)
+
+            if sys_class is not None:
+                self.found_targets[key] = sys_class
+                return sys_class
+
+            try:
+                tmp_sn = self.first_type[old_class._type]
+
+                if tmp_sn.new_class is not None:
+                    last_class = tmp_sn.new_class
+
+                while tmp_sn.next is not None:
+                    tmp_sn = tmp_sn.next
+
+                if tmp_sn.new_class is not None:
+                    last_class = tmp_sn.new_class
+                    sys_class = genpy.message.get_message_class(tmp_sn.new_class._type)
+                else:
+                    sys_class = None
+
+                if sys_class is not None:
+                    self.found_targets[key] = sys_class
+                    return sys_class
+            except KeyError:
+                pass
+
+        self.found_targets[key] = None
+        return None
+            
+    # This function determines the set of rules which must be created
+    # to get from the old type to the new type.
+    def find_path(self, old_class, new_class):
+        key = get_path_key(old_class, new_class)
+
+        # Return any path already found in the cache
+        try:
+            return self.found_paths[key]
+        except KeyError:
+            pass
+
+        # If the new_class is none, e.g., a message has been moved and
+        # we are lacking a proper rename rule, such that find-target
+        # failed, the best we can do is create a half-rule from the
+        # end-point
+        if new_class is None:
+            sn_range = self.scaffold_range(old_class._type, "")
+
+            found_start = False
+
+            for (ind, tmp_sn) in reversed(list(zip(range(len(sn_range)), sn_range))):
+                # Skip until we find the class we're trying to match
+                if (tmp_sn.old_class._type != old_class._type):
+                    continue
+                if get_message_key(tmp_sn.old_class) == get_message_key(old_class):
+                    sn_range = sn_range[ind:]
+                    found_start = True
+                    break
+
+            # Next see if we can create a valid rule
+            if not found_start:
+                for (ind, tmp_sn) in reversed(list(zip(range(len(sn_range)), sn_range))):
+                    if (tmp_sn.old_class._type != old_class._type):
+                        continue
+                    new_rule = self.make_update_rule(old_class, tmp_sn.old_class)
+                    R = new_rule(self, 'GENERATED.' + new_rule.__name__)
+                    if R.valid:
+                        R.find_sub_paths()
+                        sn = ScaffoldNode(old_class, tmp_sn.old_class, R)
+                        self.extra_nodes.append(sn)
+                        sn_range = sn_range[ind:]
+                        sn_range.insert(0,sn)
+                        found_start = True
+                        break
+
+            if sn_range == []:
+                tmp_class = old_class
+            else:
+                tmp_class = sn_range[-1].new_class
+
+            new_rule = self.make_old_half_rule(tmp_class)
+            R = new_rule(self, 'GENERATED.' + new_rule.__name__)
+            sn = ScaffoldNode(tmp_class, None, R)
+            sn_range.append(sn)
+            self.extra_nodes.append(sn)
+            self.found_paths[key] = sn_range
+            return sn_range
+
+        # If the messages are the same, there is no actually path
+        if (old_class._type == new_class._type and old_class._full_text.strip() == new_class._full_text.strip()):
+            self.found_paths[key] = []
+            return []
+
+        sn_range = self.scaffold_range(old_class._type, new_class._type)
+
+        # If we have no scaffolding, we just try to create the one path
+        if sn_range == []:
+            new_rule = self.make_update_rule(old_class, new_class)
+            R = new_rule(self, 'GENERATED.' + new_rule.__name__)
+            R.find_sub_paths()
+            sn = ScaffoldNode(old_class, new_class, R)
+            self.extra_nodes.append(sn)
+            self.found_paths[key] = [sn]
+            return [sn]
+
+
+        # Search for the stop point in the scaffold
+        found_stop = False
+
+        # First look for a trivial match
+        for (ind, tmp_sn) in reversed(list(zip(range(len(sn_range)), sn_range))):
+            # Stop looking early if the classes don't match
+            if (tmp_sn.new_class._type != new_class._type):
+                break
+            if get_message_key(tmp_sn.new_class) == get_message_key(new_class):
+                sn_range = sn_range[:ind+1]
+                found_stop = True
+                break
+
+        # Next see if we can create a valid rule, including the sub rules
+        if not found_stop:
+            for (ind, tmp_sn) in reversed(list(zip(range(len(sn_range)), sn_range))):
+                if (tmp_sn.new_class._type != new_class._type):
+                    break
+                new_rule = self.make_update_rule(tmp_sn.new_class, new_class)
+                R = new_rule(self, 'GENERATED.' + new_rule.__name__)
+                if R.valid:
+                    R.find_sub_paths()
+                    if R.sub_rules_valid:
+                        sn = ScaffoldNode(tmp_sn.new_class, new_class, R)
+                        self.extra_nodes.append(sn)
+                        sn_range = sn_range[:ind+1]
+                        sn_range.append(sn)
+                        found_stop = True
+                        break
+
+        # If there were no valid implicit rules, we suggest a new one from to the end
+        if not found_stop:
+            new_rule = self.make_update_rule(sn_range[-1].new_class, new_class)
+            R = new_rule(self, 'GENERATED.' + new_rule.__name__)
+            R.find_sub_paths()
+            sn = ScaffoldNode(sn_range[-1].new_class, new_class, R)
+            self.extra_nodes.append(sn)
+            sn_range.append(sn)
+
+        # Search for the start point in the scaffold
+        found_start = False
+
+        # First look for a trivial match
+        for (ind, tmp_sn) in reversed(list(zip(range(len(sn_range)), sn_range))):
+            # Skip until we find the class we're trying to match
+            if (tmp_sn.old_class._type != old_class._type):
+                continue
+            if get_message_key(tmp_sn.old_class) == get_message_key(old_class):
+                sn_range = sn_range[ind:]
+                found_start = True
+                break
+
+        # Next see if we can create a valid rule directly to the end, including the sub rules
+        if not found_start:
+            new_rule = self.make_update_rule(old_class, new_class)
+            R = new_rule(self, 'GENERATED.' + new_rule.__name__)
+            if R.valid:
+                R.find_sub_paths()
+                if R.sub_rules_valid:
+                    sn = ScaffoldNode(old_class, new_class, R)
+                    self.extra_nodes.append(sn)
+                    self.found_paths[key] = [sn]
+                    return [sn]
+
+        # Next see if we can create a valid rule, including the sub rules
+        if not found_start:
+            for (ind, tmp_sn) in reversed(list(zip(range(len(sn_range)), sn_range))):
+                if (tmp_sn.old_class._type != old_class._type):
+                    continue
+                new_rule = self.make_update_rule(old_class, tmp_sn.old_class)
+                R = new_rule(self, 'GENERATED.' + new_rule.__name__)
+                if R.valid:
+                    R.find_sub_paths()
+                    if R.sub_rules_valid:
+                        sn = ScaffoldNode(old_class, tmp_sn.old_class, R)
+                        self.extra_nodes.append(sn)
+                        sn_range = sn_range[ind:]
+                        sn_range.insert(0,sn)
+                        found_start = True
+                        break
+
+        # If there were no valid implicit rules, we suggest a new one from the beginning
+        if not found_start:
+            new_rule = self.make_update_rule(old_class, sn_range[0].old_class)
+            R = new_rule(self, 'GENERATED.' + new_rule.__name__)
+            R.find_sub_paths()
+            sn = ScaffoldNode(old_class, sn_range[0].old_class, R)
+            self.extra_nodes.append(sn)
+            sn_range.insert(0,sn)
+
+        self.found_paths[key] = sn_range
+        return sn_range
+
+
+    def migrate_raw(self, msg_from, msg_to):
+        path = self.find_path(msg_from[4], msg_to[4])
+
+        if False in [sn.rule.valid for sn in path]:
+            raise BagMigrationException("Migrate called, but no valid migration path from [%s] to [%s]"%(msg_from[0], msg_to[0]))
+
+        # Short cut to speed up case of matching md5sum:
+        if path == [] or msg_from[2] == msg_to[2]:
+            return (msg_to[0], msg_from[1], msg_to[2], msg_to[3], msg_to[4])
+
+        tmp_msg = path[0].old_class()
+        tmp_msg.deserialize(msg_from[1])
+
+        for sn in path:
+            tmp_msg = sn.rule.apply(tmp_msg)
+
+        buff = StringIO()
+        tmp_msg.serialize(buff)
+
+        return (msg_to[0], buff.getvalue(), msg_to[2], msg_to[3], msg_to[4])
+
+
+
+    def migrate(self, msg_from, msg_to):
+        path = self.find_path(msg_from.__class__, msg_to.__class__)
+
+        if False in [sn.rule.valid for sn in path]:
+            raise BagMigrationException("Migrate called, but no valid migration path from [%s] to [%s]"%(msg_from._type, msg_to._type))
+
+        # Short cut to speed up case of matching md5sum:
+        if path == [] or msg_from._md5sum == msg_to._md5sum:
+            buff = StringIO()
+            msg_from.serialize(buff)
+            msg_to.deserialize(buff.getvalue())
+            return
+
+        if len(path) > 0:
+            buff = StringIO()
+            msg_from.serialize(buff)
+
+            tmp_msg = path[0].old_class()
+
+            tmp_msg.deserialize(buff.getvalue())
+
+            for sn in path:
+                tmp_msg = sn.rule.apply(tmp_msg)
+        else:
+            tmp_msg = msg_from
+
+        buff = StringIO()
+        tmp_msg.serialize(buff)
+        msg_to.deserialize(buff.getvalue())
+
+    def migrate_array(self, msg_from_array, msg_to_array):
+        if len(msg_from_array) != len(msg_to_array):
+            raise BagMigrationException("Migrate array called on on arrays of unequal length.")
+
+        if len(msg_from_array) == 0:
+            return
+
+        path = self.find_path(msg_from_array[0].__class__, msg_to_array[0].__class__)
+
+        if path is None:
+            raise BagMigrationException("Migrate called, but no migration path from [%s] to [%s]"%(msg_from._type, msg_to._type))
+
+        # Short cut to speed up case of matching md5sum:
+        if path == []:
+            for i in range(len(msg_from_array)):
+                buff = StringIO()
+                msg_from_array[i].serialize(buff)
+                msg_to_array[i].deserialize(buff.getvalue())
+            return
+
+        for i in range(len(msg_from_array)):
+            buff = StringIO()
+            tmp_msg = path[0].old_class()
+            msg_from_array[i].serialize(buff)
+            tmp_msg.deserialize(buff.getvalue())
+            for sn in path:
+                tmp_msg = sn.rule.apply(tmp_msg)
+
+            buff = StringIO()
+            tmp_msg.serialize(buff)
+            msg_to_array[i].deserialize(buff.getvalue())
+
+    def make_update_rule(self, old_class, new_class):
+        name = "update_%s_%s"%(old_class._type.replace("/","_"), old_class._md5sum)
+
+        # We assemble the class as a string and then exec it to end up with a class
+        # that can essentially print its own definition.
+        classdef = "class %s(MessageUpdateRule):\n"%name
+        classdef += "\told_type = \"%s\"\n"%old_class._type
+        classdef += "\told_full_text = \"\"\"\n%s\n\"\"\"\n\n"%old_class._full_text.strip()
+        classdef += "\tnew_type = \"%s\"\n"%new_class._type
+        classdef += "\tnew_full_text = \"\"\"\n%s\n\"\"\"\n"%new_class._full_text.strip()
+        classdef += "\n"
+        classdef += "\torder = 0"
+        classdef += "\n"
+
+        validdef = "\tvalid = True\n"
+
+        migratedefs = "\tmigrated_types = ["
+
+        updatedef = "\tdef update(self, old_msg, new_msg):\n"
+
+        old_consts = constants_from_def(old_class._type, old_class._full_text)
+        new_consts = constants_from_def(new_class._type, new_class._full_text)
+
+        if (not new_consts >= old_consts):
+            validdef = "\tvalid = False\n"
+            for c in (old_consts - new_consts):
+                updatedef += "\t\t#Constant '%s' has changed\n"%(c[0],)
+        
+        old_slots = []
+        old_slots.extend(old_class.__slots__)
+
+        migrations_seen = []
+
+        # Assign across primitives, self.migrate or self.migrate_array non-primitives
+        for (s,t) in zip(new_class.__slots__, new_class._slot_types):
+            warn_msg = None
+            new_base_type, new_is_array, new_array_len = genmsg.msgs.parse_type(t)
+            try:
+                ind = old_class.__slots__.index(s)
+                old_slots.remove(s)
+                old_base_type, old_is_array, old_array_len = genmsg.msgs.parse_type(old_class._slot_types[ind])
+
+                if new_is_array != old_is_array:
+                    warn_msg = "Could not match array with nonarray"
+
+                elif new_array_len != old_array_len:
+                    if old_array_len is None:
+                        warn_msg = "Converted from variable length array to fixed array of length %d"%(new_array_len)
+                    elif new_array_len is None:
+                        warn_msg = "Converted from fixed array of length %d to variable length"%(old_array_len)
+                    else:
+                        warn_msg = "Fixed length array converted from %d to %d"%(old_array_len,new_array_len)
+
+                elif genmsg.msgs.is_builtin(new_base_type):
+                    if new_base_type != old_base_type:
+                        warn_msg = "Primitive type changed"
+                    else:
+                        updatedef += "\t\tnew_msg.%s = old_msg.%s\n"%(s,s)
+
+                else:
+                    tmp_old_type = clean_name(old_base_type, old_class._type)
+                    tmp_new_type = clean_name(new_base_type, new_class._type)
+
+                    tmp_qualified_old_type = qualified_name(old_base_type, old_class._type)
+                    tmp_qualified_new_type = qualified_name(new_base_type, new_class._type)
+
+                    # Verify the type can theoretically be migrated
+                    if (tmp_qualified_old_type == tmp_qualified_new_type) or \
+                            (tmp_qualified_old_type in self.rename_map and
+                             tmp_qualified_new_type in self.rename_map[tmp_qualified_old_type]):
+
+                        if (tmp_old_type, tmp_new_type) not in migrations_seen:
+                            migratedefs += "\n\t\t(\"%s\",\"%s\"),"%(tmp_old_type, tmp_new_type)
+                            migrations_seen.append((tmp_old_type, tmp_new_type))
+
+                        if not new_is_array:
+                            updatedef += "\t\tself.migrate(old_msg.%s, new_msg.%s)\n"%(s,s)
+                        else:
+                            updatedef += "\t\tself.migrate_array(old_msg.%s, new_msg.%s, \"%s\")\n"%(s,s,new_base_type)
+                    else:
+                        warn_msg = "No migration path between [%s] and [%s]"%(tmp_old_type, tmp_new_type)
+            except ValueError:
+                warn_msg = "No matching field name in old message"
+
+            if warn_msg is not None:
+                validdef = "\tvalid = False\n"
+                updatedef += "\t\t#%s\n"%warn_msg
+                updatedef += "\t\tnew_msg.%s = %s\n"%(s,migration_default_value(t))
+                
+        migratedefs += "]\n"
+
+        if old_slots:
+            validdef = "\tvalid = False\n"
+            for s in old_slots:
+                updatedef += "\t\t#No field to match field %s from old message\n"%(s)
+
+        classdef += migratedefs + '\n' + validdef + '\n' + updatedef
+
+        printclassdef = classdef +  "\tdef get_class_def(self):\n\t\treturn \'\'\'%s\'\'\'\n"%classdef
+        
+        # This is probably a TERRIBLE idea?
+        exec(printclassdef)
+        return locals()[name]
+
+    def make_old_half_rule(self, old_class):
+        name = "update__%s__%s"%(old_class._type.replace("/","_"), old_class._md5sum)
+
+        # We assemble the class as a string and then exec it to end up with a class
+        # that can essentially print its own definition.
+        classdef = "class %s(MessageUpdateRule):\n"%name
+        classdef += "\told_type = \"%s\"\n"%old_class._type
+        classdef += "\told_full_text = \"\"\"\n%s\n\"\"\"\n\n"%old_class._full_text.strip()
+        classdef += "\tnew_type = \"\"\n"
+        classdef += "\tnew_full_text = \"\"\"\n\n\"\"\"\n"
+        classdef += "\n"
+        classdef += "\torder = 0"
+        classdef += "\n"
+    
+        validdef = "\tvalid = False\n"
+
+        migratedefs = "\tmigrated_types = []\n"
+
+        updatedef = "\tdef update(self, old_msg, new_msg):\n"
+        updatedef += "\t\tpass\n"
+        
+        classdef += migratedefs + '\n' + validdef + '\n' + updatedef
+
+        printclassdef = classdef +  "\tdef get_class_def(self):\n\t\treturn \'\'\'%s\'\'\'\n"%classdef
+        
+        # This is probably a TERRIBLE idea?
+        exec(printclassdef)
+        return locals()[name]
+
+    def make_new_half_rule(self, new_class):
+        name = "update_to_%s_%s"%(new_class._type.replace("/","_"), new_class._md5sum)
+
+        # We assemble the class as a string and then exec it to end up with a class
+        # that can essentially print its own definition.
+        classdef = "class %s(MessageUpdateRule):\n"%name
+        classdef += "\told_type = \"\"\n"
+        classdef += "\told_full_text = \"\"\"\n\n\"\"\"\n\n"
+        classdef += "\tnew_type = \"%s\"\n"%new_class._type
+        classdef += "\tnew_full_text = \"\"\"\n%s\n\"\"\"\n"%new_class._full_text.strip()
+        classdef += "\n"
+        classdef += "\torder = 0"
+        classdef += "\n"
+    
+        validdef = "\tvalid = False\n"
+
+        migratedefs = "\tmigrated_types = []\n"
+
+        updatedef = "\tdef update(self, old_msg, new_msg):\n"
+        updatedef += "\t\tpass\n"
+        
+        classdef += migratedefs + '\n' + validdef + '\n' + updatedef
+
+        printclassdef = classdef +  "\tdef get_class_def(self):\n\t\treturn \'\'\'%s\'\'\'\n"%classdef
+        
+        # This is probably a TERRIBLE idea?
+        exec(printclassdef)
+        return locals()[name]
+
+def migration_default_value(field_type):
+    if field_type in ['bool', 'byte', 'int8', 'int16', 'int32', 'int64',\
+                          'char', 'uint8', 'uint16', 'uint32', 'uint64']:
+        return '0'
+    elif field_type in ['float32', 'float64']:
+        return '0.'
+    elif field_type == 'string':
+        # strings, byte[], and uint8s are all optimized to be strings
+        return "''"
+    elif field_type.endswith(']'): # array type
+        base_type, is_array, array_len = genmsg.msgs.parse_type(field_type)
+        if base_type in ['byte', 'uint8']:
+            # strings, byte[], and uint8s are all optimized to be strings
+            if array_len is not None:
+                return "chr(0)*%s"%array_len
+            else:
+                return "''"
+        elif array_len is None: #var-length
+            return '[]'
+        else: # fixed-length, fill values
+            def_val = migration_default_value(base_type)
+            return '[' + ','.join(itertools.repeat(def_val, array_len)) + ']'
+    else:
+        return "self.get_new_class('%s')()"%field_type
+
+def constants_from_def(core_type, msg_def):
+    core_pkg, core_base_type = genmsg.package_resource_name(core_type)
+
+    splits = msg_def.split('\n' + '=' * 80 + '\n')
+    
+    core_msg = splits[0]
+    deps_msgs = splits[1:]
+
+    # create MsgSpec representations of .msg text
+    from genmsg import MsgContext
+    context = MsgContext.create_default()
+    specs = { core_type: genmsg.msg_loader.load_msg_from_string(context, core_msg, core_pkg) }
+    # - dependencies
+#    for dep_msg in deps_msgs:
+#        # dependencies require more handling to determine type name
+#        dep_type, dep_spec = _generate_dynamic_specs(specs, dep_msg)
+#        specs[dep_type] = dep_spec
+
+    return set([(x.name, x.val, x.type) for x in specs[core_type].constants])
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/src/rosbag/rosbag_main.py
@@ -0,0 +1,1036 @@
+# Software License Agreement (BSD License)
+#
+# Copyright (c) 2010, Willow Garage, Inc.
+# All rights reserved.
+#
+# Redistribution and use in source and binary forms, with or without
+# modification, are permitted provided that the following conditions
+# are met:
+#
+#  * Redistributions of source code must retain the above copyright
+#    notice, this list of conditions and the following disclaimer.
+#  * Redistributions in binary form must reproduce the above
+#    copyright notice, this list of conditions and the following
+#    disclaimer in the documentation and/or other materials provided
+#    with the distribution.
+#  * Neither the name of Willow Garage, Inc. nor the names of its
+#    contributors may be used to endorse or promote products derived
+#    from this software without specific prior written permission.
+#
+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+# POSSIBILITY OF SUCH DAMAGE.
+
+from __future__ import print_function
+
+import optparse
+import os
+import shutil
+import signal
+import subprocess
+import sys
+import time
+try:
+    from UserDict import UserDict  # Python 2.x
+except ImportError:
+    from collections import UserDict  # Python 3.x
+
+import roslib.message
+import roslib.packages
+
+from .bag import Bag, Compression, ROSBagException, ROSBagFormatException, ROSBagUnindexedException, ROSBagEncryptNotSupportedException, ROSBagEncryptException
+from .migration import MessageMigrator, fixbag2, checkbag
+
+def print_trans(old, new, indent):
+    from_txt = '%s [%s]' % (old._type, old._md5sum)
+    if new is not None:
+        to_txt= '%s [%s]' % (new._type, new._md5sum)
+    else:
+        to_txt = 'Unknown'
+    print('    ' * indent + ' * From: %s' % from_txt)
+    print('    ' * indent + '   To:   %s' % to_txt)
+
+def handle_split(option, opt_str, value, parser):
+    parser.values.split = True
+    if len(parser.rargs) > 0 and parser.rargs[0].isdigit():
+        print("Use of \"--split <MAX_SIZE>\" has been deprecated.  Please use --split --size <MAX_SIZE> or --split --duration <MAX_DURATION>", file=sys.stderr)
+        parser.values.size = int(parser.rargs.pop(0))
+
+
+def _stop_process(signum, frame, old_handler, process):
+    process.terminate()
+    if old_handler:
+        old_handler(signum, frame)
+
+
+def _send_process_sigint(signum, frame, old_handler, process):
+    process.send_signal(signal.SIGINT)
+    if old_handler:
+        old_handler(signum, frame)
+
+
+def record_cmd(argv):
+    parser = optparse.OptionParser(usage="rosbag record TOPIC1 [TOPIC2 TOPIC3 ...]",
+                                   description="Record a bag file with the contents of specified topics.",
+                                   formatter=optparse.IndentedHelpFormatter())
+
+    parser.add_option("-a", "--all",           dest="all",           default=False, action="store_true",          help="record all topics")
+    parser.add_option("-e", "--regex",         dest="regex",         default=False, action="store_true",          help="match topics using regular expressions")
+    parser.add_option("-p", "--publish",       dest="publish",       default=False, action="store_true",          help="publish a msg when the record begin")
+    parser.add_option("-x", "--exclude",       dest="exclude_regex", default="",    action="store",               help="exclude topics matching the follow regular expression (subtracts from -a or regex)")
+    parser.add_option("-q", "--quiet",         dest="quiet",         default=False, action="store_true",          help="suppress console output")
+    parser.add_option("-o", "--output-prefix", dest="prefix",        default=None,  action="store",               help="prepend PREFIX to beginning of bag name (name will always end with date stamp)")
+    parser.add_option("-O", "--output-name",   dest="name",          default=None,  action="store",               help="record to bag with name NAME.bag")
+    parser.add_option(      "--split",         dest="split",         default=False, callback=handle_split, action="callback",    help="split the bag when maximum size or duration is reached")
+    parser.add_option(      "--max-splits",    dest="max_splits",                   type='int',   action="store", help="Keep a maximum of N bag files, when reaching the maximum erase the oldest one to keep a constant number of files.", metavar="MAX_SPLITS")
+    parser.add_option(      "--size",          dest="size",                         type='int',   action="store", help="record a bag of maximum size SIZE MB. (Default: infinite)", metavar="SIZE")
+    parser.add_option(      "--duration",      dest="duration",                     type='string',action="store", help="record a bag of maximum duration DURATION in seconds, unless 'm', or 'h' is appended.", metavar="DURATION")
+    parser.add_option("-b", "--buffsize",      dest="buffsize",      default=256,   type='int',   action="store", help="use an internal buffer of SIZE MB (Default: %default, 0 = infinite)", metavar="SIZE")
+    parser.add_option("--chunksize",           dest="chunksize",     default=768,   type='int',   action="store", help="Advanced. Record to chunks of SIZE KB (Default: %default)", metavar="SIZE")
+    parser.add_option("-l", "--limit",         dest="num",           default=0,     type='int',   action="store", help="only record NUM messages on each topic")
+    parser.add_option(      "--node",          dest="node",          default=None,  type='string',action="store", help="record all topics subscribed to by a specific node")
+    parser.add_option("-j", "--bz2",           dest="compression",   default=None,  action="store_const", const='bz2', help="use BZ2 compression")
+    parser.add_option("--lz4",                 dest="compression",                  action="store_const", const='lz4', help="use LZ4 compression")
+    parser.add_option("--tcpnodelay",          dest="tcpnodelay",                   action="store_true",          help="Use the TCP_NODELAY transport hint when subscribing to topics.")
+    parser.add_option("--udp",                 dest="udp",                          action="store_true",          help="Use the UDP transport hint when subscribing to topics.")
+    parser.add_option("--repeat-latched",      dest="repeat_latched",               action="store_true",          help="Repeat latched msgs at the start of each new bag file.")
+
+    (options, args) = parser.parse_args(argv)
+
+    if len(args) == 0 and not options.all and not options.node:
+        parser.error("You must specify a topic name or else use the '-a' option.")
+
+    if options.prefix is not None and options.name is not None:
+        parser.error("Can't set both prefix and name.")
+
+    recordpath = roslib.packages.find_node('rosbag', 'record')
+    if not recordpath:
+        parser.error("Cannot find rosbag/record executable")
+    cmd = [recordpath[0]]
+
+    cmd.extend(['--buffsize',  str(options.buffsize)])
+    cmd.extend(['--chunksize', str(options.chunksize)])
+
+    if options.num != 0:      cmd.extend(['--limit', str(options.num)])
+    if options.quiet:         cmd.extend(["--quiet"])
+    if options.prefix:        cmd.extend(["-o", options.prefix])
+    if options.name:          cmd.extend(["-O", options.name])
+    if options.exclude_regex: cmd.extend(["--exclude", options.exclude_regex])
+    if options.all:           cmd.extend(["--all"])
+    if options.regex:         cmd.extend(["--regex"])
+    if options.publish:       cmd.extend(["--publish"])
+    if options.compression:   cmd.extend(["--%s" % options.compression])
+    if options.split:
+        if not options.duration and not options.size:
+            parser.error("Split specified without giving a maximum duration or size")
+        cmd.extend(["--split"])
+        if options.max_splits:
+            cmd.extend(["--max-splits", str(options.max_splits)])
+    if options.duration:    cmd.extend(["--duration", options.duration])
+    if options.size:        cmd.extend(["--size", str(options.size)])
+    if options.node:
+        cmd.extend(["--node", options.node])
+    if options.tcpnodelay:  cmd.extend(["--tcpnodelay"])
+    if options.udp:         cmd.extend(["--udp"])
+    if options.repeat_latched:  cmd.extend(["--repeat-latched"])
+
+    cmd.extend(args)
+
+    old_handler = signal.signal(
+        signal.SIGTERM,
+        lambda signum, frame: _stop_process(signum, frame, old_handler, process)
+    )
+
+    old_handler = signal.signal(
+        signal.SIGINT,
+        lambda signum, frame: _send_process_sigint(signum, frame, old_handler, process)
+    )
+
+    # Better way of handling it than os.execv
+    # This makes sure stdin handles are passed to the process.
+    process = subprocess.Popen(cmd)
+    process.wait()
+
+
+def info_cmd(argv):
+    parser = optparse.OptionParser(usage='rosbag info [options] BAGFILE1 [BAGFILE2 BAGFILE3 ...]',
+                                   description='Summarize the contents of one or more bag files.')
+    parser.add_option('-y', '--yaml', dest='yaml', default=False, action='store_true', help='print information in YAML format')
+    parser.add_option('-k', '--key',  dest='key',  default=None,  action='store',      help='print information on the given key')
+    parser.add_option(      '--freq', dest='freq', default=False, action='store_true', help='display topic message frequency statistics')
+    (options, args) = parser.parse_args(argv)
+
+    if len(args) == 0:
+        parser.error('You must specify at least 1 bag file.')
+    if options.key and not options.yaml:
+        parser.error('You can only specify key when printing in YAML format.')
+
+    for i, arg in enumerate(args):
+        try:
+            b = Bag(arg, 'r', skip_index=not options.freq)
+            if options.yaml:
+                info = b._get_yaml_info(key=options.key)
+                if info is not None:
+                    print(info)
+            else:
+                print(b)
+            b.close()
+            if i < len(args) - 1:
+                print('---')
+        
+        except (ROSBagEncryptNotSupportedException, ROSBagEncryptException) as ex:
+            print('ERROR: %s' % str(ex), file=sys.stderr)
+        except ROSBagUnindexedException as ex:
+            print('ERROR bag unindexed: %s.  Run rosbag reindex.' % arg,
+                  file=sys.stderr)
+            sys.exit(1)
+        except ROSBagException as ex:
+            print('ERROR reading %s: %s' % (arg, str(ex)), file=sys.stderr)
+            sys.exit(1)
+        except IOError as ex:
+            print('ERROR reading %s: %s' % (arg, str(ex)), file=sys.stderr)
+            sys.exit(1)
+
+
+def handle_topics(option, opt_str, value, parser):
+    topics = []
+    for arg in parser.rargs:
+        if arg[:2] == "--" and len(arg) > 2:
+            break
+        if arg[:1] == "-" and len(arg) > 1:
+            break
+        topics.append(arg)
+    parser.values.topics.extend(topics)
+    del parser.rargs[:len(topics)]
+
+
+def handle_pause_topics(option, opt_str, value, parser):
+    pause_topics = []
+    for arg in parser.rargs:
+        if arg[:2] == "--" and len(arg) > 2:
+            break
+        if arg[:1] == "-" and len(arg) > 1:
+            break
+        pause_topics.append(arg)
+    parser.values.pause_topics.extend(pause_topics)
+    del parser.rargs[:len(pause_topics)]
+
+
+def play_cmd(argv):
+    parser = optparse.OptionParser(usage="rosbag play BAGFILE1 [BAGFILE2 BAGFILE3 ...]",
+                                   description="Play back the contents of one or more bag files in a time-synchronized fashion.")
+    parser.add_option("-p", "--prefix",       dest="prefix",     default='',    type='str',          help="prefix all output topics")
+    parser.add_option("-q", "--quiet",        dest="quiet",      default=False, action="store_true", help="suppress console output")
+    parser.add_option("-i", "--immediate",    dest="immediate",  default=False, action="store_true", help="play back all messages without waiting")
+    parser.add_option("--pause",              dest="pause",      default=False, action="store_true", help="start in paused mode")
+    parser.add_option("--queue",              dest="queue",      default=100,     type='int', action="store", help="use an outgoing queue of size SIZE (defaults to %default)", metavar="SIZE")
+    parser.add_option("--clock",              dest="clock",      default=False, action="store_true", help="publish the clock time")
+    parser.add_option("--hz",                 dest="freq",       default=100,   type='float', action="store", help="use a frequency of HZ when publishing clock time (default: %default)", metavar="HZ")
+    parser.add_option("-d", "--delay",        dest="delay",      default=0.2,   type='float', action="store", help="sleep SEC seconds after every advertise call (to allow subscribers to connect)", metavar="SEC")
+    parser.add_option("-r", "--rate",         dest="rate",       default=1.0,   type='float', action="store", help="multiply the publish rate by FACTOR", metavar="FACTOR")
+    parser.add_option("-s", "--start",        dest="start",      default=0.0,   type='float', action="store", help="start SEC seconds into the bag files", metavar="SEC")
+    parser.add_option("-u", "--duration",     dest="duration",   default=None,  type='float', action="store", help="play only SEC seconds from the bag files", metavar="SEC")
+    parser.add_option("--skip-empty",         dest="skip_empty", default=None,  type='float', action="store", help="skip regions in the bag with no messages for more than SEC seconds", metavar="SEC")
+    parser.add_option("-l", "--loop",         dest="loop",       default=False, action="store_true", help="loop playback")
+    parser.add_option("-k", "--keep-alive",   dest="keep_alive", default=False, action="store_true", help="keep alive past end of bag (useful for publishing latched topics)")
+    parser.add_option("--try-future-version", dest="try_future", default=False, action="store_true", help="still try to open a bag file, even if the version number is not known to the player")
+    parser.add_option("--topics", dest="topics", default=[],  callback=handle_topics, action="callback", help="topics to play back")
+    parser.add_option("--pause-topics", dest="pause_topics", default=[],  callback=handle_pause_topics, action="callback", help="topics to pause on during playback")
+    parser.add_option("--bags",  help="bags files to play back from")
+    parser.add_option("--wait-for-subscribers",  dest="wait_for_subscribers", default=False, action="store_true", help="wait for at least one subscriber on each topic before publishing")
+    parser.add_option("--rate-control-topic", dest="rate_control_topic", default='', type='str', help="watch the given topic, and if the last publish was more than <rate-control-max-delay> ago, wait until the topic publishes again to continue playback")
+    parser.add_option("--rate-control-max-delay", dest="rate_control_max_delay", default=1.0, type='float', help="maximum time difference from <rate-control-topic> before pausing")
+
+    (options, args) = parser.parse_args(argv)
+
+    if options.bags:
+        args.append(options.bags)
+
+    if len(args) == 0:
+        parser.error('You must specify at least 1 bag file to play back.')
+
+    playpath = roslib.packages.find_node('rosbag', 'play')
+    if not playpath:
+        parser.error("Cannot find rosbag/play executable")
+    cmd = [playpath[0]]
+
+    if options.prefix:
+        cmd.extend(["--prefix", str(options.prefix)])
+
+    if options.quiet:      cmd.extend(["--quiet"])
+    if options.pause:      cmd.extend(["--pause"])
+    if options.immediate:  cmd.extend(["--immediate"])
+    if options.loop:       cmd.extend(["--loop"])
+    if options.keep_alive: cmd.extend(["--keep-alive"])
+    if options.try_future: cmd.extend(["--try-future-version"])
+    if options.wait_for_subscribers: cmd.extend(["--wait-for-subscribers"])
+
+    if options.clock:
+        cmd.extend(["--clock", "--hz", str(options.freq)])
+
+    cmd.extend(['--queue', str(options.queue)])
+    cmd.extend(['--rate', str(options.rate)])
+    cmd.extend(['--delay', str(options.delay)])
+    cmd.extend(['--start', str(options.start)])
+    if options.duration:
+        cmd.extend(['--duration', str(options.duration)])
+    if options.skip_empty:
+        cmd.extend(['--skip-empty', str(options.skip_empty)])
+
+    if options.topics:
+        cmd.extend(['--topics'] + options.topics)
+
+    if options.pause_topics:
+        cmd.extend(['--pause-topics'] + options.pause_topics)
+
+    # prevent bag files to be passed as --topics or --pause-topics
+    if options.topics or options.pause_topics:
+        cmd.extend(['--bags'])
+
+    cmd.extend(args)
+
+    if options.rate_control_topic:
+        cmd.extend(['--rate-control-topic', str(options.rate_control_topic)])
+
+    if options.rate_control_max_delay:
+        cmd.extend(['--rate-control-max-delay', str(options.rate_control_max_delay)])
+
+    old_handler = signal.signal(
+        signal.SIGTERM,
+        lambda signum, frame: _stop_process(signum, frame, old_handler, process)
+    )
+
+    old_handler = signal.signal(
+        signal.SIGINT,
+        lambda signum, frame: _send_process_sigint(signum, frame, old_handler, process)
+    )
+
+    # Better way of handling it than os.execv
+    # This makes sure stdin handles are passed to the process.
+    process = subprocess.Popen(cmd)
+    process.wait()
+
+
+def filter_cmd(argv):
+    def expr_eval(expr):
+        return eval("lambda topic, m, t: %s" % expr)
+
+    parser = optparse.OptionParser(usage="""rosbag filter [options] INBAG OUTBAG EXPRESSION
+
+EXPRESSION can be any Python-legal expression.
+
+The following variables are available:
+ * topic: name of topic
+ * m: message
+ * t: time of message (t.secs, t.nsecs)""",
+                                   description='Filter the contents of the bag.')
+    parser.add_option('-p', '--print', action='store', dest='verbose_pattern', default=None, metavar='PRINT-EXPRESSION', help='Python expression to print for verbose debugging. Uses same variables as filter-expression')
+
+    options, args = parser.parse_args(argv)
+    if len(args) == 0:
+        parser.error('You must specify an in bag, an out bag, and an expression.')
+    if len(args) == 1:
+        parser.error('You must specify an out bag and an expression.')
+    if len(args) == 2:
+        parser.error("You must specify an expression.")
+    if len(args) > 3:
+        parser.error("Too many arguments.")
+
+    inbag_filename, outbag_filename, expr = args
+
+    if not os.path.isfile(inbag_filename):
+        print('Cannot locate input bag file [%s]' % inbag_filename, file=sys.stderr)
+        sys.exit(2)
+
+    if os.path.realpath(inbag_filename) == os.path.realpath(outbag_filename):
+        print('Cannot use same file as input and output [%s]' % inbag_filename, file=sys.stderr)
+        sys.exit(3)
+
+    filter_fn = expr_eval(expr)
+
+    outbag = Bag(outbag_filename, 'w')
+    
+    try:
+        inbag = Bag(inbag_filename)
+    except (ROSBagEncryptNotSupportedException, ROSBagEncryptException) as ex:
+        print('ERROR: %s' % str(ex), file=sys.stderr)
+        return
+    except ROSBagUnindexedException as ex:
+        print('ERROR bag unindexed: %s.  Run rosbag reindex.' % inbag_filename, file=sys.stderr)
+        sys.exit(1)
+
+    try:
+        meter = ProgressMeter(outbag_filename, inbag._uncompressed_size)
+        total_bytes = 0
+    
+        if options.verbose_pattern:
+            verbose_pattern = expr_eval(options.verbose_pattern)
+    
+            for topic, raw_msg, t, conn_header in inbag.read_messages(raw=True, return_connection_header=True):
+                msg_type, serialized_bytes, md5sum, pos, pytype = raw_msg
+                msg = pytype()
+                msg.deserialize(serialized_bytes)
+
+                if filter_fn(topic, msg, t):
+                    print('MATCH', verbose_pattern(topic, msg, t))
+                    outbag.write(topic, msg, t, connection_header=conn_header)
+                else:
+                    print('NO MATCH', verbose_pattern(topic, msg, t))          
+
+                total_bytes += len(serialized_bytes) 
+                meter.step(total_bytes)
+        else:
+            for topic, raw_msg, t, conn_header in inbag.read_messages(raw=True, return_connection_header=True):
+                msg_type, serialized_bytes, md5sum, pos, pytype = raw_msg
+                msg = pytype()
+                msg.deserialize(serialized_bytes)
+
+                if filter_fn(topic, msg, t):
+                    outbag.write(topic, msg, t, connection_header=conn_header)
+
+                total_bytes += len(serialized_bytes)
+                meter.step(total_bytes)
+        
+        meter.finish()
+
+    finally:
+        inbag.close()
+        outbag.close()
+
+def fix_cmd(argv):
+    parser = optparse.OptionParser(usage='rosbag fix INBAG OUTBAG [EXTRARULES1 EXTRARULES2 ...]', description='Repair the messages in a bag file so that it can be played in the current system.')
+    parser.add_option('-n', '--noplugins', action='store_true', dest='noplugins', help='do not load rulefiles via plugins')
+    parser.add_option('--force', action='store_true', dest='force', help='proceed with migrations, even if not all rules defined')
+
+    (options, args) = parser.parse_args(argv)
+
+    if len(args) < 1:
+        parser.error('You must pass input and output bag files.')
+    if len(args) < 2:
+        parser.error('You must pass an output bag file.')
+
+    inbag_filename  = args[0]
+    outbag_filename = args[1]
+    rules           = args[2:]   
+
+    ext = os.path.splitext(outbag_filename)[1]
+    if ext == '.bmr':
+        parser.error('Input file should be a bag file, not a rule file.')
+    if ext != '.bag':
+        parser.error('Output file must be a bag file.')
+
+    outname = outbag_filename + '.tmp'
+
+    if os.path.exists(outbag_filename):
+        if not os.access(outbag_filename, os.W_OK):
+            print('Don\'t have permissions to access %s' % outbag_filename, file=sys.stderr)
+            sys.exit(1)
+    else:
+        try:
+            file = open(outbag_filename, 'w')
+            file.close()
+        except IOError as e:
+            print('Cannot open %s for writing' % outbag_filename, file=sys.stderr)
+            sys.exit(1)
+
+    if os.path.exists(outname):
+        if not os.access(outname, os.W_OK):
+            print('Don\'t have permissions to access %s' % outname, file=sys.stderr)
+            sys.exit(1)
+    else:
+        try:
+            file = open(outname, 'w')
+            file.close()
+        except IOError as e:
+            print('Cannot open %s for writing' % outname, file=sys.stderr)
+            sys.exit(1)
+
+    if options.noplugins is None:
+        options.noplugins = False
+
+    migrator = MessageMigrator(rules, plugins=not options.noplugins)
+    
+    try:
+        migrations = fixbag2(migrator, inbag_filename, outname, options.force)
+    except (ROSBagEncryptNotSupportedException, ROSBagEncryptException) as ex:
+        print('ERROR: %s' % str(ex), file=sys.stderr)
+        return
+    except ROSBagUnindexedException as ex:
+        print('ERROR bag unindexed: %s.  Run rosbag reindex.' % inbag_filename,
+              file=sys.stderr)
+        sys.exit(1)
+
+    if len(migrations) == 0:
+        os.rename(outname, outbag_filename)
+        print('Bag migrated successfully.')
+    else:
+        print('Bag could not be migrated.  The following migrations could not be performed:')
+        for m in migrations:
+            print_trans(m[0][0].old_class, m[0][-1].new_class, 0)
+            
+            if len(m[1]) > 0:
+                print('    %d rules missing:' % len(m[1]))
+                for r in m[1]:
+                    print_trans(r.old_class, r.new_class,1)
+                    
+        print('Try running \'rosbag check\' to create the necessary rule files or run \'rosbag fix\' with the \'--force\' option.')
+        os.remove(outname)
+        sys.exit(1)
+
+def check_cmd(argv):
+    parser = optparse.OptionParser(usage='rosbag check BAG [-g RULEFILE] [EXTRARULES1 EXTRARULES2 ...]', description='Determine whether a bag is playable in the current system, or if it can be migrated.')
+    parser.add_option('-g', '--genrules',  action='store',      dest='rulefile', default=None, help='generate a rulefile named RULEFILE')
+    parser.add_option('-a', '--append',    action='store_true', dest='append',                 help='append to the end of an existing rulefile after loading it')
+    parser.add_option('-n', '--noplugins', action='store_true', dest='noplugins',              help='do not load rulefiles via plugins')
+    (options, args) = parser.parse_args(argv)
+
+    if len(args) == 0:
+        parser.error('You must specify a bag file to check.')
+    if options.append and options.rulefile is None:
+        parser.error('Cannot specify -a without also specifying -g.')
+    if options.rulefile is not None:
+        rulefile_exists = os.path.isfile(options.rulefile)
+        if rulefile_exists and not options.append:
+            parser.error('The file %s already exists.  Include -a if you intend to append.' % options.rulefile)
+        if not rulefile_exists and options.append:
+            parser.error('The file %s does not exist, and so -a is invalid.' % options.rulefile)
+    
+    if options.append:
+        append_rule = [options.rulefile]
+    else:
+        append_rule = []
+
+    # First check that the bag is not unindexed 
+    try:
+        Bag(args[0])
+    except (ROSBagEncryptNotSupportedException, ROSBagEncryptException) as ex:
+        print('ERROR: %s' % str(ex), file=sys.stderr)
+        return
+    except ROSBagUnindexedException as ex:
+        print('ERROR bag unindexed: %s.  Run rosbag reindex.' % args[0], file=sys.stderr)
+        sys.exit(1)
+
+    mm = MessageMigrator(args[1:] + append_rule, not options.noplugins)
+
+    migrations = checkbag(mm, args[0])
+       
+    if len(migrations) == 0:
+        print('Bag file does not need any migrations.')
+        exit(0)
+        
+    print('The following migrations need to occur:')
+    all_rules = []
+    for m in migrations:
+        all_rules.extend(m[1])
+
+        print_trans(m[0][0].old_class, m[0][-1].new_class, 0)
+        if len(m[1]) > 0:
+            print("    %d rules missing:" % len(m[1]))
+            for r in m[1]:
+                print_trans(r.old_class, r.new_class, 1)
+
+    if options.rulefile is None:
+        if all_rules == []:
+            print("\nAll rules defined.  Bag is ready to be migrated")
+        else:
+            print("\nTo generate rules, please run with -g <rulefile>")
+        exit(0)
+
+    output = ''
+    rules_left = mm.filter_rules_unique(all_rules)
+
+    if rules_left == []:
+        print("\nNo additional rule files needed to be generated.  %s not created."%(options.rulefile))
+        exit(0)
+
+    while len(rules_left) > 0:
+        extra_rules = []
+        for r in rules_left:
+            if r.new_class is None:
+                print('The message type %s appears to have moved.  Please enter the type to migrate it to.' % r.old_class._type)
+                new_type = input('>')
+                new_class = roslib.message.get_message_class(new_type)
+                while new_class is None:
+                    print("\'%s\' could not be found in your system.  Please make sure it is built." % new_type)
+                    new_type = input('>')
+                    new_class = roslib.message.get_message_class(new_type)
+                new_rule = mm.make_update_rule(r.old_class, new_class)
+                R = new_rule(mm, 'GENERATED.' + new_rule.__name__)
+                R.find_sub_paths()
+                new_rules = [r for r in mm.expand_rules(R.sub_rules) if r.valid == False]
+                extra_rules.extend(new_rules)
+                print('Creating the migration rule for %s requires additional missing rules:' % new_type)
+                for nr in new_rules:
+                    print_trans(nr.old_class, nr.new_class,1)
+                output += R.get_class_def()
+            else:
+                output += r.get_class_def()
+        rules_left = mm.filter_rules_unique(extra_rules)
+    f = open(options.rulefile, 'a')
+    f.write(output)
+    f.close()
+
+    print('\nThe necessary rule files have been written to: %s' % options.rulefile)
+
+def compress_cmd(argv):
+    parser = optparse.OptionParser(usage='rosbag compress [options] BAGFILE1 [BAGFILE2 ...]',
+                                   description='Compress one or more bag files.')
+    parser.add_option(      '--output-dir', action='store',       dest='output_dir',  help='write to directory DIR', metavar='DIR')
+    parser.add_option('-f', '--force',      action='store_true',  dest='force',       help='force overwriting of backup file if it exists')
+    parser.add_option('-q', '--quiet',      action='store_true',  dest='quiet',       help='suppress noncritical messages')
+    parser.add_option('-j', '--bz2',        action='store_const', dest='compression', help='use BZ2 compression', const=Compression.BZ2, default=Compression.BZ2)
+    parser.add_option(      '--lz4',        action='store_const', dest='compression', help='use lz4 compression', const=Compression.LZ4)
+    (options, args) = parser.parse_args(argv)
+
+    if len(args) < 1:
+        parser.error('You must specify at least one bag file.')
+
+    op = lambda inbag, outbag, quiet: change_compression_op(inbag, outbag, options.compression, options.quiet)
+
+    bag_op(args, False, True, lambda b: False, op, options.output_dir, options.force, options.quiet)
+
+def decompress_cmd(argv):
+    parser = optparse.OptionParser(usage='rosbag decompress [options] BAGFILE1 [BAGFILE2 ...]',
+                                   description='Decompress one or more bag files.')
+    parser.add_option(      '--output-dir', action='store',      dest='output_dir', help='write to directory DIR', metavar='DIR')
+    parser.add_option('-f', '--force',      action='store_true', dest='force',      help='force overwriting of backup file if it exists')
+    parser.add_option('-q', '--quiet',      action='store_true', dest='quiet',      help='suppress noncritical messages')
+
+    (options, args) = parser.parse_args(argv)
+
+    if len(args) < 1:
+        parser.error('You must specify at least one bag file.')
+    
+    op = lambda inbag, outbag, quiet: change_compression_op(inbag, outbag, Compression.NONE, options.quiet)
+    
+    bag_op(args, False, True, lambda b: False, op, options.output_dir, options.force, options.quiet)
+
+def reindex_cmd(argv):
+    parser = optparse.OptionParser(usage='rosbag reindex [options] BAGFILE1 [BAGFILE2 ...]',
+                                   description='Reindexes one or more bag files.')
+    parser.add_option(      '--output-dir', action='store',      dest='output_dir', help='write to directory DIR', metavar='DIR')
+    parser.add_option('-f', '--force',      action='store_true', dest='force',      help='force overwriting of backup file if it exists')
+    parser.add_option('-q', '--quiet',      action='store_true', dest='quiet',      help='suppress noncritical messages')
+
+    (options, args) = parser.parse_args(argv)
+
+    if len(args) < 1:
+        parser.error('You must specify at least one bag file.')
+    
+    op = lambda inbag, outbag, quiet: reindex_op(inbag, outbag, options.quiet)
+
+    bag_op(args, True, True, lambda b: b.version > 102, op, options.output_dir, options.force, options.quiet)
+
+def encrypt_cmd(argv):
+    parser = optparse.OptionParser(usage='rosbag encrypt [options] BAGFILE1 [BAGFILE2 ...]',
+                                   description='Encrypt one or more bag files.')
+    parser.add_option(      '--output-dir', action='store',       dest='output_dir',  help='write to directory DIR', metavar='DIR')
+    parser.add_option('-f', '--force',      action='store_true',  dest='force',       help='force overwriting of backup file if it exists')
+    parser.add_option('-q', '--quiet',      action='store_true',  dest='quiet',       help='suppress noncritical messages')
+    parser.add_option("-p", "--plugin",     action='store',       dest="plugin",      default='rosbag/AesCbcEncryptor', help='encryptor plugin name')
+    parser.add_option("-r", "--param",      action='store',       dest="param",       default='*', help='encryptor plugin parameter')
+    parser.add_option('-j', '--bz2',        action='store_const', dest='compression', help='use BZ2 compression', const=Compression.BZ2, default=Compression.NONE)
+    parser.add_option(      '--lz4',        action='store_const', dest='compression', help='use lz4 compression', const=Compression.LZ4)
+    (options, args) = parser.parse_args(argv)
+
+    if len(args) < 1:
+        parser.error('You must specify at least one bag file.')
+
+    op = lambda inbag, outbag, quiet: change_encryption_op(inbag, outbag, options.plugin, options.param, options.compression, options.quiet)
+
+    bag_op(args, False, True, lambda b: False, op, options.output_dir, options.force, options.quiet)
+
+def decrypt_cmd(argv):
+    parser = optparse.OptionParser(usage='rosbag decrypt [options] BAGFILE1 [BAGFILE2 ...]',
+                                   description='Decrypt one or more bag files.')
+    parser.add_option(      '--output-dir', action='store',       dest='output_dir',  help='write to directory DIR', metavar='DIR')
+    parser.add_option('-f', '--force',      action='store_true',  dest='force',       help='force overwriting of backup file if it exists')
+    parser.add_option('-q', '--quiet',      action='store_true',  dest='quiet',       help='suppress noncritical messages')
+    parser.add_option('-j', '--bz2',        action='store_const', dest='compression', help='use BZ2 compression', const=Compression.BZ2, default=Compression.NONE)
+    parser.add_option(      '--lz4',        action='store_const', dest='compression', help='use lz4 compression', const=Compression.LZ4)
+    (options, args) = parser.parse_args(argv)
+
+    if len(args) < 1:
+        parser.error('You must specify at least one bag file.')
+
+    op = lambda inbag, outbag, quiet: change_encryption_op(inbag, outbag, 'rosbag/NoEncryptor', '*', options.compression, options.quiet)
+    # Note the second paramater is True: Python Bag class cannot read index information from encrypted bag files
+    bag_op(args, True, False, lambda b: False, op, options.output_dir, options.force, options.quiet)
+
+def bag_op(inbag_filenames, allow_unindexed, open_inbag, copy_fn, op, output_dir=None, force=False, quiet=False):
+    for inbag_filename in inbag_filenames:
+        if open_inbag:
+            # Check we can read the file
+            try:
+                inbag = Bag(inbag_filename, 'r', allow_unindexed=allow_unindexed)
+            except ROSBagUnindexedException:
+                print('ERROR bag unindexed: %s.  Run rosbag reindex.' % inbag_filename, file=sys.stderr)
+                continue
+            except (ROSBagException, IOError) as ex:
+                print('ERROR reading %s: %s' % (inbag_filename, str(ex)), file=sys.stderr)
+                continue
+
+            # Determine whether we should copy the bag
+            copy = copy_fn(inbag)
+
+            inbag.close()
+        else:
+            copy = False
+
+        # Determine filename for output bag
+        if output_dir is None:
+            outbag_filename = inbag_filename
+        else:
+            outbag_filename = os.path.join(output_dir, os.path.split(inbag_filename)[1])
+
+        backup_filename = None
+        if outbag_filename == inbag_filename:
+            # Rename the input bag to ###.orig.###, and open for reading
+            backup_filename = '%s.orig%s' % os.path.splitext(inbag_filename)
+            
+            if not force and os.path.exists(backup_filename):
+                if not quiet:
+                    print('Skipping %s. Backup path %s already exists.' % (inbag_filename, backup_filename), file=sys.stderr)
+                continue
+            
+            try:
+                if copy:
+                    shutil.copy(inbag_filename, backup_filename)
+                else:
+                    os.rename(inbag_filename, backup_filename)
+            except OSError as ex:
+                print('ERROR %s %s to %s: %s' % ('copying' if copy else 'moving', inbag_filename, backup_filename, str(ex)), file=sys.stderr)
+                continue
+            
+            source_filename = backup_filename
+        else:
+            if copy:
+                shutil.copy(inbag_filename, outbag_filename)
+                source_filename = outbag_filename
+            else:
+                source_filename = inbag_filename
+
+        try:
+            if open_inbag:
+                inbag = Bag(source_filename, 'r', allow_unindexed=allow_unindexed)
+
+                # Open the output bag file for writing
+                try:
+                    if copy:
+                        outbag = Bag(outbag_filename, 'a', allow_unindexed=allow_unindexed)
+                    else:
+                        outbag = Bag(outbag_filename, 'w')
+                except (ROSBagException, IOError) as ex:
+                    print('ERROR writing to %s: %s' % (outbag_filename, str(ex)), file=sys.stderr)
+                    inbag.close()
+                    continue
+
+                # Perform the operation
+                try:
+                    op(inbag, outbag, quiet=quiet)
+                except ROSBagException as ex:
+                    print('\nERROR operating on %s: %s' % (source_filename, str(ex)), file=sys.stderr)
+                    inbag.close()
+                    outbag.close()
+                    continue
+
+                outbag.close()
+                inbag.close()
+            else:
+                # Open the output bag file for writing
+                try:
+                    if copy:
+                        outbag = Bag(outbag_filename, 'a', allow_unindexed=allow_unindexed)
+                    else:
+                        outbag = Bag(outbag_filename, 'w')
+                except (ROSBagException, IOError) as ex:
+                    print('ERROR writing to %s: %s' % (outbag_filename, str(ex)), file=sys.stderr)
+                    continue
+
+                # Perform the operation
+                try:
+                    op(source_filename, outbag, quiet=quiet)
+                except ROSBagException as ex:
+                    print('\nERROR operating on %s: %s' % (source_filename, str(ex)), file=sys.stderr)
+                    outbag.close()
+                    continue
+
+                outbag.close()
+
+        except KeyboardInterrupt:
+            if backup_filename is not None:
+                try:
+                    if copy:
+                        os.remove(backup_filename)
+                    else:
+                        os.rename(backup_filename, inbag_filename)
+                except OSError as ex:
+                    print('ERROR %s %s to %s: %s', ('removing' if copy else 'moving', backup_filename, inbag_filename, str(ex)), file=sys.stderr)
+                    break
+    
+        except (ROSBagException, IOError) as ex:
+            print('ERROR operating on %s: %s' % (inbag_filename, str(ex)), file=sys.stderr)
+
+def change_compression_op(inbag, outbag, compression, quiet):
+    outbag.compression = compression
+
+    if quiet:
+        for topic, msg, t, conn_header in inbag.read_messages(raw=True, return_connection_header=True):
+            outbag.write(topic, msg, t, raw=True, connection_header=conn_header)
+    else:
+        meter = ProgressMeter(outbag.filename, inbag._uncompressed_size)
+
+        total_bytes = 0
+        for topic, msg, t, conn_header in inbag.read_messages(raw=True, return_connection_header=True):
+            msg_type, serialized_bytes, md5sum, pos, pytype = msg
+    
+            outbag.write(topic, msg, t, raw=True, connection_header=conn_header)
+            
+            total_bytes += len(serialized_bytes) 
+            meter.step(total_bytes)
+        
+        meter.finish()
+
+def reindex_op(inbag, outbag, quiet):
+    if inbag.version == 102:
+        if quiet:
+            try:
+                for offset in inbag.reindex():
+                    pass
+            except:
+                pass
+
+            for (topic, msg, t, conn_header) in inbag.read_messages(return_connection_header=True):
+                outbag.write(topic, msg, t, connection_header=conn_header)
+        else:
+            meter = ProgressMeter(outbag.filename, inbag.size)
+            try:
+                for offset in inbag.reindex():
+                    meter.step(offset)
+            except:
+                pass
+            meter.finish()
+
+            meter = ProgressMeter(outbag.filename, inbag.size)
+            for (topic, msg, t, conn_header) in inbag.read_messages(return_connection_header=True):
+                outbag.write(topic, msg, t, connection_header=conn_header)
+                meter.step(inbag._file.tell())
+            meter.finish()
+    else:
+        if quiet:
+            try:
+                for offset in outbag.reindex():
+                    pass
+            except (ROSBagEncryptNotSupportedException, ROSBagEncryptException) as ex:
+                raise
+            except:
+                pass
+        else:
+            meter = ProgressMeter(outbag.filename, outbag.size)
+            try:
+                for offset in outbag.reindex():
+                    meter.step(offset)
+            except (ROSBagEncryptNotSupportedException, ROSBagEncryptException) as ex:
+                raise
+            except:
+                pass
+            meter.finish()
+
+def change_encryption_op(inbag, outbag, plugin, param, compression, quiet):
+    # Output file must be closed before written by the encrypt process
+    outbag.close()
+
+    encryptpath = roslib.packages.find_node('rosbag', 'encrypt')
+    if not encryptpath:
+        parser.error("Cannot find rosbag/encrypt executable")
+    cmd = [encryptpath[0]]
+    if type(inbag) is str:
+        cmd.extend([inbag])
+    else:
+        cmd.extend([inbag.filename])
+    cmd.extend(['-o', outbag.filename])
+    cmd.extend(['-p', plugin])
+    cmd.extend(['-r', param])
+    if compression == 'bz2':
+        cmd.extend(['-j'])
+    elif compression == 'lz4':
+        cmd.extend(['--lz4'])
+    if quiet:
+        cmd.extend(['-q'])
+
+    old_handler = signal.signal(
+        signal.SIGTERM,
+        lambda signum, frame: _stop_process(signum, frame, old_handler, process)
+    )
+
+    process = subprocess.Popen(cmd)
+    process.wait()
+
+class RosbagCmds(UserDict):
+    def __init__(self):
+        UserDict.__init__(self)
+        self._description = {}
+        self['help'] = self.help_cmd
+
+    def add_cmd(self, name, function, description):
+        self[name] = function
+        self._description[name] = description
+        
+    def get_valid_cmds(self):
+        str = "Available subcommands:\n"
+        for k in sorted(self.keys()):
+            str += "   %s  " % k
+            if k in self._description.keys():
+                str +="\t%s" % self._description[k]
+            str += "\n"
+        return str
+
+    def help_cmd(self,argv):
+        argv = [a for a in argv if a != '-h' and a != '--help']
+
+        if len(argv) == 0:
+            print('Usage: rosbag <subcommand> [options] [args]')
+            print()
+            print("A bag is a file format in ROS for storing ROS message data. The rosbag command can record, replay and manipulate bags.")
+            print()
+            print(self.get_valid_cmds())
+            print('For additional information, see http://wiki.ros.org/rosbag')
+            print()
+            return
+
+        cmd = argv[0]
+        if cmd in self:
+            self[cmd](['-h'])
+        else:
+            print("Unknown command: '%s'" % cmd, file=sys.stderr)
+            print(self.get_valid_cmds(), file=sys.stderr)
+
+class ProgressMeter(object):
+    def __init__(self, path, bytes_total, refresh_rate=1.0):
+        self.path           = path
+        self.bytes_total    = bytes_total
+        self.refresh_rate   = refresh_rate
+        
+        self.elapsed        = 0.0
+        self.update_elapsed = 0.0
+        self.bytes_read     = 0.0
+
+        self.start_time     = time.time()
+
+        self._update_progress()
+
+    def step(self, bytes_read, force_update=False):
+        self.bytes_read = bytes_read
+        self.elapsed    = time.time() - self.start_time
+        
+        if force_update or self.elapsed - self.update_elapsed > self.refresh_rate:
+            self._update_progress()
+            self.update_elapsed = self.elapsed
+
+    def _update_progress(self):
+        max_path_len = self.terminal_width() - 37
+        path = self.path
+        if len(path) > max_path_len:
+            path = '...' + self.path[-max_path_len + 3:]
+
+        bytes_read_str  = self._human_readable_size(float(self.bytes_read))
+        bytes_total_str = self._human_readable_size(float(self.bytes_total))
+        
+        if self.bytes_read < self.bytes_total:
+            complete_fraction = float(self.bytes_read) / self.bytes_total
+            pct_complete      = int(100.0 * complete_fraction)
+
+            if complete_fraction > 0.0:
+                eta = self.elapsed * (1.0 / complete_fraction - 1.0)
+                eta_min, eta_sec = eta / 60, eta % 60
+                if eta_min > 99:
+                    eta_str = '--:--'
+                else:
+                    eta_str = '%02d:%02d' % (eta_min, eta_sec)
+            else:
+                eta_str = '--:--'
+
+            progress = '%-*s %3d%% %8s / %8s %s ETA' % (max_path_len, path, pct_complete, bytes_read_str, bytes_total_str, eta_str)
+        else:
+            progress = '%-*s 100%% %19s %02d:%02d    ' % (max_path_len, path, bytes_total_str, self.elapsed / 60, self.elapsed % 60)
+
+        print('\r', progress, end='')
+        sys.stdout.flush()
+        
+    def _human_readable_size(self, size):
+        multiple = 1024.0
+        for suffix in ['KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB']:
+            size /= multiple
+            if size < multiple:
+                return '%.1f %s' % (size, suffix)
+    
+        raise ValueError('number too large')
+
+    def finish(self):
+        self.step(self.bytes_total, force_update=True)
+        print()
+
+    @staticmethod
+    def terminal_width():
+        """Estimate the width of the terminal"""
+        width = 0
+        try:
+            import struct, fcntl, termios
+            s     = struct.pack('HHHH', 0, 0, 0, 0)
+            x     = fcntl.ioctl(1, termios.TIOCGWINSZ, s)
+            width = struct.unpack('HHHH', x)[1]
+        except (IOError, ImportError):
+            pass
+        if width <= 0:
+            try:
+                width = int(os.environ['COLUMNS'])
+            except:
+                pass
+        if width <= 0:
+            width = 80
+    
+        return width
+
+def rosbagmain(argv=None):
+    cmds = RosbagCmds()
+    cmds.add_cmd('record', record_cmd, "Record a bag file with the contents of specified topics.")
+    cmds.add_cmd('info', info_cmd, 'Summarize the contents of one or more bag files.')
+    cmds.add_cmd('play', play_cmd, "Play back the contents of one or more bag files in a time-synchronized fashion.")
+    cmds.add_cmd('check', check_cmd, 'Determine whether a bag is playable in the current system, or if it can be migrated.')
+    cmds.add_cmd('fix', fix_cmd, 'Repair the messages in a bag file so that it can be played in the current system.')
+    cmds.add_cmd('filter', filter_cmd, 'Filter the contents of the bag.')
+    cmds.add_cmd('compress', compress_cmd, 'Compress one or more bag files.')
+    cmds.add_cmd('decompress', decompress_cmd, 'Decompress one or more bag files.')
+    cmds.add_cmd('reindex', reindex_cmd, 'Reindexes one or more bag files.')
+    if sys.platform != 'win32':
+        cmds.add_cmd('encrypt', encrypt_cmd, 'Encrypt one or more bag files.')
+        cmds.add_cmd('decrypt', decrypt_cmd, 'Decrypt one or more bag files.')
+
+    if argv is None:
+        argv = sys.argv
+
+    if '-h' in argv or '--help' in argv:
+        argv = [a for a in argv if a != '-h' and a != '--help']
+        argv.insert(1, 'help')
+
+    if len(argv) > 1:
+        cmd = argv[1]
+    else:
+        cmd = 'help'
+
+    try:
+        if cmd in cmds:
+            cmds[cmd](argv[2:])
+        else:
+            cmds['help']([cmd])
+    except KeyboardInterrupt:
+        pass
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/src/time_translator.cpp
@@ -0,0 +1,64 @@
+/*********************************************************************
+* Software License Agreement (BSD License)
+*
+*  Copyright (c) 2010, Willow Garage, Inc.
+*  All rights reserved.
+*
+*  Redistribution and use in source and binary forms, with or without
+*  modification, are permitted provided that the following conditions
+*  are met:
+*
+*   * Redistributions of source code must retain the above copyright
+*     notice, this list of conditions and the following disclaimer.
+*   * Redistributions in binary form must reproduce the above
+*     copyright notice, this list of conditions and the following
+*     disclaimer in the documentation and/or other materials provided
+*     with the distribution.
+*   * Neither the name of Willow Garage, Inc. nor the names of its
+*     contributors may be used to endorse or promote products derived
+*     from this software without specific prior written permission.
+*
+*  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+*  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+*  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+*  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
+*  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
+*  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+*  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+*  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+*  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+*  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
+*  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+*  POSSIBILITY OF SUCH DAMAGE.
+*********************************************************************/
+
+#include "rosbag/time_translator.h"
+
+namespace rosbag {
+
+TimeTranslator::TimeTranslator()
+    : time_scale_(1.0), real_start_(ros::TIME_MIN), translated_start_(ros::TIME_MIN)
+{
+}
+
+void TimeTranslator::setTimeScale(double const& s) {
+    time_scale_ = s;
+}
+
+void TimeTranslator::setRealStartTime(ros::Time const& t) {
+    real_start_ = t;
+}
+
+void TimeTranslator::setTranslatedStartTime(ros::Time const& t) {
+    translated_start_ = t;
+}
+
+void TimeTranslator::shift(ros::Duration const& d) {
+    translated_start_ += d;
+}
+
+ros::Time TimeTranslator::translate(ros::Time const& t) {
+    return translated_start_ + (t - real_start_) * (1.0 / time_scale_);
+}
+
+} // namespace rosbag
--- /dev/null
+++ ros-noetic-rosbag-1.16.0/test/test_roundtrip.py
@@ -0,0 +1,75 @@
+import unittest
+import tempfile
+import os
+
+import rosbag
+import rospy
+
+BAG_DIR = tempfile.mkdtemp(prefix='rosbag_tests')
+
+class TestRoundTrip(unittest.TestCase):
+    def _write_simple_bag(self, name):
+        from std_msgs.msg import Int32, String
+
+        with rosbag.Bag(name, 'w') as bag:
+            s = String(data='foo')
+            i = Int32(data=42)
+
+            bag.write('chatter', s)
+            bag.write('numbers', i)
+
+    def _fname(self, name):
+        return os.path.join(BAG_DIR, name)
+
+    def test_value_equality(self):
+        fname = self._fname('test_value_equality.bag')
+
+        self._write_simple_bag(fname)
+
+        with rosbag.Bag(fname) as bag:
+            numbers = list(bag.read_messages('numbers'))
+            chatter = list(bag.read_messages('chatter'))
+
+        self.assertEqual(len(numbers), 1)
+        self.assertEqual(len(chatter), 1)
+
+        numbers = numbers[0]
+        chatter = chatter[0]
+
+        # channel names
+        self.assertEqual(numbers[0], 'numbers')
+        self.assertEqual(chatter[0], 'chatter')
+
+        # values
+        self.assertEqual(numbers[1].data, 42)
+        self.assertEqual(chatter[1].data, 'foo')
+
+    @unittest.expectedFailure
+    def test_type_equality(self):
+        fname = self._fname('test_type_equality.bag')
+
+        from std_msgs.msg import Int32, String
+
+        self._write_simple_bag(fname)
+
+        with rosbag.Bag(fname) as bag:
+            numbers = next(bag.read_messages('numbers'))
+            chatter = next(bag.read_messages('chatter'))
+
+        self.assertEqual(numbers[1].__class__, Int32)
+        self.assertEqual(chatter[1].__class__, String)
+
+    @unittest.expectedFailure
+    def test_type_isinstance(self):
+        fname = self._fname('test_type_isinstance.bag')
+
+        from std_msgs.msg import Int32, String
+
+        self._write_simple_bag(fname)
+
+        with rosbag.Bag(fname) as bag:
+            numbers = next(bag.read_messages('numbers'))
+            chatter = next(bag.read_messages('chatter'))
+
+        self.assertIsInstance(numbers[1], Int32)
+        self.assertIsInstance(chatter[1], String)
